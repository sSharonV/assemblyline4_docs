{"config":{"indexing":"full","lang":["en","fr"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"","title":"Home"},{"location":"administration/service_management/","text":"Service management \u00b6 Assemblyline's service management interface lets you: List all the services in the system View details about those services Add/Modify/Remove services Download/Restore a backup of the current services configurations You can find the service managment interface by clicking the User Avatar then choose Services from the administrator menu. Service list \u00b6 The first page you will be taken to when loading the service management interface will list all the services of the system. From this interface you can: Add services to the system Perform service updates Download/Restore a backup of the current services configurations View the detail of a service Update services \u00b6 If the system detected that there is a container with a newer version for your current deployment type (dev/stable). The service list will show an update button. Hovering over the button will let you know which new service version is available and clicking the button will kick off the update for the service. Add a service \u00b6 From the service management page, you can add a service by clicking the circled green \" + \" sign in the top right corner. This will open a popup window with an empty textbox. Simply paste the service_manifest.yml content of the service you which to add to the system then hit the \" Add \" button to add it to the system. Tip If your manifest properly uses the following environment variables, they will be replaced by the right values by the service add API: $SERVICE_TAG : Will de replaced by the latest tag for you current deployment type (dev/stable) found in the docker registry where the service container is hosted $REGISTRY : Will be replaced by your local registry Create / Restore Service config backups \u00b6 At the top right corner of the service management page, you will also find backup and restore buttons for creating and restoring backups for the services configurations. The backup button which looks like an \" arrow pointing down \" will create a yml with a filename of the following format: <FQDN>_service_backup.yml . The file will automatically be downloaded by your browser in your download directory. Once you want to restore the backup in your system, you can simply click the restore button, \" clock with a counter-clockwise arrow \", This will open a modal window with an empty textbox. Simply paste the content of the backup created earlier in the text box and hit the \" Restore \" button to restore the services configurations to their backed up values. Service Details \u00b6 If you which to modify or remove a service, you can simply click on that service from the service list which will bring you to the service detail page. The service detail page header contains two button shown all time that will let you: Delete the service (red \" circled minus \" button) Toggle between enabled/disable state (big square button right on top of the tabs) You will then have a tabbed interface which we will describe each tab bellow. General tab \u00b6 The \" General \" tab will let you see general informations about the service. In this tab, you will be able to modify the service's: Version Description Execution Stage Category Accepted/Rejected file types Execution timeout Maximum number of instances Location Result caching Tip You can refer to the service manifest documentation for more information about those different fields. Container tab \u00b6 The \" Container \" tab will show information about containers used by the service. In this tab, you will be able to: Change the update channel (Development/Stable) Change the main service service container Add/modify/remove dependency containers Main service container \u00b6 The main service container is the container containing and running the service code. By clicking the main service container, you will be able to modify the parameters used to launch that container. The list of parameters you will be able to modify is the following: Container image name Type of container registry Resources limits (CPU/RAM) Container registry credentials (username/password) Command executed in the container Allow internet access to the container Environment variables set before loading the container Tip Check the docker config block from the service manifest documentation to know more about the different field you can modify in the docker container configuration. Dependency containers \u00b6 Dependency containers are containers use to support the main services in some ways. Either by offering an external place to store data (A database for exemple) or to perform service updates. A service can have multiple dependency containers and these containers are shared between the multiple intances of the service that can be loaded in the system i.e. there will only be one instance of each dependency containers. By either click the \" Add Dependency \" button or clicking a dependency container, you will be able to either add or modify container dependencies of the current service. The dependency container configuration window look almost the same and let you modify the same values as the main service container window. There is however an added parameter that you can configure to give the container persistent storage. Tip Check the persistent volume block from the service manifest documentation to know more about the different fields to configure to get persistent storage in a dependency container. Updates tab \u00b6 The \" Updates \" tab shows information about how the service updates itself or its signatures. Warning This tab is optional and will not be shown for all service. Only services that define and update config block in their service manifest will have that tab shown. In this tab, you will be able to view/modify the following information: Interval at which the service updates If the service generates signatures in the system or not If the service needs to wait for a successful updates to start intances of itself The different sources where the service pulls its updates from Tip Checkout the Modifying sources documentation to know more about the different values you can change in the signature sources. Parameters tab \u00b6 Finally, the \" Parameters \" tab will let you view and customize the different parameters the service can take in. Service parameters are split into two categories: User specified parameters Service variables User specified parameters \u00b6 User specified parameters are parameters that a user can modify for each specific submission it does in the system. They are often but not exclusively used for things like: Turning on/off features of a service Specifying a password used during a submission Limit what the service can and cannot do Extract more or less files when a service runs Tip When these parameters are defined for a service, they will be shown in the submission options available for the user at submission time. Service variables \u00b6 Service variable are configuration parameters only shared between the service an your deployment. They are used to help the service configure itself to run well in your environment. Service variables are often but not exclusively things like: URLs to connect to external services Credentials use to connect to external services List of default values used in a service Configuration parameter that will limit or increase scanning capabilities of a service","title":"Service management"},{"location":"administration/service_management/#service-management","text":"Assemblyline's service management interface lets you: List all the services in the system View details about those services Add/Modify/Remove services Download/Restore a backup of the current services configurations You can find the service managment interface by clicking the User Avatar then choose Services from the administrator menu.","title":"Service management"},{"location":"administration/service_management/#service-list","text":"The first page you will be taken to when loading the service management interface will list all the services of the system. From this interface you can: Add services to the system Perform service updates Download/Restore a backup of the current services configurations View the detail of a service","title":"Service list"},{"location":"administration/service_management/#update-services","text":"If the system detected that there is a container with a newer version for your current deployment type (dev/stable). The service list will show an update button. Hovering over the button will let you know which new service version is available and clicking the button will kick off the update for the service.","title":"Update services"},{"location":"administration/service_management/#add-a-service","text":"From the service management page, you can add a service by clicking the circled green \" + \" sign in the top right corner. This will open a popup window with an empty textbox. Simply paste the service_manifest.yml content of the service you which to add to the system then hit the \" Add \" button to add it to the system. Tip If your manifest properly uses the following environment variables, they will be replaced by the right values by the service add API: $SERVICE_TAG : Will de replaced by the latest tag for you current deployment type (dev/stable) found in the docker registry where the service container is hosted $REGISTRY : Will be replaced by your local registry","title":"Add a service"},{"location":"administration/service_management/#create-restore-service-config-backups","text":"At the top right corner of the service management page, you will also find backup and restore buttons for creating and restoring backups for the services configurations. The backup button which looks like an \" arrow pointing down \" will create a yml with a filename of the following format: <FQDN>_service_backup.yml . The file will automatically be downloaded by your browser in your download directory. Once you want to restore the backup in your system, you can simply click the restore button, \" clock with a counter-clockwise arrow \", This will open a modal window with an empty textbox. Simply paste the content of the backup created earlier in the text box and hit the \" Restore \" button to restore the services configurations to their backed up values.","title":"Create / Restore Service config backups"},{"location":"administration/service_management/#service-details","text":"If you which to modify or remove a service, you can simply click on that service from the service list which will bring you to the service detail page. The service detail page header contains two button shown all time that will let you: Delete the service (red \" circled minus \" button) Toggle between enabled/disable state (big square button right on top of the tabs) You will then have a tabbed interface which we will describe each tab bellow.","title":"Service Details"},{"location":"administration/service_management/#general-tab","text":"The \" General \" tab will let you see general informations about the service. In this tab, you will be able to modify the service's: Version Description Execution Stage Category Accepted/Rejected file types Execution timeout Maximum number of instances Location Result caching Tip You can refer to the service manifest documentation for more information about those different fields.","title":"General tab"},{"location":"administration/service_management/#container-tab","text":"The \" Container \" tab will show information about containers used by the service. In this tab, you will be able to: Change the update channel (Development/Stable) Change the main service service container Add/modify/remove dependency containers","title":"Container tab"},{"location":"administration/service_management/#main-service-container","text":"The main service container is the container containing and running the service code. By clicking the main service container, you will be able to modify the parameters used to launch that container. The list of parameters you will be able to modify is the following: Container image name Type of container registry Resources limits (CPU/RAM) Container registry credentials (username/password) Command executed in the container Allow internet access to the container Environment variables set before loading the container Tip Check the docker config block from the service manifest documentation to know more about the different field you can modify in the docker container configuration.","title":"Main service container"},{"location":"administration/service_management/#dependency-containers","text":"Dependency containers are containers use to support the main services in some ways. Either by offering an external place to store data (A database for exemple) or to perform service updates. A service can have multiple dependency containers and these containers are shared between the multiple intances of the service that can be loaded in the system i.e. there will only be one instance of each dependency containers. By either click the \" Add Dependency \" button or clicking a dependency container, you will be able to either add or modify container dependencies of the current service. The dependency container configuration window look almost the same and let you modify the same values as the main service container window. There is however an added parameter that you can configure to give the container persistent storage. Tip Check the persistent volume block from the service manifest documentation to know more about the different fields to configure to get persistent storage in a dependency container.","title":"Dependency containers"},{"location":"administration/service_management/#updates-tab","text":"The \" Updates \" tab shows information about how the service updates itself or its signatures. Warning This tab is optional and will not be shown for all service. Only services that define and update config block in their service manifest will have that tab shown. In this tab, you will be able to view/modify the following information: Interval at which the service updates If the service generates signatures in the system or not If the service needs to wait for a successful updates to start intances of itself The different sources where the service pulls its updates from Tip Checkout the Modifying sources documentation to know more about the different values you can change in the signature sources.","title":"Updates tab"},{"location":"administration/service_management/#parameters-tab","text":"Finally, the \" Parameters \" tab will let you view and customize the different parameters the service can take in. Service parameters are split into two categories: User specified parameters Service variables","title":"Parameters tab"},{"location":"administration/service_management/#user-specified-parameters","text":"User specified parameters are parameters that a user can modify for each specific submission it does in the system. They are often but not exclusively used for things like: Turning on/off features of a service Specifying a password used during a submission Limit what the service can and cannot do Extract more or less files when a service runs Tip When these parameters are defined for a service, they will be shown in the submission options available for the user at submission time.","title":"User specified parameters"},{"location":"administration/service_management/#service-variables","text":"Service variable are configuration parameters only shared between the service an your deployment. They are used to help the service configure itself to run well in your environment. Service variables are often but not exclusively things like: URLs to connect to external services Credentials use to connect to external services List of default values used in a service Configuration parameter that will limit or increase scanning capabilities of a service","title":"Service variables"},{"location":"administration/signature_management/","text":"Signature Management \u00b6 Assemblyline's signature management interface lets you: List all signatures in the system Filter and search the current set of signatures View details about those signatures Set the status of a specific signature Remove signatures from the system You can find the signature management interface by clicking Manage then the Signatures menu from the navigation bar. Warning You cannot add new signatures to the system via this interface. Instead, Assemblyline has a source management interface which lets you add a variety of external sources to fetch signatures from. The updater of the different services takes care of loading the source URLs and the new signature(s) into the system. It will also sync existing signatures that have changed since the last import. Signature list \u00b6 The first page you will be taken to when loading the signature management interface will list all signatures that have been loaded into the system. From this interface you can: Page through the different signatures from the list Filter the displayed signatures with the search bar Assemblyline signatures can be searched using a Lucene query. As you start typing in the search box, the system will suggest fields that you can search into. You can also use the quick filter buttons for pre-defined searches. These pre-defined searches will help you get started with writing more complex signature searches. Download the currently viewed signature set with the download arrow on the top right View the detail of a signature by clicking on it Signature detail \u00b6 Once you click on a signature, the detail view for that signature will be shown. This page will show you the following information: ID of the signature (under the signature detail header) The raw signature Statistics about the signature A histogram of the signature for the last 30 days A list of the last ten hits for that signature On the top right, it will also show actions on the signature: You can hit the search button to find all instances where that signature hits in the system Use the red delete button to delete the signature from the system If the signature is still present in the source where it was retrieved, it will be re-added on the next update. In this case, you should disable the signature instead. Change the state of a signature Changing the signature state \u00b6 Signature states are synced with the source they are coming from but the state in your Assemblyline deployment will supersede the state that the rule updater is trying to set. This means that if you disable a rule in your Assemblyline instance, it will remain disabled even if the source where that rule is from changes. There are three different signature states: Deployed , Noisy , and Disabled Deployed : Deployed will be used for detection and will generate a score depending on how the service handles these types of signatures Noisy: Noisy will be used for detection but rules with these states will not affect the score of the file Disabled: Disabled signatures are completely ignored in the system and the service will not even realize that these signatures exist You can change the signatures by clicking the current signature state in the signature detail view. This will bring up the state-changing modal window which will let you pick a new state for the current rule.","title":"Signature Management"},{"location":"administration/signature_management/#signature-management","text":"Assemblyline's signature management interface lets you: List all signatures in the system Filter and search the current set of signatures View details about those signatures Set the status of a specific signature Remove signatures from the system You can find the signature management interface by clicking Manage then the Signatures menu from the navigation bar. Warning You cannot add new signatures to the system via this interface. Instead, Assemblyline has a source management interface which lets you add a variety of external sources to fetch signatures from. The updater of the different services takes care of loading the source URLs and the new signature(s) into the system. It will also sync existing signatures that have changed since the last import.","title":"Signature Management"},{"location":"administration/signature_management/#signature-list","text":"The first page you will be taken to when loading the signature management interface will list all signatures that have been loaded into the system. From this interface you can: Page through the different signatures from the list Filter the displayed signatures with the search bar Assemblyline signatures can be searched using a Lucene query. As you start typing in the search box, the system will suggest fields that you can search into. You can also use the quick filter buttons for pre-defined searches. These pre-defined searches will help you get started with writing more complex signature searches. Download the currently viewed signature set with the download arrow on the top right View the detail of a signature by clicking on it","title":"Signature list"},{"location":"administration/signature_management/#signature-detail","text":"Once you click on a signature, the detail view for that signature will be shown. This page will show you the following information: ID of the signature (under the signature detail header) The raw signature Statistics about the signature A histogram of the signature for the last 30 days A list of the last ten hits for that signature On the top right, it will also show actions on the signature: You can hit the search button to find all instances where that signature hits in the system Use the red delete button to delete the signature from the system If the signature is still present in the source where it was retrieved, it will be re-added on the next update. In this case, you should disable the signature instead. Change the state of a signature","title":"Signature detail"},{"location":"administration/signature_management/#changing-the-signature-state","text":"Signature states are synced with the source they are coming from but the state in your Assemblyline deployment will supersede the state that the rule updater is trying to set. This means that if you disable a rule in your Assemblyline instance, it will remain disabled even if the source where that rule is from changes. There are three different signature states: Deployed , Noisy , and Disabled Deployed : Deployed will be used for detection and will generate a score depending on how the service handles these types of signatures Noisy: Noisy will be used for detection but rules with these states will not affect the score of the file Disabled: Disabled signatures are completely ignored in the system and the service will not even realize that these signatures exist You can change the signatures by clicking the current signature state in the signature detail view. This will bring up the state-changing modal window which will let you pick a new state for the current rule.","title":"Changing the signature state"},{"location":"administration/source_management/","text":"Signature Source Management \u00b6 Modifying the signature set to support analysis is very simple and can be done directly through the Assemblyline User Interface. You can access the source management interface by selecting \" Source management \" in the navigation menu. The source management interface will list all services that support external sources and will show you the different sources currently configured in the system. Modifying sources \u00b6 With the source management interface, you can add, modify, or delete any sources of any service with the following actions: To add a new source to a given service, you can simply press the \" + \" button beside the service name for which you want to add the source. To modify or delete a source, simply click on the source you want to modify/delete. Both options will bring you to an interface that looks like this: Required input \u00b6 The following sections are required to add/modify a signature source in Assemblyline: URI This is the path to your sources. In this case, we will use a GitHub repository. The URI section also accepts HTTP URLs as input. Source Name This can be labeled at the user\u2019s discretion. For this example, we have used REVERSING_LABS_EXAMPLE. Please note that input for \"Source Name\" must not have any spaces. Optional input \u00b6 Pattern The user may add a regex pattern to pull certain file types for a particular service. In this example, only .yara or .yar files will be added as signatures. Username / Password This is the username and password for the URL or git repository that you are targeting. Private Key If using SSH to connect to GitHub, you must generate a private SSH key and add it to this section. Headers Header name and Header value are for special HTTP headers that may be passed to the HTTP server, such as passing an API key. Alternate methods of updating sources \u00b6 There are alternate ways that the system administrator can use to modify the signatures in the system: Before loading the service into Assemblyline Inside the service management interface Option 1 - Before loading the service \u00b6 The updater can be configured through the service_manifest.yml , which is in the root directory of each service. If you edit the files before pasting them into the system to add that service, the correct signature source(s) will be set once the service is first loaded. Suricata's updater You can find the Suricata updater configuration in its service_manifest.yml file. Its config block looks like this: ... update_config : generates_signatures : true method : run run_options : allow_internet_access : true command : [ \"python\" , \"-m\" , \"suricata_.suricata_updater\" ] image : ${REGISTRY}cccs/assemblyline-service-suricata:$SERVICE_TAG sources : - name : emt pattern : .*\\.rules uri : https://rules.emergingthreats.net/open/suricata/emerging.rules.tar.gz update_interval_seconds : 21600 # Quarter-day (every 6 hours) ... For more information about the update config block, you should check out the update config and update source sections of the service_manifest.yml documentation. Option 2 - Inside the service management interface \u00b6 First navigate to \" User \" -> \" Administration \" -> \" Services \" through the navigation bar: Click on the relevant service that you wish to update. Navigate to the \" Updates \" tab. You can change any value related to the updates in this section. Tip The source update interface in this section is like the Source management page although there are a few added options for: Turning the \"generate signature flag\" on/off Waiting for a valid update or not Setting the updating interval","title":"Signature Source Management"},{"location":"administration/source_management/#signature-source-management","text":"Modifying the signature set to support analysis is very simple and can be done directly through the Assemblyline User Interface. You can access the source management interface by selecting \" Source management \" in the navigation menu. The source management interface will list all services that support external sources and will show you the different sources currently configured in the system.","title":"Signature Source Management"},{"location":"administration/source_management/#modifying-sources","text":"With the source management interface, you can add, modify, or delete any sources of any service with the following actions: To add a new source to a given service, you can simply press the \" + \" button beside the service name for which you want to add the source. To modify or delete a source, simply click on the source you want to modify/delete. Both options will bring you to an interface that looks like this:","title":"Modifying sources"},{"location":"administration/source_management/#required-input","text":"The following sections are required to add/modify a signature source in Assemblyline: URI This is the path to your sources. In this case, we will use a GitHub repository. The URI section also accepts HTTP URLs as input. Source Name This can be labeled at the user\u2019s discretion. For this example, we have used REVERSING_LABS_EXAMPLE. Please note that input for \"Source Name\" must not have any spaces.","title":"Required input"},{"location":"administration/source_management/#optional-input","text":"Pattern The user may add a regex pattern to pull certain file types for a particular service. In this example, only .yara or .yar files will be added as signatures. Username / Password This is the username and password for the URL or git repository that you are targeting. Private Key If using SSH to connect to GitHub, you must generate a private SSH key and add it to this section. Headers Header name and Header value are for special HTTP headers that may be passed to the HTTP server, such as passing an API key.","title":"Optional input"},{"location":"administration/source_management/#alternate-methods-of-updating-sources","text":"There are alternate ways that the system administrator can use to modify the signatures in the system: Before loading the service into Assemblyline Inside the service management interface","title":"Alternate methods of updating sources"},{"location":"administration/source_management/#option-1-before-loading-the-service","text":"The updater can be configured through the service_manifest.yml , which is in the root directory of each service. If you edit the files before pasting them into the system to add that service, the correct signature source(s) will be set once the service is first loaded. Suricata's updater You can find the Suricata updater configuration in its service_manifest.yml file. Its config block looks like this: ... update_config : generates_signatures : true method : run run_options : allow_internet_access : true command : [ \"python\" , \"-m\" , \"suricata_.suricata_updater\" ] image : ${REGISTRY}cccs/assemblyline-service-suricata:$SERVICE_TAG sources : - name : emt pattern : .*\\.rules uri : https://rules.emergingthreats.net/open/suricata/emerging.rules.tar.gz update_interval_seconds : 21600 # Quarter-day (every 6 hours) ... For more information about the update config block, you should check out the update config and update source sections of the service_manifest.yml documentation.","title":"Option 1 - Before loading the service"},{"location":"administration/source_management/#option-2-inside-the-service-management-interface","text":"First navigate to \" User \" -> \" Administration \" -> \" Services \" through the navigation bar: Click on the relevant service that you wish to update. Navigate to the \" Updates \" tab. You can change any value related to the updates in this section. Tip The source update interface in this section is like the Source management page although there are a few added options for: Turning the \"generate signature flag\" on/off Waiting for a valid update or not Setting the updating interval","title":"Option 2 - Inside the service management interface"},{"location":"administration/system_safelist/","text":"System safelist \u00b6 Assemblyline includes a safelisting system that will let you ignore certain tags generated by services. Although safelisting is available to all users throughout the interface, you can specify more complex rules via the administration interface. Editing the safelist \u00b6 You can access the system safelist management page by clicking the \" System Safelist \" menu item in the user dropdown menu. Once the safelist management interface is open you will be greeted with a YAML file in an editor. From here you can edit the YAML directly in the interface and hit the \" Save changes \" button to apply the changes to the system. The system safelist is composed of two sections and each of those sections are composed of tag types with lists of values: match is where you list values for specific tag types that you want to safelist by using a direct string comparison ( == ) regex is where you list regular expressions for specific tag types that you want to safelist by using regular expression matching ( .match() ) Example match : <tag-type> : - <value> regex : <tag-type> : - <regular expression> Default system safelist \u00b6 There is a safelist installed in the system by default which covers some basic cases. These are the tags that are safelisted by default: Default system safelist # Default tag_safelist.yml file # # The following tags are safelisted: # - Domains pointing to localhost # - Domains commonly found in XML files, certificates, and during dynamic analysis runs # - IPs in the private network IP space # - URIs pointing to IPs in the private network IP space # - URIs commonly found in XML files, certificates, and during dynamic analysis runs # # Note: - You can override the default tag_safelist.yml by putting an # updated version in /etc/assemblyline/tag_safelist.yml. # - If you want to add values to one of the following tag types, # you must copy the default values to the new file. # - You can nullify values by putting an empty object or an empty list # in your new file # Match section contains tag types where each tag type is # a list of values that should be safelisted by using a direct # string comparison. match : # Direct match to dynamic domains network.dynamic.domain : - localhost - android.googlesource.com - play.google.com - schemas.android.com - xmlpull.org - schemas.openxmlformats.org - schemas.microsoft.com - settings-win.data.microsoft.com - vortex-win.data.microsoft.com - wpad.reddog.microsoft.com - verisign.com - csc3-2010-crl.verisign.com - csc3-2010-aia.verisign.com - ocsp.verisign.com - logo.verisign.com - crl.verisign.com - ctldl.windowsupdate.com - ns.adobe.com - www.w3.org - purl.org # Direct match to static domains network.static.domain : - localhost - android.googlesource.com - play.google.com - schemas.android.com - xmlpull.org - schemas.openxmlformats.org - schemas.microsoft.com - settings-win.data.microsoft.com - vortex-win.data.microsoft.com - wpad.reddog.microsoft.com - verisign.com - csc3-2010-crl.verisign.com - csc3-2010-aia.verisign.com - ocsp.verisign.com - logo.verisign.com - crl.verisign.com - ctldl.windowsupdate.com - ns.adobe.com - www.w3.org - purl.org # Regex section contains tag types where each tag type is # a list of regular expressions to be run to safelist # the associated tags. regex : # Regular expressions to safelist dynamic IPs (Private IPs) # Note: Since IPs have already been validated, the regular expression is simpler network.dynamic.ip : - (?:127\\.|10\\.|192\\.168|172\\.1[6-9]\\.|172\\.2[0-9]\\.|172\\.3[01]\\.).* # Regular expression to safelist static IPs (Private IPs) # Note: Since IPs have already been validated, the regular expression is simpler network.static.ip : - (?:127\\.|10\\.|192\\.168|172\\.1[6-9]\\.|172\\.2[0-9]\\.|172\\.3[01]\\.).* # Regular expression to safelist dynamic URIs network.dynamic.uri : - (?:ftp|http)s?://localhost(?:$|/.*) - (?:ftp|http)s?://(?:(?:(?:10|127)(?:\\.(?:[2](?:[0-5][0-5]|[01234][6-9])|[1][0-9][0-9]|[1-9][0-9]|[0-9])){3})|(?:172\\.(?:1[6-9]|2[0-9]|3[0-1])(?:\\.(?:2[0-4][0-9]|25[0-5]|[1][0-9][0-9]|[1-9][0-9]|[0-9])){2}|(?:192\\.168(?:\\.(?:25[0-5]|2[0-4][0-9]|1[0-9][0-9]|[1-9][0-9]|[0-9])){2})))(?:$|/.*) - https?://schemas\\.android\\.com/apk/res(-auto|/android) - https?://xmlpull\\.org/v1/doc/features\\.html(?:$|.*) - https?://android\\.googlesource\\.com/toolchain/llvm-project - https?://schemas\\.microsoft\\.com(?:$|/.*) - https?://schemas\\.openxmlformats\\.org(?:$|/.*) - https?://schemas\\.openxmlformats\\.org/officeDocument/2006/relationships/(image|attachedTemplate|header|footnotes|fontTable|customXml|endnotes|theme|settings|webSettings|glossaryDocument|numbering|footer|styles) - https?://schemas\\.microsoft\\.com/office/word/2010/(wordml|wordprocessingCanvas|wordprocessingInk|wordprocessingGroup|wordprocessingDrawing) - https?://schemas\\.microsoft\\.com/office/word/(2012|2006)/wordml - https?://schemas\\.microsoft\\.com/office/word/2015/wordml/symex - https?://schemas\\.microsoft\\.com/office/drawing/2014/chartex - https?://schemas\\.microsoft\\.com/office/drawing/2015/9/8/chartex - https?://schemas\\.openxmlformats\\.org/drawingml/2006/(main|wordprocessingDrawing) - https?://schemas\\.openxmlformats\\.org/package/2006/relationships - https?://schemas\\.openxmlformats\\.org/markup-compatibility/2006 - https?://schemas\\.openxmlformats\\.org/officeDocument/2006/(relationships|math) - https?://schemas\\.openxmlformats\\.org/word/2010/wordprocessingShape - https?://schemas\\.openxmlformats\\.org/wordprocessingml/2006/main - https?://www\\.verisign\\.com/(rpa0|rpa|cps0) - https?://wpad\\.reddog\\.microsoft\\.com/wpad\\.dat - https?://ocsp\\.verisign\\.com - https?://logo\\.verisign\\.com/vslogo\\.gif04 - https?://crl\\.verisign\\.com/pca3-g5\\.crl04 - https?://csc3-2010-crl\\.verisign\\.com/CSC3-2010\\.crl0D - https?://csc3-2010-aia\\.verisign\\.com/CSC3-2010\\.cer0 - https?://ctldl\\.windowsupdate\\.com/.* - https?://ns\\.adobe\\.com/photoshop/1\\.0/ - https?://ns\\.adobe\\.com/xap/1\\.0/ - https?://ns\\.adobe\\.com/xap/1\\.0/mm/ - https?://ns\\.adobe\\.com/xap/1\\.0/sType/ResourceEvent# - https?://purl\\.org/dc/elements/1\\.1/ - https?://www\\.w3\\.org/1999/02/22-rdf-syntax-ns# # Regular expression to safelist static URIs network.static.uri : - (?:ftp|http)s?://localhost(?:$|/.*) - (?:ftp|http)s?://(?:(?:(?:10|127)(?:\\.(?:[2](?:[0-5][0-5]|[01234][6-9])|[1][0-9][0-9]|[1-9][0-9]|[0-9])){3})|(?:172\\.(?:1[6-9]|2[0-9]|3[0-1])(?:\\.(?:2[0-4][0-9]|25[0-5]|[1][0-9][0-9]|[1-9][0-9]|[0-9])){2}|(?:192\\.168(?:\\.(?:25[0-5]|2[0-4][0-9]|1[0-9][0-9]|[1-9][0-9]|[0-9])){2})))(?:$|/.*) - https?://schemas\\.android\\.com/apk/res(-auto|/android) - https?://xmlpull\\.org/v1/doc/features\\.html(?:$|.*) - https?://android\\.googlesource\\.com/toolchain/llvm-project - https?://schemas\\.microsoft\\.com(?:$|/.*) - https?://schemas\\.openxmlformats\\.org(?:$|/.*) - https?://schemas\\.openxmlformats\\.org/officeDocument/2006/relationships/(image|attachedTemplate|header|footnotes|fontTable|customXml|endnotes|theme|settings|webSettings|glossaryDocument|numbering|footer|styles) - https?://schemas\\.microsoft\\.com/office/word/2010/(wordml|wordprocessingCanvas|wordprocessingInk|wordprocessingGroup|wordprocessingDrawing) - https?://schemas\\.microsoft\\.com/office/word/(2012|2006)/wordml - https?://schemas\\.microsoft\\.com/office/word/2015/wordml/symex - https?://schemas\\.microsoft\\.com/office/drawing/2014/chartex - https?://schemas\\.microsoft\\.com/office/drawing/2015/9/8/chartex - https?://schemas\\.openxmlformats\\.org/drawingml/2006/(main|wordprocessingDrawing) - https?://schemas\\.openxmlformats\\.org/package/2006/relationships - https?://schemas\\.openxmlformats\\.org/markup-compatibility/2006 - https?://schemas\\.openxmlformats\\.org/officeDocument/2006/(relationships|math) - https?://schemas\\.openxmlformats\\.org/word/2010/wordprocessingShape - https?://schemas\\.openxmlformats\\.org/wordprocessingml/2006/main - https?://www\\.verisign\\.com/(rpa0|rpa|cps0) - https?://wpad\\.reddog\\.microsoft\\.com/wpad\\.dat - https?://ocsp\\.verisign\\.com - https?://logo\\.verisign\\.com/vslogo\\.gif04 - https?://crl\\.verisign\\.com/pca3-g5\\.crl04 - https?://csc3-2010-crl\\.verisign\\.com/CSC3-2010\\.crl0D - https?://csc3-2010-aia\\.verisign\\.com/CSC3-2010\\.cer0 - https?://ctldl\\.windowsupdate\\.com/.* - https?://ns\\.adobe\\.com/photoshop/1\\.0/ - https?://ns\\.adobe\\.com/xap/1\\.0/ - https?://ns\\.adobe\\.com/xap/1\\.0/mm/ - https?://ns\\.adobe\\.com/xap/1\\.0/sType/ResourceEvent# - https?://purl\\.org/dc/elements/1\\.1/ - https?://www\\.w3\\.org/1999/02/22-rdf-syntax-ns# Tip You can also find the default safelist in the code: tag_safelist.yml","title":"System safelist"},{"location":"administration/system_safelist/#system-safelist","text":"Assemblyline includes a safelisting system that will let you ignore certain tags generated by services. Although safelisting is available to all users throughout the interface, you can specify more complex rules via the administration interface.","title":"System safelist"},{"location":"administration/system_safelist/#editing-the-safelist","text":"You can access the system safelist management page by clicking the \" System Safelist \" menu item in the user dropdown menu. Once the safelist management interface is open you will be greeted with a YAML file in an editor. From here you can edit the YAML directly in the interface and hit the \" Save changes \" button to apply the changes to the system. The system safelist is composed of two sections and each of those sections are composed of tag types with lists of values: match is where you list values for specific tag types that you want to safelist by using a direct string comparison ( == ) regex is where you list regular expressions for specific tag types that you want to safelist by using regular expression matching ( .match() ) Example match : <tag-type> : - <value> regex : <tag-type> : - <regular expression>","title":"Editing the safelist"},{"location":"administration/system_safelist/#default-system-safelist","text":"There is a safelist installed in the system by default which covers some basic cases. These are the tags that are safelisted by default: Default system safelist # Default tag_safelist.yml file # # The following tags are safelisted: # - Domains pointing to localhost # - Domains commonly found in XML files, certificates, and during dynamic analysis runs # - IPs in the private network IP space # - URIs pointing to IPs in the private network IP space # - URIs commonly found in XML files, certificates, and during dynamic analysis runs # # Note: - You can override the default tag_safelist.yml by putting an # updated version in /etc/assemblyline/tag_safelist.yml. # - If you want to add values to one of the following tag types, # you must copy the default values to the new file. # - You can nullify values by putting an empty object or an empty list # in your new file # Match section contains tag types where each tag type is # a list of values that should be safelisted by using a direct # string comparison. match : # Direct match to dynamic domains network.dynamic.domain : - localhost - android.googlesource.com - play.google.com - schemas.android.com - xmlpull.org - schemas.openxmlformats.org - schemas.microsoft.com - settings-win.data.microsoft.com - vortex-win.data.microsoft.com - wpad.reddog.microsoft.com - verisign.com - csc3-2010-crl.verisign.com - csc3-2010-aia.verisign.com - ocsp.verisign.com - logo.verisign.com - crl.verisign.com - ctldl.windowsupdate.com - ns.adobe.com - www.w3.org - purl.org # Direct match to static domains network.static.domain : - localhost - android.googlesource.com - play.google.com - schemas.android.com - xmlpull.org - schemas.openxmlformats.org - schemas.microsoft.com - settings-win.data.microsoft.com - vortex-win.data.microsoft.com - wpad.reddog.microsoft.com - verisign.com - csc3-2010-crl.verisign.com - csc3-2010-aia.verisign.com - ocsp.verisign.com - logo.verisign.com - crl.verisign.com - ctldl.windowsupdate.com - ns.adobe.com - www.w3.org - purl.org # Regex section contains tag types where each tag type is # a list of regular expressions to be run to safelist # the associated tags. regex : # Regular expressions to safelist dynamic IPs (Private IPs) # Note: Since IPs have already been validated, the regular expression is simpler network.dynamic.ip : - (?:127\\.|10\\.|192\\.168|172\\.1[6-9]\\.|172\\.2[0-9]\\.|172\\.3[01]\\.).* # Regular expression to safelist static IPs (Private IPs) # Note: Since IPs have already been validated, the regular expression is simpler network.static.ip : - (?:127\\.|10\\.|192\\.168|172\\.1[6-9]\\.|172\\.2[0-9]\\.|172\\.3[01]\\.).* # Regular expression to safelist dynamic URIs network.dynamic.uri : - (?:ftp|http)s?://localhost(?:$|/.*) - (?:ftp|http)s?://(?:(?:(?:10|127)(?:\\.(?:[2](?:[0-5][0-5]|[01234][6-9])|[1][0-9][0-9]|[1-9][0-9]|[0-9])){3})|(?:172\\.(?:1[6-9]|2[0-9]|3[0-1])(?:\\.(?:2[0-4][0-9]|25[0-5]|[1][0-9][0-9]|[1-9][0-9]|[0-9])){2}|(?:192\\.168(?:\\.(?:25[0-5]|2[0-4][0-9]|1[0-9][0-9]|[1-9][0-9]|[0-9])){2})))(?:$|/.*) - https?://schemas\\.android\\.com/apk/res(-auto|/android) - https?://xmlpull\\.org/v1/doc/features\\.html(?:$|.*) - https?://android\\.googlesource\\.com/toolchain/llvm-project - https?://schemas\\.microsoft\\.com(?:$|/.*) - https?://schemas\\.openxmlformats\\.org(?:$|/.*) - https?://schemas\\.openxmlformats\\.org/officeDocument/2006/relationships/(image|attachedTemplate|header|footnotes|fontTable|customXml|endnotes|theme|settings|webSettings|glossaryDocument|numbering|footer|styles) - https?://schemas\\.microsoft\\.com/office/word/2010/(wordml|wordprocessingCanvas|wordprocessingInk|wordprocessingGroup|wordprocessingDrawing) - https?://schemas\\.microsoft\\.com/office/word/(2012|2006)/wordml - https?://schemas\\.microsoft\\.com/office/word/2015/wordml/symex - https?://schemas\\.microsoft\\.com/office/drawing/2014/chartex - https?://schemas\\.microsoft\\.com/office/drawing/2015/9/8/chartex - https?://schemas\\.openxmlformats\\.org/drawingml/2006/(main|wordprocessingDrawing) - https?://schemas\\.openxmlformats\\.org/package/2006/relationships - https?://schemas\\.openxmlformats\\.org/markup-compatibility/2006 - https?://schemas\\.openxmlformats\\.org/officeDocument/2006/(relationships|math) - https?://schemas\\.openxmlformats\\.org/word/2010/wordprocessingShape - https?://schemas\\.openxmlformats\\.org/wordprocessingml/2006/main - https?://www\\.verisign\\.com/(rpa0|rpa|cps0) - https?://wpad\\.reddog\\.microsoft\\.com/wpad\\.dat - https?://ocsp\\.verisign\\.com - https?://logo\\.verisign\\.com/vslogo\\.gif04 - https?://crl\\.verisign\\.com/pca3-g5\\.crl04 - https?://csc3-2010-crl\\.verisign\\.com/CSC3-2010\\.crl0D - https?://csc3-2010-aia\\.verisign\\.com/CSC3-2010\\.cer0 - https?://ctldl\\.windowsupdate\\.com/.* - https?://ns\\.adobe\\.com/photoshop/1\\.0/ - https?://ns\\.adobe\\.com/xap/1\\.0/ - https?://ns\\.adobe\\.com/xap/1\\.0/mm/ - https?://ns\\.adobe\\.com/xap/1\\.0/sType/ResourceEvent# - https?://purl\\.org/dc/elements/1\\.1/ - https?://www\\.w3\\.org/1999/02/22-rdf-syntax-ns# # Regular expression to safelist static URIs network.static.uri : - (?:ftp|http)s?://localhost(?:$|/.*) - (?:ftp|http)s?://(?:(?:(?:10|127)(?:\\.(?:[2](?:[0-5][0-5]|[01234][6-9])|[1][0-9][0-9]|[1-9][0-9]|[0-9])){3})|(?:172\\.(?:1[6-9]|2[0-9]|3[0-1])(?:\\.(?:2[0-4][0-9]|25[0-5]|[1][0-9][0-9]|[1-9][0-9]|[0-9])){2}|(?:192\\.168(?:\\.(?:25[0-5]|2[0-4][0-9]|1[0-9][0-9]|[1-9][0-9]|[0-9])){2})))(?:$|/.*) - https?://schemas\\.android\\.com/apk/res(-auto|/android) - https?://xmlpull\\.org/v1/doc/features\\.html(?:$|.*) - https?://android\\.googlesource\\.com/toolchain/llvm-project - https?://schemas\\.microsoft\\.com(?:$|/.*) - https?://schemas\\.openxmlformats\\.org(?:$|/.*) - https?://schemas\\.openxmlformats\\.org/officeDocument/2006/relationships/(image|attachedTemplate|header|footnotes|fontTable|customXml|endnotes|theme|settings|webSettings|glossaryDocument|numbering|footer|styles) - https?://schemas\\.microsoft\\.com/office/word/2010/(wordml|wordprocessingCanvas|wordprocessingInk|wordprocessingGroup|wordprocessingDrawing) - https?://schemas\\.microsoft\\.com/office/word/(2012|2006)/wordml - https?://schemas\\.microsoft\\.com/office/word/2015/wordml/symex - https?://schemas\\.microsoft\\.com/office/drawing/2014/chartex - https?://schemas\\.microsoft\\.com/office/drawing/2015/9/8/chartex - https?://schemas\\.openxmlformats\\.org/drawingml/2006/(main|wordprocessingDrawing) - https?://schemas\\.openxmlformats\\.org/package/2006/relationships - https?://schemas\\.openxmlformats\\.org/markup-compatibility/2006 - https?://schemas\\.openxmlformats\\.org/officeDocument/2006/(relationships|math) - https?://schemas\\.openxmlformats\\.org/word/2010/wordprocessingShape - https?://schemas\\.openxmlformats\\.org/wordprocessingml/2006/main - https?://www\\.verisign\\.com/(rpa0|rpa|cps0) - https?://wpad\\.reddog\\.microsoft\\.com/wpad\\.dat - https?://ocsp\\.verisign\\.com - https?://logo\\.verisign\\.com/vslogo\\.gif04 - https?://crl\\.verisign\\.com/pca3-g5\\.crl04 - https?://csc3-2010-crl\\.verisign\\.com/CSC3-2010\\.crl0D - https?://csc3-2010-aia\\.verisign\\.com/CSC3-2010\\.cer0 - https?://ctldl\\.windowsupdate\\.com/.* - https?://ns\\.adobe\\.com/photoshop/1\\.0/ - https?://ns\\.adobe\\.com/xap/1\\.0/ - https?://ns\\.adobe\\.com/xap/1\\.0/mm/ - https?://ns\\.adobe\\.com/xap/1\\.0/sType/ResourceEvent# - https?://purl\\.org/dc/elements/1\\.1/ - https?://www\\.w3\\.org/1999/02/22-rdf-syntax-ns# Tip You can also find the default safelist in the code: tag_safelist.yml","title":"Default system safelist"},{"location":"administration/troubleshooting/","text":"Troubleshooting / FAQ \u00b6 We will update this page with typical issues and solutions. You can post your question to our Assemblyline Google Group or join our Discord community! Core Statistics Why is the hits counter for a certain signature not incrementing even though it hit 5000 times in the last hour? Rules' hit count are not calculated live. There is an external process that calculates them daily for performance optimization. However, you can click on the rule itself and it does calculate the stats for that specific rule and updates it right away. Updater Failed to establish a new connection: [Errno 110] Connection timed out Depending on the host mentioned in the error, ensure the deployment has access to the registry and its able to call the associated API. The following modifications will have to be made to your values.yaml: External Registry Let's say the domain of the registry is 'registry.local' and is hosted on port 443 configuration : services : image_variables : REGISTRY : \"registry.local/\" allow_insecure_registry : true Local Registry Let's say the local registry is hosted on port 32000 configuration : services : image_variables : REGISTRY : \"localhost:32000/\" update_image_variables : REGISTRY : \"<HOST_IP>:32000/\" allow_insecure_registry : true Kubernetes Connection timeouts to external domains By default, coreDNS is configured to the Google's Public DNS when trying to resolve external domains outside the cluster. You can configure it to use a different DNS via: sudo microk8s disable dns && sudo microk8s enabled dns:1.1.1.1 If this still poses an issue, refer to: Teleport - Troubleshooting Kubernetes Networking Issues How can I monitor the status of the deployment/cluster? The quickest way to monitor the status of your cluster is: kubectl get pods -n <assemblyline_namespace> There are other tools such as k9s and Lens that allow you to monitor your cluster in a more user-friendly manner. Are HPAs adjustable? Depending on the amount of activity you're receiving, you'll likely have to tweak the TargetUsage and ReqRam settings in your values.yaml for your particular deployment, either to cause it to scale faster or slower. Refer to: Kubernetes - Horizontal Pod Autoscaler for more information. NGINX 504 but everything seems to be running It's possible the domain you're accessing the UI with doesn't match the setting configuration.ui.fqdn in your values.yaml. If your setting is set to 'localhost' but you're accessing the UI using '192.0.0.1.nip.io', there is no ingress path using '192.0.0.1.nip.io' as a base. The simplest solution is to update your values.yaml to the appropriate FQDN and redeploy. Is it possible to mount an internal root CA bundle into core components to use? Yes! This would involve creating a configmap containing your CA bundle and using coreVolumes, coreMounts, and coreEnv in your values.yaml to pass that information onto the core deployments. An example of this configuration would be: coreEnv : - name : REQUESTS_CA_BUNDLE value : /usr/share/internal-ca coreMounts : - name : internal-certs mountPath : /usr/share/internal-ca coreVolumes : - name : internal-certs configMap : name : internal-certs-config Services General TASK PRE-EMPTED A service task can pre-empt for a number of reasons, such as: Container being killed due to OOM (Out of Memory) Increase service memory limits Debug service memory usage with sample Service instance ran beyond the service timeout Increase service timeout Debug service runtime with sample The service result didn't make it back to service-server Check the state of service-server deployments Inspect network policies Is there a limit resources to allocate to a service instance? No! The sky's the limit (or more accurately), you're bound to the resources allocated on your cluster. That being said, it's best practice to use what you need rather than what you want (especially if it's unwarranted). Can I deploy a 4.X service on a 4.Y Assemblyline system (where X,Y are system versions: X < Y) Yes and no. You can add a service with to the system with a lesser system version but it won't be enabled due to potential compatibility issues. It's advised to rebuild the service and tag with the system version that you want to deploy on. Service Updater My service doesn't seem to be getting any signatures for analysis.. Check the following on your service's updater instance: Does your service have the following environment variables set: updates_host, updates_port, updates_key Edit the deployment manually Disable the service, delete the related deployments from Kubernetes, restart scaler, then re-enable service Is there any downloaded signatures in /tmp/updater/update_dir*/*/ of the updater container? If empty, it suggests there was a problem pulling the signatures from Assemblyline If some sources are missing, it suggests there was a problem pulling signatures from that source I've added my signature sources, but nothing shows up when searching the Signature index.. Are the source links accessible from the cluster? Is the authentication for each source valid? How frequently is the service updater configured to check for source updates? If I modify signatures using the Assemblyline client or the API, will the service get these rules? Yes! Changes made involving our API trigger messaging events made to other components to Assemblyline causing the system to be more responsive. In the case of services with updaters, they'll be notified immediately and the service will gather the new rules after a submissions has processed.","title":"Troubleshooting / FAQ"},{"location":"administration/troubleshooting/#troubleshooting-faq","text":"We will update this page with typical issues and solutions. You can post your question to our Assemblyline Google Group or join our Discord community! Core Statistics Why is the hits counter for a certain signature not incrementing even though it hit 5000 times in the last hour? Rules' hit count are not calculated live. There is an external process that calculates them daily for performance optimization. However, you can click on the rule itself and it does calculate the stats for that specific rule and updates it right away. Updater Failed to establish a new connection: [Errno 110] Connection timed out Depending on the host mentioned in the error, ensure the deployment has access to the registry and its able to call the associated API. The following modifications will have to be made to your values.yaml: External Registry Let's say the domain of the registry is 'registry.local' and is hosted on port 443 configuration : services : image_variables : REGISTRY : \"registry.local/\" allow_insecure_registry : true Local Registry Let's say the local registry is hosted on port 32000 configuration : services : image_variables : REGISTRY : \"localhost:32000/\" update_image_variables : REGISTRY : \"<HOST_IP>:32000/\" allow_insecure_registry : true Kubernetes Connection timeouts to external domains By default, coreDNS is configured to the Google's Public DNS when trying to resolve external domains outside the cluster. You can configure it to use a different DNS via: sudo microk8s disable dns && sudo microk8s enabled dns:1.1.1.1 If this still poses an issue, refer to: Teleport - Troubleshooting Kubernetes Networking Issues How can I monitor the status of the deployment/cluster? The quickest way to monitor the status of your cluster is: kubectl get pods -n <assemblyline_namespace> There are other tools such as k9s and Lens that allow you to monitor your cluster in a more user-friendly manner. Are HPAs adjustable? Depending on the amount of activity you're receiving, you'll likely have to tweak the TargetUsage and ReqRam settings in your values.yaml for your particular deployment, either to cause it to scale faster or slower. Refer to: Kubernetes - Horizontal Pod Autoscaler for more information. NGINX 504 but everything seems to be running It's possible the domain you're accessing the UI with doesn't match the setting configuration.ui.fqdn in your values.yaml. If your setting is set to 'localhost' but you're accessing the UI using '192.0.0.1.nip.io', there is no ingress path using '192.0.0.1.nip.io' as a base. The simplest solution is to update your values.yaml to the appropriate FQDN and redeploy. Is it possible to mount an internal root CA bundle into core components to use? Yes! This would involve creating a configmap containing your CA bundle and using coreVolumes, coreMounts, and coreEnv in your values.yaml to pass that information onto the core deployments. An example of this configuration would be: coreEnv : - name : REQUESTS_CA_BUNDLE value : /usr/share/internal-ca coreMounts : - name : internal-certs mountPath : /usr/share/internal-ca coreVolumes : - name : internal-certs configMap : name : internal-certs-config Services General TASK PRE-EMPTED A service task can pre-empt for a number of reasons, such as: Container being killed due to OOM (Out of Memory) Increase service memory limits Debug service memory usage with sample Service instance ran beyond the service timeout Increase service timeout Debug service runtime with sample The service result didn't make it back to service-server Check the state of service-server deployments Inspect network policies Is there a limit resources to allocate to a service instance? No! The sky's the limit (or more accurately), you're bound to the resources allocated on your cluster. That being said, it's best practice to use what you need rather than what you want (especially if it's unwarranted). Can I deploy a 4.X service on a 4.Y Assemblyline system (where X,Y are system versions: X < Y) Yes and no. You can add a service with to the system with a lesser system version but it won't be enabled due to potential compatibility issues. It's advised to rebuild the service and tag with the system version that you want to deploy on. Service Updater My service doesn't seem to be getting any signatures for analysis.. Check the following on your service's updater instance: Does your service have the following environment variables set: updates_host, updates_port, updates_key Edit the deployment manually Disable the service, delete the related deployments from Kubernetes, restart scaler, then re-enable service Is there any downloaded signatures in /tmp/updater/update_dir*/*/ of the updater container? If empty, it suggests there was a problem pulling the signatures from Assemblyline If some sources are missing, it suggests there was a problem pulling signatures from that source I've added my signature sources, but nothing shows up when searching the Signature index.. Are the source links accessible from the cluster? Is the authentication for each source valid? How frequently is the service updater configured to check for source updates? If I modify signatures using the Assemblyline client or the API, will the service get these rules? Yes! Changes made involving our API trigger messaging events made to other components to Assemblyline causing the system to be more responsive. In the case of services with updaters, they'll be notified immediately and the service will gather the new rules after a submissions has processed.","title":"Troubleshooting / FAQ"},{"location":"administration/user_management/","text":"User Management \u00b6 Assemblyline's user management interface lets you: List all the users in the system Filter and search the current user list View details about users Remove users Enable/disable users Change the users' roles, quotas, and passwords You can find the user management interface by clicking the User Avatar then choose Users from the administrator menu. User list \u00b6 The first page you will be taken to when loading the user management interface will list all the users of the system. From this interface you can: Page through the different users of the system Filter the displayed users with the search bar Assemblyline users can be searched using a Lucene query. As you start typing in the search box, the system will suggest to you fields that you can search into. You can also use the quick filter buttons for pre-defined searches. These pre-defined searches will help you get started writing more complex user searches. Add users to the system using the top right \" Add User \" button View the detail of a user User detail \u00b6 Once you click on a user in the user list, the detail view for that user will be shown. This page will show you the following information: Identity of the user (username, avatar, name, groups, email address) Roles of the user Allowed quotas for the user Status of the user From this page, you will be able to take the following actions on the current user: Remove it Enable/disable access its access to the system Set its roles Set its quotas Change its password","title":"User Management"},{"location":"administration/user_management/#user-management","text":"Assemblyline's user management interface lets you: List all the users in the system Filter and search the current user list View details about users Remove users Enable/disable users Change the users' roles, quotas, and passwords You can find the user management interface by clicking the User Avatar then choose Users from the administrator menu.","title":"User Management"},{"location":"administration/user_management/#user-list","text":"The first page you will be taken to when loading the user management interface will list all the users of the system. From this interface you can: Page through the different users of the system Filter the displayed users with the search bar Assemblyline users can be searched using a Lucene query. As you start typing in the search box, the system will suggest to you fields that you can search into. You can also use the quick filter buttons for pre-defined searches. These pre-defined searches will help you get started writing more complex user searches. Add users to the system using the top right \" Add User \" button View the detail of a user","title":"User list"},{"location":"administration/user_management/#user-detail","text":"Once you click on a user in the user list, the detail view for that user will be shown. This page will show you the following information: Identity of the user (username, avatar, name, groups, email address) Roles of the user Allowed quotas for the user Status of the user From this page, you will be able to take the following actions on the current user: Remove it Enable/disable access its access to the system Set its roles Set its quotas Change its password","title":"User detail"},{"location":"developer_manual/core/infrastructure/","text":"Infrastructure \u00b6 This section gives you a complete overview of all the different components of Assemblyline, so you have a better idea of what they are used for. Dependencies \u00b6 The components listed here are external dependencies used by Assemblyline: Dependency Description Docker Docker is now at the heart of Assemblyline because all Assemblyline's components are now running as Docker containers. https://www.docker.com/ Kubernetes For multi-computer installations, Assemblyline uses Kubernetes to deploy the different Docker containers and keep them healthy. https://kubernetes.io/ Helm Helm is used to easily deploy and maintain our Kubernetes instance. https://helm.sh/ Elastic Stack Assemblyline uses the full Elastic stack to store results, logs, metrics, and APMs. It consists of the following components: https://www.elastic.co/elastic-stack Elasticsearch Elasticsearch is used for storing results, logs, and metrics of the system. It also provides search capability to Assemblyline. Kibana (optional) Provides dashboards to monitor your Assemblyline cluster. APM (optional) Gather Application Performance Metrics so that we can pinpoint potential performance issues with the system and fix them. Filebeat (optional) Gather all the logs for the different components into Elasticsearch to be displayed in Kibana. Metricbeat (optional) Gather metrics for the different hosts where the Docker containers are run. Redis We are using Redis for the queueing system, for messaging between the components, and as a remote data structure to keep multiple instances of a given component working in sync. https://redis.io/ Nginx Nginx is used by Assemblyline as a proxy to give the user access to the different user-facing components: UI, API, Socket Server, Kibana. https://www.nginx.com/ Minio For our default file storage, we use Minio which perfectly replicates the Amazon S3 API and is built to work with Kubernetes. https://min.io/ Core Components \u00b6 The components listed here are Assemblyline-made processes that perform various tasks in the system: Core components Description Link Alerter Create alerts for the different submissions in the system. Source code Dispatcher Route the files in the system while a submission is taking place. Make sure all files during a submission are completed by all required services. Source code Expiry Delete submissions and their results when their TTL (Time-to-live) expires. Source code Frontend Provides the user interface to interact with Assemblyline. Source code Ingester Move ingested files from the priority queues to the processing queues. Source code Metrics Aggregator Aggregate metrics of the different components in the system to save them into an ELK (Elasticsearch-Logstash-Kibana) stack. Source code Metrics Heartbeat Provide live metrics in the system for the dashboard. Source code Scaler Spin up and down services in the system depending on the load. Source code Statistics Aggregator Generate daily statistics about signatures and heuristics. Source code Updater Make sure the different services get their latest update files. Source code Workflow Run the different workflows in the system and apply their labels, priority, and status. Source code Service Server Provides an API for services to get tasks and post their results. Source code UI / Socket Server Provides the APIs and a socket.io interface to interact with Assemblyline. Source code Service interfaces \u00b6 The interfaces listed here are used by Assemblyline's services to process files, generate results, and communicate back to the Service Server: Service Interface Description Link Python 2 Compatibility Library A library that gives services Python 2 compatibility. Source code Result Class used by a service to generate results in the system. Source code Service Base Base Assemblyline service class. Source code Service Request Class that defines a request to scan a file for a given service. Source code Task Handler A Python wrapper that communicates with the service server to get a task, download files, and publish results. It communicates with the service via named pipes so that the service can execute the received tasks. Source code","title":"Infrastructure"},{"location":"developer_manual/core/infrastructure/#infrastructure","text":"This section gives you a complete overview of all the different components of Assemblyline, so you have a better idea of what they are used for.","title":"Infrastructure"},{"location":"developer_manual/core/infrastructure/#dependencies","text":"The components listed here are external dependencies used by Assemblyline: Dependency Description Docker Docker is now at the heart of Assemblyline because all Assemblyline's components are now running as Docker containers. https://www.docker.com/ Kubernetes For multi-computer installations, Assemblyline uses Kubernetes to deploy the different Docker containers and keep them healthy. https://kubernetes.io/ Helm Helm is used to easily deploy and maintain our Kubernetes instance. https://helm.sh/ Elastic Stack Assemblyline uses the full Elastic stack to store results, logs, metrics, and APMs. It consists of the following components: https://www.elastic.co/elastic-stack Elasticsearch Elasticsearch is used for storing results, logs, and metrics of the system. It also provides search capability to Assemblyline. Kibana (optional) Provides dashboards to monitor your Assemblyline cluster. APM (optional) Gather Application Performance Metrics so that we can pinpoint potential performance issues with the system and fix them. Filebeat (optional) Gather all the logs for the different components into Elasticsearch to be displayed in Kibana. Metricbeat (optional) Gather metrics for the different hosts where the Docker containers are run. Redis We are using Redis for the queueing system, for messaging between the components, and as a remote data structure to keep multiple instances of a given component working in sync. https://redis.io/ Nginx Nginx is used by Assemblyline as a proxy to give the user access to the different user-facing components: UI, API, Socket Server, Kibana. https://www.nginx.com/ Minio For our default file storage, we use Minio which perfectly replicates the Amazon S3 API and is built to work with Kubernetes. https://min.io/","title":"Dependencies"},{"location":"developer_manual/core/infrastructure/#core-components","text":"The components listed here are Assemblyline-made processes that perform various tasks in the system: Core components Description Link Alerter Create alerts for the different submissions in the system. Source code Dispatcher Route the files in the system while a submission is taking place. Make sure all files during a submission are completed by all required services. Source code Expiry Delete submissions and their results when their TTL (Time-to-live) expires. Source code Frontend Provides the user interface to interact with Assemblyline. Source code Ingester Move ingested files from the priority queues to the processing queues. Source code Metrics Aggregator Aggregate metrics of the different components in the system to save them into an ELK (Elasticsearch-Logstash-Kibana) stack. Source code Metrics Heartbeat Provide live metrics in the system for the dashboard. Source code Scaler Spin up and down services in the system depending on the load. Source code Statistics Aggregator Generate daily statistics about signatures and heuristics. Source code Updater Make sure the different services get their latest update files. Source code Workflow Run the different workflows in the system and apply their labels, priority, and status. Source code Service Server Provides an API for services to get tasks and post their results. Source code UI / Socket Server Provides the APIs and a socket.io interface to interact with Assemblyline. Source code","title":"Core Components"},{"location":"developer_manual/core/infrastructure/#service-interfaces","text":"The interfaces listed here are used by Assemblyline's services to process files, generate results, and communicate back to the Service Server: Service Interface Description Link Python 2 Compatibility Library A library that gives services Python 2 compatibility. Source code Result Class used by a service to generate results in the system. Source code Service Base Base Assemblyline service class. Source code Service Request Class that defines a request to scan a file for a given service. Source code Task Handler A Python wrapper that communicates with the service server to get a task, download files, and publish results. It communicates with the service via named pipes so that the service can execute the received tasks. Source code","title":"Service interfaces"},{"location":"developer_manual/docs/docs/","text":"Updating documentation \u00b6 This documentation is built using the awesome project: mkdocs-material Clone the documentation \u00b6 The documentation can be cloned from the following repository: git clone git@github.com:CybercentreCanada/assemblyline4_docs.git Install dependencies \u00b6 Create a virtual environment for your documentation in the /venv/ directory right in the assemblyline4_docs source and install mkdocs dependencies: cd assemblyline4_docs python -m venv venv pip install mkdocs-material == 7 .3.6 pip install mkdocs-static-i18n Run the documentation locally \u00b6 You can manually run the documentation from a shell: cd assemblyline4_docs mkdocs serve Alternatively, if you are using VSCode, you can launch the pre-configured task: Launch Assemblyline Documentation","title":"Documentation"},{"location":"developer_manual/docs/docs/#updating-documentation","text":"This documentation is built using the awesome project: mkdocs-material","title":"Updating documentation"},{"location":"developer_manual/docs/docs/#clone-the-documentation","text":"The documentation can be cloned from the following repository: git clone git@github.com:CybercentreCanada/assemblyline4_docs.git","title":"Clone the documentation"},{"location":"developer_manual/docs/docs/#install-dependencies","text":"Create a virtual environment for your documentation in the /venv/ directory right in the assemblyline4_docs source and install mkdocs dependencies: cd assemblyline4_docs python -m venv venv pip install mkdocs-material == 7 .3.6 pip install mkdocs-static-i18n","title":"Install dependencies"},{"location":"developer_manual/docs/docs/#run-the-documentation-locally","text":"You can manually run the documentation from a shell: cd assemblyline4_docs mkdocs serve Alternatively, if you are using VSCode, you can launch the pre-configured task: Launch Assemblyline Documentation","title":"Run the documentation locally"},{"location":"developer_manual/env/getting_started/","text":"Getting Started \u00b6 Before starting to develop for Assemblyline, you will need to set up your environment. We have a couple of options to assist you, from a free-to-use and easy-to-setup script to more complex setups. Development virtual machine \u00b6 Whether you are developing a new service or working on core components, Assemblyline requires specific external packages and files installed in specific directories in the containers. For this reason, it is recommended that you do not develop directly from your desktop unless you use remote debugging features. The minimum specifications for development virtual machine \u00b6 There are quite a few containers to run to spin-up the Assemblyline dependencies. For this reason, the development VM should have at least the following specifications: 2 cores 8 GB of RAM 40 GB of disk space Operating system \u00b6 We recommend that you use Ubuntu 20.04 for your development VM, because all instructions that we provide in the document has been built with this OS in mind. There are two versions that you can pick from: Ubuntu 20.04 Desktop - For local development where your IDE runs in the same VM as the Assemblyline containers (Definitely the easiest setup) Ubuntu 20.04 Server - For remote development where your IDE runs on your local computer and the Assemblyline containers run on the VM Choosing your IDE \u00b6 We have two IDEs for you to pick from, both of which support local and remote development: VSCode \u00b6 This is our recommended IDE since it is free and very easy to setup. Most of the Assemblyline team has moved to this IDE and we use a mix of local and remote development with it. Once you're done installing your VM, you can follow the instructions to get VSCode up and running using our simple setup script . For instructions on how to use VSCode, refer to the \" use VSCode \" documentation. PyCharm \u00b6 This IDE is much more robust in terms Python development but the setup is more complex. There is both a paid and free version of this IDE. The free Community edition of PyCharm will allow you to do local develpment only and you will have to run the dependencies by hand using docker-compose . The paid version, PyCharm Professional, will let you manage Docker dependencies inside the IDE and utilize remote development. Here are the installation instructions for the different setups: Local development (minimum requirement: PyCharm Community edition) Remote development (minimum requirement: PyCharm Professional edition) For instructions on how to use PyCharm, refer to the \" use PyCharm \" documentation.","title":"Getting Started"},{"location":"developer_manual/env/getting_started/#getting-started","text":"Before starting to develop for Assemblyline, you will need to set up your environment. We have a couple of options to assist you, from a free-to-use and easy-to-setup script to more complex setups.","title":"Getting Started"},{"location":"developer_manual/env/getting_started/#development-virtual-machine","text":"Whether you are developing a new service or working on core components, Assemblyline requires specific external packages and files installed in specific directories in the containers. For this reason, it is recommended that you do not develop directly from your desktop unless you use remote debugging features.","title":"Development virtual machine"},{"location":"developer_manual/env/getting_started/#the-minimum-specifications-for-development-virtual-machine","text":"There are quite a few containers to run to spin-up the Assemblyline dependencies. For this reason, the development VM should have at least the following specifications: 2 cores 8 GB of RAM 40 GB of disk space","title":"The minimum specifications for development virtual machine"},{"location":"developer_manual/env/getting_started/#operating-system","text":"We recommend that you use Ubuntu 20.04 for your development VM, because all instructions that we provide in the document has been built with this OS in mind. There are two versions that you can pick from: Ubuntu 20.04 Desktop - For local development where your IDE runs in the same VM as the Assemblyline containers (Definitely the easiest setup) Ubuntu 20.04 Server - For remote development where your IDE runs on your local computer and the Assemblyline containers run on the VM","title":"Operating system"},{"location":"developer_manual/env/getting_started/#choosing-your-ide","text":"We have two IDEs for you to pick from, both of which support local and remote development:","title":"Choosing your IDE"},{"location":"developer_manual/env/getting_started/#vscode","text":"This is our recommended IDE since it is free and very easy to setup. Most of the Assemblyline team has moved to this IDE and we use a mix of local and remote development with it. Once you're done installing your VM, you can follow the instructions to get VSCode up and running using our simple setup script . For instructions on how to use VSCode, refer to the \" use VSCode \" documentation.","title":"VSCode"},{"location":"developer_manual/env/getting_started/#pycharm","text":"This IDE is much more robust in terms Python development but the setup is more complex. There is both a paid and free version of this IDE. The free Community edition of PyCharm will allow you to do local develpment only and you will have to run the dependencies by hand using docker-compose . The paid version, PyCharm Professional, will let you manage Docker dependencies inside the IDE and utilize remote development. Here are the installation instructions for the different setups: Local development (minimum requirement: PyCharm Community edition) Remote development (minimum requirement: PyCharm Professional edition) For instructions on how to use PyCharm, refer to the \" use PyCharm \" documentation.","title":"PyCharm"},{"location":"developer_manual/env/pycharm/local_development/","text":"Local development \u00b6 This documentation will show you how to set up your development virtual machine for local development using the PyCharm Community Edition IDE (This would also work with PyCharm Professional). In this setup, you will run your IDE inside the virtual machine where the Assemblyline containers are running. This is by far the easiest setup to get PyCharm working and removes a lot of headaches. Operating system \u00b6 For this documentation, we will assume that you are working on a fresh installation of Ubuntu 20.04 Desktop . Update VM \u00b6 Make sure Ubuntu is running the latest software sudo apt update sudo apt dist-upgrade sudo snap refresh Reboot if needed sudo reboot Installing pre-requisite software \u00b6 Install Assemblyline APT dependencies \u00b6 sudo apt update sudo apt-get install -yy libfuzzy2 libmagic1 libldap-2.4-2 libsasl2-2 build-essential libffi-dev libfuzzy-dev libldap2-dev libsasl2-dev libssl-dev Install Python 3.9 \u00b6 Assemblyline 4 containers are now all built on Python 3.9 therefore we will install Python 3.9. sudo apt install -y software-properties-common sudo add-apt-repository -y ppa:deadsnakes/ppa sudo apt-get install -yy python3-venv python3.9 python3.9-dev python3.9-venv libffi7 Installing Docker \u00b6 Follow these simple commands to get Docker running on your machine: # Add Docker repository sudo apt-get update sudo apt-get install -y apt-transport-https ca-certificates curl gnupg-agent software-properties-common curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - sudo apt-key fingerprint 0EBFCD88 sudo add-apt-repository \"deb [arch=amd64] https://download.docker.com/linux/ubuntu $( lsb_release -cs ) stable\" # Install Docker sudo apt-get install -y docker-ce docker-ce-cli containerd.io # Test Docker installation sudo docker run hello-world Installing docker-compose \u00b6 Installing docker-compose is done the same way on all Linux distros. Follow these simple instructions: # Install docker-compose sudo curl -L \"https://github.com/docker/compose/releases/download/1.28.5/docker-compose- $( uname -s ) - $( uname -m ) \" -o /usr/local/bin/docker-compose sudo chmod +x /usr/local/bin/docker-compose # Test docker-compose installation docker-compose --version Installing PyCharm \u00b6 Let's install the desired PyCharm version. The Professional version is not needed but if you have a licence you can use it. PyCharm Community sudo snap install --classic pycharm-community PyCharm Professional sudo snap install --classic pycharm-professional Adding Assemblyline specific configuration \u00b6 Data directories \u00b6 Because Assemblyline uses its own set of folders inside the core, service-server, and UI containers, we must create the same folder structure here so we can run the components in debug mode. sudo mkdir -p /etc/assemblyline sudo mkdir -p /var/cache/assemblyline sudo mkdir -p /var/lib/assemblyline sudo mkdir -p /var/log/assemblyline sudo chown $USER /etc/assemblyline sudo chown $USER /var/cache/assemblyline sudo chown $USER /var/lib/assemblyline sudo chown $USER /var/log/assemblyline Dev default configuration files \u00b6 Here we will create configuration files that match the default dev docker-compose configuration files so that we can swap any of the components to the one that is being debugged. echo \"enforce: true\" > /etc/assemblyline/classification.yml echo \" auth: internal: enabled: true core: alerter: delay: 0 metrics: apm_server: server_url: http://localhost:8200/ elasticsearch: hosts: [http://elastic:devpass@localhost] datastore: ilm: indexes: alert: unit: m error: unit: m file: unit: m result: unit: m submission: unit: m filestore: cache: - file:///var/cache/assemblyline/ logging: log_level: INFO log_as_json: false ui: audit: false debug: false enforce_quota: false fqdn: 127.0.0.1.nip.io \" > /etc/assemblyline/config.yml Tip As you can see in the last command we are setting the FQDN to 127.0.0.1.nip.io. NIP.IO is a service that will resolve the first part of the domain 127.0.0.1 .nip.io to its IP value. We use this to fake DNS when there are none. This is especially useful for oAuth because some providers are forbidding redirect URLs to IPs. Setup Assemblyline source \u00b6 Install git \u00b6 Since your VM is running Ubuntu 20.04 you can just install it with APT: sudo apt install -y git Tip You should add your desktop SSH keys to your GitHub account to use Git via SSH. Follow these instructions to do so: GitHub Help Clone core repositories \u00b6 Create the core working directory mkdir -p ~/git/alv4 cd ~/git/alv4 Clone Assemblyline's repositories Git via SSH Use SSH if you have your SSH id_rsa file configured to your GitHub account git clone git@github.com:CybercentreCanada/assemblyline-base.git git clone git@github.com:CybercentreCanada/assemblyline-core.git git clone git@github.com:CybercentreCanada/assemblyline-service-client.git git clone git@github.com:CybercentreCanada/assemblyline-service-server.git git clone git@github.com:CybercentreCanada/assemblyline-ui.git git clone git@github.com:CybercentreCanada/assemblyline-v4-service.git Git via HTTPS Use HTTPS if you don't have your GitHub account configured with an SSH key git clone https://github.com/CybercentreCanada/assemblyline-base.git git clone https://github.com/CybercentreCanada/assemblyline-core.git git clone https://github.com/CybercentreCanada/assemblyline-service-client.git git clone https://github.com/CybercentreCanada/assemblyline-service-server.git git clone https://github.com/CybercentreCanada/assemblyline-ui.git git clone https://github.com/CybercentreCanada/assemblyline-v4-service.git Virtual Environment \u00b6 # Directly in the alv4 source directory cd ~/git/alv4 # Create the virtualenv python3.9 -m venv venv # Install Assemblyline packages with their test dependencies ~/git/alv4/venv/bin/pip install assemblyline [ test ] assemblyline-core [ test ] assemblyline-service-server [ test ] assemblyline-ui [ test ] # Remove Assemblyline packages because we will use the live code ~/git/alv4/venv/bin/pip uninstall -y assemblyline assemblyline-core assemblyline-service-server assemblyline-ui Setting up Services (Optional) \u00b6 If you plan on doing service development in PyCharm you will need a dedicated directory for services with its own virtual environment. Clone service repositories \u00b6 Create the service working directory mkdir -p ~/git/services cd ~/git/services Clone Assemblyline's services repositories Git via SSH Use SSH if you have your SSH id_rsa file configured to your GitHub account git clone git@github.com:CybercentreCanada/assemblyline-service-antivirus.git git clone git@github.com:CybercentreCanada/assemblyline-service-apkaye.git git clone git@github.com:CybercentreCanada/assemblyline-service-avclass.git git clone git@github.com:CybercentreCanada/assemblyline-service-beaver.git git clone git@github.com:CybercentreCanada/assemblyline-service-characterize.git git clone git@github.com:CybercentreCanada/assemblyline-service-configextractor.git git clone git@github.com:CybercentreCanada/assemblyline-service-cuckoo.git git clone git@github.com:CybercentreCanada/assemblyline-service-deobfuscripter.git git clone git@github.com:CybercentreCanada/assemblyline-service-emlparser.git git clone git@github.com:CybercentreCanada/assemblyline-service-espresso.git git clone git@github.com:CybercentreCanada/assemblyline-service-extract.git git clone git@github.com:CybercentreCanada/assemblyline-service-floss.git git clone git@github.com:CybercentreCanada/assemblyline-service-frankenstrings.git git clone git@github.com:CybercentreCanada/assemblyline-service-iparse.git git clone git@github.com:CybercentreCanada/assemblyline-service-metadefender.git git clone git@github.com:CybercentreCanada/assemblyline-service-metapeek.git git clone git@github.com:CybercentreCanada/assemblyline-service-oletools.git git clone git@github.com:CybercentreCanada/assemblyline-service-pdfid.git git clone git@github.com:CybercentreCanada/assemblyline-service-peepdf.git git clone git@github.com:CybercentreCanada/assemblyline-service-pefile.git git clone git@github.com:CybercentreCanada/assemblyline-service-pixaxe.git git clone git@github.com:CybercentreCanada/assemblyline-service-safelist.git git clone git@github.com:CybercentreCanada/assemblyline-service-sigma.git git clone git@github.com:CybercentreCanada/assemblyline-service-suricata.git git clone git@github.com:CybercentreCanada/assemblyline-service-swiffer.git git clone git@github.com:CybercentreCanada/assemblyline-service-torrentslicer.git git clone git@github.com:CybercentreCanada/assemblyline-service-unpacker.git git clone git@github.com:CybercentreCanada/assemblyline-service-unpacme.git git clone git@github.com:CybercentreCanada/assemblyline-service-vipermonkey.git git clone git@github.com:CybercentreCanada/assemblyline-service-virustotal-dynamic.git git clone git@github.com:CybercentreCanada/assemblyline-service-virustotal-static.git git clone git@github.com:CybercentreCanada/assemblyline-service-XLMMacroDeobfuscator.git git clone git@github.com:CybercentreCanada/assemblyline-service-yara.git Git via HTTPS Use HTTPS if you don't have your GitHub account configured with an SSH key git clone https://github.com/CybercentreCanada/assemblyline-service-antivirus.git git clone https://github.com/CybercentreCanada/assemblyline-service-apkaye.git git clone https://github.com/CybercentreCanada/assemblyline-service-avclass.git git clone https://github.com/CybercentreCanada/assemblyline-service-beaver.git git clone https://github.com/CybercentreCanada/assemblyline-service-characterize.git git clone https://github.com/CybercentreCanada/assemblyline-service-configextractor.git git clone https://github.com/CybercentreCanada/assemblyline-service-cuckoo.git git clone https://github.com/CybercentreCanada/assemblyline-service-deobfuscripter.git git clone https://github.com/CybercentreCanada/assemblyline-service-emlparser.git git clone https://github.com/CybercentreCanada/assemblyline-service-espresso.git git clone https://github.com/CybercentreCanada/assemblyline-service-extract.git git clone https://github.com/CybercentreCanada/assemblyline-service-floss.git git clone https://github.com/CybercentreCanada/assemblyline-service-frankenstrings.git git clone https://github.com/CybercentreCanada/assemblyline-service-iparse.git git clone https://github.com/CybercentreCanada/assemblyline-service-metadefender.git git clone https://github.com/CybercentreCanada/assemblyline-service-metapeek.git git clone https://github.com/CybercentreCanada/assemblyline-service-oletools.git git clone https://github.com/CybercentreCanada/assemblyline-service-pdfid.git git clone https://github.com/CybercentreCanada/assemblyline-service-peepdf.git git clone https://github.com/CybercentreCanada/assemblyline-service-pefile.git git clone https://github.com/CybercentreCanada/assemblyline-service-pixaxe.git git clone https://github.com/CybercentreCanada/assemblyline-service-safelist.git git clone https://github.com/CybercentreCanada/assemblyline-service-sigma.git git clone https://github.com/CybercentreCanada/assemblyline-service-suricata.git git clone https://github.com/CybercentreCanada/assemblyline-service-swiffer.git git clone https://github.com/CybercentreCanada/assemblyline-service-torrentslicer.git git clone https://github.com/CybercentreCanada/assemblyline-service-unpacker.git git clone https://github.com/CybercentreCanada/assemblyline-service-unpacme.git git clone https://github.com/CybercentreCanada/assemblyline-service-vipermonkey.git git clone https://github.com/CybercentreCanada/assemblyline-service-virustotal-dynamic.git git clone https://github.com/CybercentreCanada/assemblyline-service-virustotal-static.git git clone https://github.com/CybercentreCanada/assemblyline-service-XLMMacroDeobfuscator.git git clone https://github.com/CybercentreCanada/assemblyline-service-yara.git Virtual Environment \u00b6 # Directly in the services source directory cd ~/git/services # Create the virtualenv python3.9 -m venv venv # Install Assemblyline packages from source in the services virtualenv ~/git/services/venv/bin/pip install -e ~/git/alv4/assemblyline-base ~/git/services/venv/bin/pip install -e ~/git/alv4/assemblyline-core ~/git/services/venv/bin/pip install -e ~/git/alv4/assemblyline-service-client ~/git/services/venv/bin/pip install -e ~/git/alv4/assemblyline-v4-service Setup PyCharm for core \u00b6 Load PyCharm Choose whatever configuration option you want until the Welcome screen Click the Open button Choose the ~/git/alv4 directory Info Your Python interpreter shows as No Interpreter in the bottom right corner of the window, do the following: Click on it Click add interpreter Choose \"Existing Environment\" Click \"OK\" Setup PyCharm for service (optional) \u00b6 From your core PyCharm window open the File menu then click Open Choose the ~/git/services directory Select New Window Info Your Python interpreter shows as No Interpreter in the bottom right corner of the window, do the following: Click on it Click add interpreter Choose \"Existing Environment\" Click \"OK\" Use PyCharm \u00b6 Now that your Local development VM is set up you should read the use PyCharm documentation to get you started.","title":"Local development"},{"location":"developer_manual/env/pycharm/local_development/#local-development","text":"This documentation will show you how to set up your development virtual machine for local development using the PyCharm Community Edition IDE (This would also work with PyCharm Professional). In this setup, you will run your IDE inside the virtual machine where the Assemblyline containers are running. This is by far the easiest setup to get PyCharm working and removes a lot of headaches.","title":"Local development"},{"location":"developer_manual/env/pycharm/local_development/#operating-system","text":"For this documentation, we will assume that you are working on a fresh installation of Ubuntu 20.04 Desktop .","title":"Operating system"},{"location":"developer_manual/env/pycharm/local_development/#update-vm","text":"Make sure Ubuntu is running the latest software sudo apt update sudo apt dist-upgrade sudo snap refresh Reboot if needed sudo reboot","title":"Update VM"},{"location":"developer_manual/env/pycharm/local_development/#installing-pre-requisite-software","text":"","title":"Installing pre-requisite software"},{"location":"developer_manual/env/pycharm/local_development/#install-assemblyline-apt-dependencies","text":"sudo apt update sudo apt-get install -yy libfuzzy2 libmagic1 libldap-2.4-2 libsasl2-2 build-essential libffi-dev libfuzzy-dev libldap2-dev libsasl2-dev libssl-dev","title":"Install Assemblyline APT dependencies"},{"location":"developer_manual/env/pycharm/local_development/#install-python-39","text":"Assemblyline 4 containers are now all built on Python 3.9 therefore we will install Python 3.9. sudo apt install -y software-properties-common sudo add-apt-repository -y ppa:deadsnakes/ppa sudo apt-get install -yy python3-venv python3.9 python3.9-dev python3.9-venv libffi7","title":"Install Python 3.9"},{"location":"developer_manual/env/pycharm/local_development/#installing-docker","text":"Follow these simple commands to get Docker running on your machine: # Add Docker repository sudo apt-get update sudo apt-get install -y apt-transport-https ca-certificates curl gnupg-agent software-properties-common curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - sudo apt-key fingerprint 0EBFCD88 sudo add-apt-repository \"deb [arch=amd64] https://download.docker.com/linux/ubuntu $( lsb_release -cs ) stable\" # Install Docker sudo apt-get install -y docker-ce docker-ce-cli containerd.io # Test Docker installation sudo docker run hello-world","title":"Installing Docker"},{"location":"developer_manual/env/pycharm/local_development/#installing-docker-compose","text":"Installing docker-compose is done the same way on all Linux distros. Follow these simple instructions: # Install docker-compose sudo curl -L \"https://github.com/docker/compose/releases/download/1.28.5/docker-compose- $( uname -s ) - $( uname -m ) \" -o /usr/local/bin/docker-compose sudo chmod +x /usr/local/bin/docker-compose # Test docker-compose installation docker-compose --version","title":"Installing docker-compose"},{"location":"developer_manual/env/pycharm/local_development/#installing-pycharm","text":"Let's install the desired PyCharm version. The Professional version is not needed but if you have a licence you can use it. PyCharm Community sudo snap install --classic pycharm-community PyCharm Professional sudo snap install --classic pycharm-professional","title":"Installing PyCharm"},{"location":"developer_manual/env/pycharm/local_development/#adding-assemblyline-specific-configuration","text":"","title":"Adding Assemblyline specific configuration"},{"location":"developer_manual/env/pycharm/local_development/#data-directories","text":"Because Assemblyline uses its own set of folders inside the core, service-server, and UI containers, we must create the same folder structure here so we can run the components in debug mode. sudo mkdir -p /etc/assemblyline sudo mkdir -p /var/cache/assemblyline sudo mkdir -p /var/lib/assemblyline sudo mkdir -p /var/log/assemblyline sudo chown $USER /etc/assemblyline sudo chown $USER /var/cache/assemblyline sudo chown $USER /var/lib/assemblyline sudo chown $USER /var/log/assemblyline","title":"Data directories"},{"location":"developer_manual/env/pycharm/local_development/#dev-default-configuration-files","text":"Here we will create configuration files that match the default dev docker-compose configuration files so that we can swap any of the components to the one that is being debugged. echo \"enforce: true\" > /etc/assemblyline/classification.yml echo \" auth: internal: enabled: true core: alerter: delay: 0 metrics: apm_server: server_url: http://localhost:8200/ elasticsearch: hosts: [http://elastic:devpass@localhost] datastore: ilm: indexes: alert: unit: m error: unit: m file: unit: m result: unit: m submission: unit: m filestore: cache: - file:///var/cache/assemblyline/ logging: log_level: INFO log_as_json: false ui: audit: false debug: false enforce_quota: false fqdn: 127.0.0.1.nip.io \" > /etc/assemblyline/config.yml Tip As you can see in the last command we are setting the FQDN to 127.0.0.1.nip.io. NIP.IO is a service that will resolve the first part of the domain 127.0.0.1 .nip.io to its IP value. We use this to fake DNS when there are none. This is especially useful for oAuth because some providers are forbidding redirect URLs to IPs.","title":"Dev default configuration files"},{"location":"developer_manual/env/pycharm/local_development/#setup-assemblyline-source","text":"","title":"Setup Assemblyline source"},{"location":"developer_manual/env/pycharm/local_development/#install-git","text":"Since your VM is running Ubuntu 20.04 you can just install it with APT: sudo apt install -y git Tip You should add your desktop SSH keys to your GitHub account to use Git via SSH. Follow these instructions to do so: GitHub Help","title":"Install git"},{"location":"developer_manual/env/pycharm/local_development/#clone-core-repositories","text":"Create the core working directory mkdir -p ~/git/alv4 cd ~/git/alv4 Clone Assemblyline's repositories Git via SSH Use SSH if you have your SSH id_rsa file configured to your GitHub account git clone git@github.com:CybercentreCanada/assemblyline-base.git git clone git@github.com:CybercentreCanada/assemblyline-core.git git clone git@github.com:CybercentreCanada/assemblyline-service-client.git git clone git@github.com:CybercentreCanada/assemblyline-service-server.git git clone git@github.com:CybercentreCanada/assemblyline-ui.git git clone git@github.com:CybercentreCanada/assemblyline-v4-service.git Git via HTTPS Use HTTPS if you don't have your GitHub account configured with an SSH key git clone https://github.com/CybercentreCanada/assemblyline-base.git git clone https://github.com/CybercentreCanada/assemblyline-core.git git clone https://github.com/CybercentreCanada/assemblyline-service-client.git git clone https://github.com/CybercentreCanada/assemblyline-service-server.git git clone https://github.com/CybercentreCanada/assemblyline-ui.git git clone https://github.com/CybercentreCanada/assemblyline-v4-service.git","title":"Clone core repositories"},{"location":"developer_manual/env/pycharm/local_development/#virtual-environment","text":"# Directly in the alv4 source directory cd ~/git/alv4 # Create the virtualenv python3.9 -m venv venv # Install Assemblyline packages with their test dependencies ~/git/alv4/venv/bin/pip install assemblyline [ test ] assemblyline-core [ test ] assemblyline-service-server [ test ] assemblyline-ui [ test ] # Remove Assemblyline packages because we will use the live code ~/git/alv4/venv/bin/pip uninstall -y assemblyline assemblyline-core assemblyline-service-server assemblyline-ui","title":"Virtual Environment"},{"location":"developer_manual/env/pycharm/local_development/#setting-up-services-optional","text":"If you plan on doing service development in PyCharm you will need a dedicated directory for services with its own virtual environment.","title":"Setting up Services (Optional)"},{"location":"developer_manual/env/pycharm/local_development/#clone-service-repositories","text":"Create the service working directory mkdir -p ~/git/services cd ~/git/services Clone Assemblyline's services repositories Git via SSH Use SSH if you have your SSH id_rsa file configured to your GitHub account git clone git@github.com:CybercentreCanada/assemblyline-service-antivirus.git git clone git@github.com:CybercentreCanada/assemblyline-service-apkaye.git git clone git@github.com:CybercentreCanada/assemblyline-service-avclass.git git clone git@github.com:CybercentreCanada/assemblyline-service-beaver.git git clone git@github.com:CybercentreCanada/assemblyline-service-characterize.git git clone git@github.com:CybercentreCanada/assemblyline-service-configextractor.git git clone git@github.com:CybercentreCanada/assemblyline-service-cuckoo.git git clone git@github.com:CybercentreCanada/assemblyline-service-deobfuscripter.git git clone git@github.com:CybercentreCanada/assemblyline-service-emlparser.git git clone git@github.com:CybercentreCanada/assemblyline-service-espresso.git git clone git@github.com:CybercentreCanada/assemblyline-service-extract.git git clone git@github.com:CybercentreCanada/assemblyline-service-floss.git git clone git@github.com:CybercentreCanada/assemblyline-service-frankenstrings.git git clone git@github.com:CybercentreCanada/assemblyline-service-iparse.git git clone git@github.com:CybercentreCanada/assemblyline-service-metadefender.git git clone git@github.com:CybercentreCanada/assemblyline-service-metapeek.git git clone git@github.com:CybercentreCanada/assemblyline-service-oletools.git git clone git@github.com:CybercentreCanada/assemblyline-service-pdfid.git git clone git@github.com:CybercentreCanada/assemblyline-service-peepdf.git git clone git@github.com:CybercentreCanada/assemblyline-service-pefile.git git clone git@github.com:CybercentreCanada/assemblyline-service-pixaxe.git git clone git@github.com:CybercentreCanada/assemblyline-service-safelist.git git clone git@github.com:CybercentreCanada/assemblyline-service-sigma.git git clone git@github.com:CybercentreCanada/assemblyline-service-suricata.git git clone git@github.com:CybercentreCanada/assemblyline-service-swiffer.git git clone git@github.com:CybercentreCanada/assemblyline-service-torrentslicer.git git clone git@github.com:CybercentreCanada/assemblyline-service-unpacker.git git clone git@github.com:CybercentreCanada/assemblyline-service-unpacme.git git clone git@github.com:CybercentreCanada/assemblyline-service-vipermonkey.git git clone git@github.com:CybercentreCanada/assemblyline-service-virustotal-dynamic.git git clone git@github.com:CybercentreCanada/assemblyline-service-virustotal-static.git git clone git@github.com:CybercentreCanada/assemblyline-service-XLMMacroDeobfuscator.git git clone git@github.com:CybercentreCanada/assemblyline-service-yara.git Git via HTTPS Use HTTPS if you don't have your GitHub account configured with an SSH key git clone https://github.com/CybercentreCanada/assemblyline-service-antivirus.git git clone https://github.com/CybercentreCanada/assemblyline-service-apkaye.git git clone https://github.com/CybercentreCanada/assemblyline-service-avclass.git git clone https://github.com/CybercentreCanada/assemblyline-service-beaver.git git clone https://github.com/CybercentreCanada/assemblyline-service-characterize.git git clone https://github.com/CybercentreCanada/assemblyline-service-configextractor.git git clone https://github.com/CybercentreCanada/assemblyline-service-cuckoo.git git clone https://github.com/CybercentreCanada/assemblyline-service-deobfuscripter.git git clone https://github.com/CybercentreCanada/assemblyline-service-emlparser.git git clone https://github.com/CybercentreCanada/assemblyline-service-espresso.git git clone https://github.com/CybercentreCanada/assemblyline-service-extract.git git clone https://github.com/CybercentreCanada/assemblyline-service-floss.git git clone https://github.com/CybercentreCanada/assemblyline-service-frankenstrings.git git clone https://github.com/CybercentreCanada/assemblyline-service-iparse.git git clone https://github.com/CybercentreCanada/assemblyline-service-metadefender.git git clone https://github.com/CybercentreCanada/assemblyline-service-metapeek.git git clone https://github.com/CybercentreCanada/assemblyline-service-oletools.git git clone https://github.com/CybercentreCanada/assemblyline-service-pdfid.git git clone https://github.com/CybercentreCanada/assemblyline-service-peepdf.git git clone https://github.com/CybercentreCanada/assemblyline-service-pefile.git git clone https://github.com/CybercentreCanada/assemblyline-service-pixaxe.git git clone https://github.com/CybercentreCanada/assemblyline-service-safelist.git git clone https://github.com/CybercentreCanada/assemblyline-service-sigma.git git clone https://github.com/CybercentreCanada/assemblyline-service-suricata.git git clone https://github.com/CybercentreCanada/assemblyline-service-swiffer.git git clone https://github.com/CybercentreCanada/assemblyline-service-torrentslicer.git git clone https://github.com/CybercentreCanada/assemblyline-service-unpacker.git git clone https://github.com/CybercentreCanada/assemblyline-service-unpacme.git git clone https://github.com/CybercentreCanada/assemblyline-service-vipermonkey.git git clone https://github.com/CybercentreCanada/assemblyline-service-virustotal-dynamic.git git clone https://github.com/CybercentreCanada/assemblyline-service-virustotal-static.git git clone https://github.com/CybercentreCanada/assemblyline-service-XLMMacroDeobfuscator.git git clone https://github.com/CybercentreCanada/assemblyline-service-yara.git","title":"Clone service repositories"},{"location":"developer_manual/env/pycharm/local_development/#virtual-environment_1","text":"# Directly in the services source directory cd ~/git/services # Create the virtualenv python3.9 -m venv venv # Install Assemblyline packages from source in the services virtualenv ~/git/services/venv/bin/pip install -e ~/git/alv4/assemblyline-base ~/git/services/venv/bin/pip install -e ~/git/alv4/assemblyline-core ~/git/services/venv/bin/pip install -e ~/git/alv4/assemblyline-service-client ~/git/services/venv/bin/pip install -e ~/git/alv4/assemblyline-v4-service","title":"Virtual Environment"},{"location":"developer_manual/env/pycharm/local_development/#setup-pycharm-for-core","text":"Load PyCharm Choose whatever configuration option you want until the Welcome screen Click the Open button Choose the ~/git/alv4 directory Info Your Python interpreter shows as No Interpreter in the bottom right corner of the window, do the following: Click on it Click add interpreter Choose \"Existing Environment\" Click \"OK\"","title":"Setup PyCharm for core"},{"location":"developer_manual/env/pycharm/local_development/#setup-pycharm-for-service-optional","text":"From your core PyCharm window open the File menu then click Open Choose the ~/git/services directory Select New Window Info Your Python interpreter shows as No Interpreter in the bottom right corner of the window, do the following: Click on it Click add interpreter Choose \"Existing Environment\" Click \"OK\"","title":"Setup PyCharm for service (optional)"},{"location":"developer_manual/env/pycharm/local_development/#use-pycharm","text":"Now that your Local development VM is set up you should read the use PyCharm documentation to get you started.","title":"Use PyCharm"},{"location":"developer_manual/env/pycharm/remote_development/","text":"Remote development \u00b6 Warning To use this setup, we assume that you have a paid version of PyCharm ( Pycharm professional ) because we will be using features that are exclusive to the Professional version. If you don't, use the Local development setup instead. This document will show you how to set up your target virtual machine for remote development which means that you will run your IDE on your desktop and run the Assemblyline containers on the remote target VM. On the target VM \u00b6 Operating system \u00b6 For this document, we will assume that you are working on a fresh installation of Ubuntu 20.04 Server . Update target VM \u00b6 Make sure Ubuntu is running the latest software sudo apt update sudo apt dist-upgrade Reboot if needed sudo reboot Installing pre-requisite software \u00b6 Install SSH Daemon \u00b6 We need to make sure the remote target has an SSH daemon installed for remote debugging sudo apt update sudo apt install -y ssh Install Assemblyline APT dependencies \u00b6 sudo apt update sudo apt-get install -yy libfuzzy2 libmagic1 libldap-2.4-2 libsasl2-2 build-essential libffi-dev libfuzzy-dev libldap2-dev libsasl2-dev libssl-dev Install Python 3.9 \u00b6 Assemblyline 4 containers are now all built on Python 3.9 therefore we will install Python 3.9. sudo apt install -y software-properties-common sudo add-apt-repository -y ppa:deadsnakes/ppa sudo apt-get install -yy python3-venv python3.9 python3.9-dev python3.9-venv libffi7 Installing Docker \u00b6 Follow these simple commands to get Docker running on your machine: # Add Docker repository sudo apt-get update sudo apt-get install -y apt-transport-https ca-certificates curl gnupg-agent software-properties-common curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - sudo apt-key fingerprint 0EBFCD88 sudo add-apt-repository \"deb [arch=amd64] https://download.docker.com/linux/ubuntu $( lsb_release -cs ) stable\" # Install Docker sudo apt-get install -y docker-ce docker-ce-cli containerd.io # Test Docker installation sudo docker run hello-world Installing docker-compose \u00b6 Installing docker-compose is done the same way on all Linux distributions. Follow these simple instructions: # Install docker-compose sudo curl -L \"https://github.com/docker/compose/releases/download/1.28.5/docker-compose- $( uname -s ) - $( uname -m ) \" -o /usr/local/bin/docker-compose sudo chmod +x /usr/local/bin/docker-compose # Test docker-compose installation docker-compose --version Securing Docker for remote access \u00b6 We are going to make your Docker server accessible from the internet. To make it secure, we need to enable TLS authentication in the Docker daemon. Anywhere that you see assemblyline.local, you can change that value to your own DNS name. If you're planning on using an IP, you'll have to set a static IP to the remote VM because your certificate (cert) will only allow connections to that IP. # Create a cert directory mkdir ~/certs cd ~/certs # Create a CA (Remember the password you've set) openssl genrsa -aes256 -out ca-key.pem 4096 # Create a certificate-signing request (ignore the .rng error) openssl req -new -x509 -days 365 -key ca-key.pem -sha256 -out ca.pem -subj \"/C=CA/ST=Ontario/L=Ottawa/O=CCCS/CN=assemblyline.local\" # Creating the server public/private key openssl genrsa -out server-key.pem 4096 openssl req -subj \"/CN=assemblyline.local\" -sha256 -new -key server-key.pem -out server.csr echo subjectAltName = DNS:assemblyline.local,IP: ` ip route get 8 .8.8.8 | grep 8 .8.8.8 | awk '{ print $7 }' ` ,IP:127.0.0.1 >> extfile.cnf echo extendedKeyUsage = serverAuth >> extfile.cnf openssl x509 -req -days 365 -sha256 -in server.csr -CA ca.pem -CAkey ca-key.pem -CAcreateserial -out server-cert.pem -extfile extfile.cnf # Creating the client public/private key openssl genrsa -out key.pem 4096 openssl req -subj '/CN=client' -new -key key.pem -out client.csr echo extendedKeyUsage = clientAuth > extfile-client.cnf openssl x509 -req -days 365 -sha256 -in client.csr -CA ca.pem -CAkey ca-key.pem -CAcreateserial -out cert.pem -extfile extfile-client.cnf # Remove unnecessary files rm -v client.csr server.csr extfile.cnf extfile-client.cnf # Change private and public key permissions chmod -v 0400 ca-key.pem key.pem server-key.pem chmod -v 0444 ca.pem server-cert.pem cert.pem # Moving server certs to their permanent location sudo mkdir -p /etc/docker/certs sudo mv server*.pem /etc/docker/certs sudo cp ca.pem /etc/docker/certs # Add system.d override configuration for Docker to start the TCP with TLS port sudo mkdir -p /etc/systemd/system/docker.service.d/ sudo su -c 'echo \"# /etc/systemd/system/docker.service.d/override.conf [Service] ExecStart= ExecStart=/usr/bin/dockerd -H fd:// --tlsverify --tlscacert=/etc/docker/certs/ca.pem --tlscert=/etc/docker/certs/server-cert.pem --tlskey=/etc/docker/certs/server-key.pem -H 0.0.0.0:2376\" >> /etc/systemd/system/docker.service.d/override.conf' sudo systemctl daemon-reload sudo systemctl restart docker # Test the TLS connection with curl curl https://127.0.0.1:2376/images/json --cert ~/certs/cert.pem --key ~/certs/key.pem --cacert ~/certs/ca.pem # Create an archive with the client certs tar czvf certs.tgz ca.pem cert.pem key.pem The archive file ~/certs/certs.tgz will have to be transferred to your desktop. We will use its content to log into the Docker daemon from your desktop. Adding Assemblyline specific configuration \u00b6 Assemblyline folders \u00b6 Because Assemblyline uses its own set of folders inside the core, service-server, and UI container, we have to create the same folder structure here so that we can run the components in debug mode. sudo mkdir -p ~/git sudo mkdir -p /etc/assemblyline sudo mkdir -p /var/cache/assemblyline sudo mkdir -p /var/lib/assemblyline sudo mkdir -p /var/log/assemblyline sudo chown $USER /etc/assemblyline sudo chown $USER /var/cache/assemblyline sudo chown $USER /var/lib/assemblyline sudo chown $USER /var/log/assemblyline Assemblyline dev configuration files \u00b6 Here we will create configuration files that match the default dev docker-compose configuration files so that we can swap any of the components to the one that is being debugged. echo \"enforce: true\" > /etc/assemblyline/classification.yml echo \" auth: internal: enabled: true core: alerter: delay: 0 metrics: apm_server: server_url: http://localhost:8200/ elasticsearch: hosts: [http://elastic:devpass@localhost] datastore: ilm: indexes: alert: unit: m error: unit: m file: unit: m result: unit: m submission: unit: m filestore: cache: - file:///var/cache/assemblyline/ logging: log_level: INFO log_as_json: false ui: audit: false debug: false enforce_quota: false fqdn: `ip route get 8.8.8.8 | grep 8.8.8.8 | awk '{ print $7 }'`.nip.io \" > /etc/assemblyline/config.yml Tip As you can see in the last command we are setting the FQDN to YOUR_IP.nip.io. NIP.IO is a service that will resolve the first part of the domain YOUR_IP .nip.io to its IP value. We use this to fake DNS when there are none. This is especially useful for oAuth because some providers are forbidding redirect URLs to IPs. You can also replace the FQDN with your own DNS name if you have one. Setup Python Virtual Environments \u00b6 We will make two Python virtual environments: One for the core components One for services That should be enough to cover most cases. If a service has conflicting dependencies with another, I suggest you create a separate virtualenv for it when you try to debug it. The core components should all be fine in the same environment. Setting up Core Virtualenv \u00b6 # Make sure the venv directory exists and we are in it mkdir -p ~/venv cd ~/venv # Create the virtualenv python3.9 -m venv core # Install Assemblyline packages with their test dependencies ~/venv/core/bin/pip install assemblyline [ test ] assemblyline-core [ test ] assemblyline-service-server [ test ] assemblyline-ui [ test ] # Remove Assemblyline packages because we will use the live code ~/venv/core/bin/pip uninstall -y assemblyline assemblyline-core assemblyline-service-server assemblyline-ui Setting up Service Virtualenv (optional) \u00b6 # Make sure the venv directory exists and we are in it mkdir -p ~/venv cd ~/venv # Create the virtualenv python3.9 -m venv services # Install Assemblyline Python client ~/venv/services/bin/pip install assemblyline-client # Install Assemblyline service packages ~/venv/services/bin/pip install assemblyline-service-client assemblyline-v4-service # Remove Assemblyline packages because we will use the live code ~/venv/services/bin/pip uninstall -y assemblyline assemblyline-core assemblyline-service-client assemblyline-v4-service On your desktop \u00b6 We are now done setting up the target VM. For the rest of the instructions, we will mainly setup your PyCharm IDE to interface with the target VM. Get your Docker certs and install them \u00b6 mkdir -p ~/docker_certs cd ~/docker_certs scp USER_OF_TARGET_VM@IP_OF_TARGET_VM:certs/certs.tgz ~/docker_certs/ tar zxvf certs.tgz rm certs.tgz Install PyCharm \u00b6 You can download PyCharm Professional directly from JetBrains 's website but if your desktop is running Ubuntu 20.04, you can just install it with snap : sudo snap install --classic pycharm-professional Install Git \u00b6 You can get Git directly from GIT 's website but if your desktop is running Ubuntu 20.04 you can just install it with APT : sudo apt install -y git Tip You should add your desktop SSH keys to your GitHub account to use Git via SSH. Follow these instructions to do so: GitHub Help Clone repositories \u00b6 Core components \u00b6 Create the core working directory mkdir -p ~/git/alv4 cd ~/git/alv4 Clone Assemblyline's repositories Git via SSH Use SSH if you have your SSH id_rsa file configured to your GitHub account git clone git@github.com:CybercentreCanada/assemblyline-base.git git clone git@github.com:CybercentreCanada/assemblyline-core.git git clone git@github.com:CybercentreCanada/assemblyline-service-client.git git clone git@github.com:CybercentreCanada/assemblyline-service-server.git git clone git@github.com:CybercentreCanada/assemblyline-ui.git git clone git@github.com:CybercentreCanada/assemblyline-v4-service.git Git via HTTPS Use HTTPS if you don't have your GitHub account configured with an SSH key git clone https://github.com/CybercentreCanada/assemblyline-base.git git clone https://github.com/CybercentreCanada/assemblyline-core.git git clone https://github.com/CybercentreCanada/assemblyline-service-client.git git clone https://github.com/CybercentreCanada/assemblyline-service-server.git git clone https://github.com/CybercentreCanada/assemblyline-ui.git git clone https://github.com/CybercentreCanada/assemblyline-v4-service.git Services (optional) \u00b6 Create the service working directory mkdir -p ~/git/services cd ~/git/services Clone Assemblyline's services repositories Git via SSH Use SSH if you have your SSH id_rsa file configured to your GitHub account git clone git@github.com:CybercentreCanada/assemblyline-service-antivirus.git git clone git@github.com:CybercentreCanada/assemblyline-service-apkaye.git git clone git@github.com:CybercentreCanada/assemblyline-service-avclass.git git clone git@github.com:CybercentreCanada/assemblyline-service-beaver.git git clone git@github.com:CybercentreCanada/assemblyline-service-characterize.git git clone git@github.com:CybercentreCanada/assemblyline-service-configextractor.git git clone git@github.com:CybercentreCanada/assemblyline-service-cuckoo.git git clone git@github.com:CybercentreCanada/assemblyline-service-deobfuscripter.git git clone git@github.com:CybercentreCanada/assemblyline-service-emlparser.git git clone git@github.com:CybercentreCanada/assemblyline-service-espresso.git git clone git@github.com:CybercentreCanada/assemblyline-service-extract.git git clone git@github.com:CybercentreCanada/assemblyline-service-floss.git git clone git@github.com:CybercentreCanada/assemblyline-service-frankenstrings.git git clone git@github.com:CybercentreCanada/assemblyline-service-iparse.git git clone git@github.com:CybercentreCanada/assemblyline-service-metadefender.git git clone git@github.com:CybercentreCanada/assemblyline-service-metapeek.git git clone git@github.com:CybercentreCanada/assemblyline-service-oletools.git git clone git@github.com:CybercentreCanada/assemblyline-service-pdfid.git git clone git@github.com:CybercentreCanada/assemblyline-service-peepdf.git git clone git@github.com:CybercentreCanada/assemblyline-service-pefile.git git clone git@github.com:CybercentreCanada/assemblyline-service-pixaxe.git git clone git@github.com:CybercentreCanada/assemblyline-service-safelist.git git clone git@github.com:CybercentreCanada/assemblyline-service-sigma.git git clone git@github.com:CybercentreCanada/assemblyline-service-suricata.git git clone git@github.com:CybercentreCanada/assemblyline-service-swiffer.git git clone git@github.com:CybercentreCanada/assemblyline-service-torrentslicer.git git clone git@github.com:CybercentreCanada/assemblyline-service-unpacker.git git clone git@github.com:CybercentreCanada/assemblyline-service-unpacme.git git clone git@github.com:CybercentreCanada/assemblyline-service-vipermonkey.git git clone git@github.com:CybercentreCanada/assemblyline-service-virustotal-dynamic.git git clone git@github.com:CybercentreCanada/assemblyline-service-virustotal-static.git git clone git@github.com:CybercentreCanada/assemblyline-service-XLMMacroDeobfuscator.git git clone git@github.com:CybercentreCanada/assemblyline-service-yara.git Git via HTTPS Use HTTPS if you don't have your GitHub account configured with an SSH key git clone https://github.com/CybercentreCanada/assemblyline-service-antivirus.git git clone https://github.com/CybercentreCanada/assemblyline-service-apkaye.git git clone https://github.com/CybercentreCanada/assemblyline-service-avclass.git git clone https://github.com/CybercentreCanada/assemblyline-service-beaver.git git clone https://github.com/CybercentreCanada/assemblyline-service-characterize.git git clone https://github.com/CybercentreCanada/assemblyline-service-configextractor.git git clone https://github.com/CybercentreCanada/assemblyline-service-cuckoo.git git clone https://github.com/CybercentreCanada/assemblyline-service-deobfuscripter.git git clone https://github.com/CybercentreCanada/assemblyline-service-emlparser.git git clone https://github.com/CybercentreCanada/assemblyline-service-espresso.git git clone https://github.com/CybercentreCanada/assemblyline-service-extract.git git clone https://github.com/CybercentreCanada/assemblyline-service-floss.git git clone https://github.com/CybercentreCanada/assemblyline-service-frankenstrings.git git clone https://github.com/CybercentreCanada/assemblyline-service-iparse.git git clone https://github.com/CybercentreCanada/assemblyline-service-metadefender.git git clone https://github.com/CybercentreCanada/assemblyline-service-metapeek.git git clone https://github.com/CybercentreCanada/assemblyline-service-oletools.git git clone https://github.com/CybercentreCanada/assemblyline-service-pdfid.git git clone https://github.com/CybercentreCanada/assemblyline-service-peepdf.git git clone https://github.com/CybercentreCanada/assemblyline-service-pefile.git git clone https://github.com/CybercentreCanada/assemblyline-service-pixaxe.git git clone https://github.com/CybercentreCanada/assemblyline-service-safelist.git git clone https://github.com/CybercentreCanada/assemblyline-service-sigma.git git clone https://github.com/CybercentreCanada/assemblyline-service-suricata.git git clone https://github.com/CybercentreCanada/assemblyline-service-swiffer.git git clone https://github.com/CybercentreCanada/assemblyline-service-torrentslicer.git git clone https://github.com/CybercentreCanada/assemblyline-service-unpacker.git git clone https://github.com/CybercentreCanada/assemblyline-service-unpacme.git git clone https://github.com/CybercentreCanada/assemblyline-service-vipermonkey.git git clone https://github.com/CybercentreCanada/assemblyline-service-virustotal-dynamic.git git clone https://github.com/CybercentreCanada/assemblyline-service-virustotal-static.git git clone https://github.com/CybercentreCanada/assemblyline-service-XLMMacroDeobfuscator.git git clone https://github.com/CybercentreCanada/assemblyline-service-yara.git Setup PyCharm for core \u00b6 Start with loading the core directory in Pycharm: Load core folder Load Pycharm Professional Choose whatever configuration option you want until the Welcome screen Click the Open button Choose the ~/git/alv4 directory The setup of the remote deployment interpreter: Setup core remote interpreter Click Files -> Settings Select Project: alv4 -> Python Interpreter Click the cog wheel on the top right -> Add Select SSH Interpreter -> New Configuration Host: IP or DNS name of your target VM Username: username of the user on the target VM Port: 22 unless you changed it... Click Next Put your target VM password in the box, check Save password , and click Next In the next window, do the following: For the interpreter box , click the little folder and select your core venv ( /home/YOUR_TARGET_USER/venv/core/bin/python3.9 ) For the Sync folders box, click the little folder and for the remote path set the path to: /home/YOUR_TARGET_USER/git/alv4 then click OK (ensure target directory has write permissions for all users) Make sure Automatically upload files to the server is checked Make sure Execute code using this interpreter with root privileges via sudo is checked Hit Finish Click Ok Let it load the interpreter and do the transfers Finally link Docker for remote management: Setup Docker remote management Click Files -> Settings Select build, Execution, Deployment -> Docker Click the little + on top left Select TCP Socket In engine API URL put: https://TARGET_VM_IP:2376 In Certificates folder, click the little folder and browse to ~/docker_certs directory Click OK Setup PyCharm for service (optional) \u00b6 Start with loading the core directory in PyCharm: Load services folder From your core PyCharm window open the File menu then click Open Choose the ~/git/services directory Select New Window The setup of the remote deployment interpreter: Setup services remote interpreter Click Files -> Settings Select Project: services -> Python Interpreter Click the cog wheel on the top right -> Add Select SSH Interpreter -> New Configuration Host: IP or DNS name of your target VM Username: username of the user on the target VM Port: 22 unless you changed it... Click Next Put your target VM password in the box, check Save password , and click Next In the next window, do the following: For the interpreter box , click the little folder and select your core venv ( /home/YOUR_TARGET_USER/venv/services/bin/python3.9 ) For the Sync folders box, click the little folder and for the remote path set the path to: /home/YOUR_TARGET_USER/git/services then click OK (ensure target directory has write permissions for all users) Make sure Automatically upload files to the server is checked Make sure Execute code using this interpreter with root privileges via sudo is checked Hit Finish Click Ok Let it load the interpreter and do the transfers Use Pycharm \u00b6 Now that your remote development VM is set up you should read the use PyCharm documentation to get yourself started.","title":"Remote development"},{"location":"developer_manual/env/pycharm/remote_development/#remote-development","text":"Warning To use this setup, we assume that you have a paid version of PyCharm ( Pycharm professional ) because we will be using features that are exclusive to the Professional version. If you don't, use the Local development setup instead. This document will show you how to set up your target virtual machine for remote development which means that you will run your IDE on your desktop and run the Assemblyline containers on the remote target VM.","title":"Remote development"},{"location":"developer_manual/env/pycharm/remote_development/#on-the-target-vm","text":"","title":"On the target VM"},{"location":"developer_manual/env/pycharm/remote_development/#operating-system","text":"For this document, we will assume that you are working on a fresh installation of Ubuntu 20.04 Server .","title":"Operating system"},{"location":"developer_manual/env/pycharm/remote_development/#update-target-vm","text":"Make sure Ubuntu is running the latest software sudo apt update sudo apt dist-upgrade Reboot if needed sudo reboot","title":"Update target VM"},{"location":"developer_manual/env/pycharm/remote_development/#installing-pre-requisite-software","text":"","title":"Installing pre-requisite software"},{"location":"developer_manual/env/pycharm/remote_development/#install-ssh-daemon","text":"We need to make sure the remote target has an SSH daemon installed for remote debugging sudo apt update sudo apt install -y ssh","title":"Install SSH Daemon"},{"location":"developer_manual/env/pycharm/remote_development/#install-assemblyline-apt-dependencies","text":"sudo apt update sudo apt-get install -yy libfuzzy2 libmagic1 libldap-2.4-2 libsasl2-2 build-essential libffi-dev libfuzzy-dev libldap2-dev libsasl2-dev libssl-dev","title":"Install Assemblyline APT dependencies"},{"location":"developer_manual/env/pycharm/remote_development/#install-python-39","text":"Assemblyline 4 containers are now all built on Python 3.9 therefore we will install Python 3.9. sudo apt install -y software-properties-common sudo add-apt-repository -y ppa:deadsnakes/ppa sudo apt-get install -yy python3-venv python3.9 python3.9-dev python3.9-venv libffi7","title":"Install Python 3.9"},{"location":"developer_manual/env/pycharm/remote_development/#installing-docker","text":"Follow these simple commands to get Docker running on your machine: # Add Docker repository sudo apt-get update sudo apt-get install -y apt-transport-https ca-certificates curl gnupg-agent software-properties-common curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - sudo apt-key fingerprint 0EBFCD88 sudo add-apt-repository \"deb [arch=amd64] https://download.docker.com/linux/ubuntu $( lsb_release -cs ) stable\" # Install Docker sudo apt-get install -y docker-ce docker-ce-cli containerd.io # Test Docker installation sudo docker run hello-world","title":"Installing Docker"},{"location":"developer_manual/env/pycharm/remote_development/#installing-docker-compose","text":"Installing docker-compose is done the same way on all Linux distributions. Follow these simple instructions: # Install docker-compose sudo curl -L \"https://github.com/docker/compose/releases/download/1.28.5/docker-compose- $( uname -s ) - $( uname -m ) \" -o /usr/local/bin/docker-compose sudo chmod +x /usr/local/bin/docker-compose # Test docker-compose installation docker-compose --version","title":"Installing docker-compose"},{"location":"developer_manual/env/pycharm/remote_development/#securing-docker-for-remote-access","text":"We are going to make your Docker server accessible from the internet. To make it secure, we need to enable TLS authentication in the Docker daemon. Anywhere that you see assemblyline.local, you can change that value to your own DNS name. If you're planning on using an IP, you'll have to set a static IP to the remote VM because your certificate (cert) will only allow connections to that IP. # Create a cert directory mkdir ~/certs cd ~/certs # Create a CA (Remember the password you've set) openssl genrsa -aes256 -out ca-key.pem 4096 # Create a certificate-signing request (ignore the .rng error) openssl req -new -x509 -days 365 -key ca-key.pem -sha256 -out ca.pem -subj \"/C=CA/ST=Ontario/L=Ottawa/O=CCCS/CN=assemblyline.local\" # Creating the server public/private key openssl genrsa -out server-key.pem 4096 openssl req -subj \"/CN=assemblyline.local\" -sha256 -new -key server-key.pem -out server.csr echo subjectAltName = DNS:assemblyline.local,IP: ` ip route get 8 .8.8.8 | grep 8 .8.8.8 | awk '{ print $7 }' ` ,IP:127.0.0.1 >> extfile.cnf echo extendedKeyUsage = serverAuth >> extfile.cnf openssl x509 -req -days 365 -sha256 -in server.csr -CA ca.pem -CAkey ca-key.pem -CAcreateserial -out server-cert.pem -extfile extfile.cnf # Creating the client public/private key openssl genrsa -out key.pem 4096 openssl req -subj '/CN=client' -new -key key.pem -out client.csr echo extendedKeyUsage = clientAuth > extfile-client.cnf openssl x509 -req -days 365 -sha256 -in client.csr -CA ca.pem -CAkey ca-key.pem -CAcreateserial -out cert.pem -extfile extfile-client.cnf # Remove unnecessary files rm -v client.csr server.csr extfile.cnf extfile-client.cnf # Change private and public key permissions chmod -v 0400 ca-key.pem key.pem server-key.pem chmod -v 0444 ca.pem server-cert.pem cert.pem # Moving server certs to their permanent location sudo mkdir -p /etc/docker/certs sudo mv server*.pem /etc/docker/certs sudo cp ca.pem /etc/docker/certs # Add system.d override configuration for Docker to start the TCP with TLS port sudo mkdir -p /etc/systemd/system/docker.service.d/ sudo su -c 'echo \"# /etc/systemd/system/docker.service.d/override.conf [Service] ExecStart= ExecStart=/usr/bin/dockerd -H fd:// --tlsverify --tlscacert=/etc/docker/certs/ca.pem --tlscert=/etc/docker/certs/server-cert.pem --tlskey=/etc/docker/certs/server-key.pem -H 0.0.0.0:2376\" >> /etc/systemd/system/docker.service.d/override.conf' sudo systemctl daemon-reload sudo systemctl restart docker # Test the TLS connection with curl curl https://127.0.0.1:2376/images/json --cert ~/certs/cert.pem --key ~/certs/key.pem --cacert ~/certs/ca.pem # Create an archive with the client certs tar czvf certs.tgz ca.pem cert.pem key.pem The archive file ~/certs/certs.tgz will have to be transferred to your desktop. We will use its content to log into the Docker daemon from your desktop.","title":"Securing Docker for remote access"},{"location":"developer_manual/env/pycharm/remote_development/#adding-assemblyline-specific-configuration","text":"","title":"Adding Assemblyline specific configuration"},{"location":"developer_manual/env/pycharm/remote_development/#assemblyline-folders","text":"Because Assemblyline uses its own set of folders inside the core, service-server, and UI container, we have to create the same folder structure here so that we can run the components in debug mode. sudo mkdir -p ~/git sudo mkdir -p /etc/assemblyline sudo mkdir -p /var/cache/assemblyline sudo mkdir -p /var/lib/assemblyline sudo mkdir -p /var/log/assemblyline sudo chown $USER /etc/assemblyline sudo chown $USER /var/cache/assemblyline sudo chown $USER /var/lib/assemblyline sudo chown $USER /var/log/assemblyline","title":"Assemblyline folders"},{"location":"developer_manual/env/pycharm/remote_development/#assemblyline-dev-configuration-files","text":"Here we will create configuration files that match the default dev docker-compose configuration files so that we can swap any of the components to the one that is being debugged. echo \"enforce: true\" > /etc/assemblyline/classification.yml echo \" auth: internal: enabled: true core: alerter: delay: 0 metrics: apm_server: server_url: http://localhost:8200/ elasticsearch: hosts: [http://elastic:devpass@localhost] datastore: ilm: indexes: alert: unit: m error: unit: m file: unit: m result: unit: m submission: unit: m filestore: cache: - file:///var/cache/assemblyline/ logging: log_level: INFO log_as_json: false ui: audit: false debug: false enforce_quota: false fqdn: `ip route get 8.8.8.8 | grep 8.8.8.8 | awk '{ print $7 }'`.nip.io \" > /etc/assemblyline/config.yml Tip As you can see in the last command we are setting the FQDN to YOUR_IP.nip.io. NIP.IO is a service that will resolve the first part of the domain YOUR_IP .nip.io to its IP value. We use this to fake DNS when there are none. This is especially useful for oAuth because some providers are forbidding redirect URLs to IPs. You can also replace the FQDN with your own DNS name if you have one.","title":"Assemblyline dev configuration files"},{"location":"developer_manual/env/pycharm/remote_development/#setup-python-virtual-environments","text":"We will make two Python virtual environments: One for the core components One for services That should be enough to cover most cases. If a service has conflicting dependencies with another, I suggest you create a separate virtualenv for it when you try to debug it. The core components should all be fine in the same environment.","title":"Setup Python Virtual Environments"},{"location":"developer_manual/env/pycharm/remote_development/#setting-up-core-virtualenv","text":"# Make sure the venv directory exists and we are in it mkdir -p ~/venv cd ~/venv # Create the virtualenv python3.9 -m venv core # Install Assemblyline packages with their test dependencies ~/venv/core/bin/pip install assemblyline [ test ] assemblyline-core [ test ] assemblyline-service-server [ test ] assemblyline-ui [ test ] # Remove Assemblyline packages because we will use the live code ~/venv/core/bin/pip uninstall -y assemblyline assemblyline-core assemblyline-service-server assemblyline-ui","title":"Setting up Core Virtualenv"},{"location":"developer_manual/env/pycharm/remote_development/#setting-up-service-virtualenv-optional","text":"# Make sure the venv directory exists and we are in it mkdir -p ~/venv cd ~/venv # Create the virtualenv python3.9 -m venv services # Install Assemblyline Python client ~/venv/services/bin/pip install assemblyline-client # Install Assemblyline service packages ~/venv/services/bin/pip install assemblyline-service-client assemblyline-v4-service # Remove Assemblyline packages because we will use the live code ~/venv/services/bin/pip uninstall -y assemblyline assemblyline-core assemblyline-service-client assemblyline-v4-service","title":"Setting up Service Virtualenv (optional)"},{"location":"developer_manual/env/pycharm/remote_development/#on-your-desktop","text":"We are now done setting up the target VM. For the rest of the instructions, we will mainly setup your PyCharm IDE to interface with the target VM.","title":"On your desktop"},{"location":"developer_manual/env/pycharm/remote_development/#get-your-docker-certs-and-install-them","text":"mkdir -p ~/docker_certs cd ~/docker_certs scp USER_OF_TARGET_VM@IP_OF_TARGET_VM:certs/certs.tgz ~/docker_certs/ tar zxvf certs.tgz rm certs.tgz","title":"Get your Docker certs and install them"},{"location":"developer_manual/env/pycharm/remote_development/#install-pycharm","text":"You can download PyCharm Professional directly from JetBrains 's website but if your desktop is running Ubuntu 20.04, you can just install it with snap : sudo snap install --classic pycharm-professional","title":"Install PyCharm"},{"location":"developer_manual/env/pycharm/remote_development/#install-git","text":"You can get Git directly from GIT 's website but if your desktop is running Ubuntu 20.04 you can just install it with APT : sudo apt install -y git Tip You should add your desktop SSH keys to your GitHub account to use Git via SSH. Follow these instructions to do so: GitHub Help","title":"Install Git"},{"location":"developer_manual/env/pycharm/remote_development/#clone-repositories","text":"","title":"Clone repositories"},{"location":"developer_manual/env/pycharm/remote_development/#core-components","text":"Create the core working directory mkdir -p ~/git/alv4 cd ~/git/alv4 Clone Assemblyline's repositories Git via SSH Use SSH if you have your SSH id_rsa file configured to your GitHub account git clone git@github.com:CybercentreCanada/assemblyline-base.git git clone git@github.com:CybercentreCanada/assemblyline-core.git git clone git@github.com:CybercentreCanada/assemblyline-service-client.git git clone git@github.com:CybercentreCanada/assemblyline-service-server.git git clone git@github.com:CybercentreCanada/assemblyline-ui.git git clone git@github.com:CybercentreCanada/assemblyline-v4-service.git Git via HTTPS Use HTTPS if you don't have your GitHub account configured with an SSH key git clone https://github.com/CybercentreCanada/assemblyline-base.git git clone https://github.com/CybercentreCanada/assemblyline-core.git git clone https://github.com/CybercentreCanada/assemblyline-service-client.git git clone https://github.com/CybercentreCanada/assemblyline-service-server.git git clone https://github.com/CybercentreCanada/assemblyline-ui.git git clone https://github.com/CybercentreCanada/assemblyline-v4-service.git","title":"Core components"},{"location":"developer_manual/env/pycharm/remote_development/#services-optional","text":"Create the service working directory mkdir -p ~/git/services cd ~/git/services Clone Assemblyline's services repositories Git via SSH Use SSH if you have your SSH id_rsa file configured to your GitHub account git clone git@github.com:CybercentreCanada/assemblyline-service-antivirus.git git clone git@github.com:CybercentreCanada/assemblyline-service-apkaye.git git clone git@github.com:CybercentreCanada/assemblyline-service-avclass.git git clone git@github.com:CybercentreCanada/assemblyline-service-beaver.git git clone git@github.com:CybercentreCanada/assemblyline-service-characterize.git git clone git@github.com:CybercentreCanada/assemblyline-service-configextractor.git git clone git@github.com:CybercentreCanada/assemblyline-service-cuckoo.git git clone git@github.com:CybercentreCanada/assemblyline-service-deobfuscripter.git git clone git@github.com:CybercentreCanada/assemblyline-service-emlparser.git git clone git@github.com:CybercentreCanada/assemblyline-service-espresso.git git clone git@github.com:CybercentreCanada/assemblyline-service-extract.git git clone git@github.com:CybercentreCanada/assemblyline-service-floss.git git clone git@github.com:CybercentreCanada/assemblyline-service-frankenstrings.git git clone git@github.com:CybercentreCanada/assemblyline-service-iparse.git git clone git@github.com:CybercentreCanada/assemblyline-service-metadefender.git git clone git@github.com:CybercentreCanada/assemblyline-service-metapeek.git git clone git@github.com:CybercentreCanada/assemblyline-service-oletools.git git clone git@github.com:CybercentreCanada/assemblyline-service-pdfid.git git clone git@github.com:CybercentreCanada/assemblyline-service-peepdf.git git clone git@github.com:CybercentreCanada/assemblyline-service-pefile.git git clone git@github.com:CybercentreCanada/assemblyline-service-pixaxe.git git clone git@github.com:CybercentreCanada/assemblyline-service-safelist.git git clone git@github.com:CybercentreCanada/assemblyline-service-sigma.git git clone git@github.com:CybercentreCanada/assemblyline-service-suricata.git git clone git@github.com:CybercentreCanada/assemblyline-service-swiffer.git git clone git@github.com:CybercentreCanada/assemblyline-service-torrentslicer.git git clone git@github.com:CybercentreCanada/assemblyline-service-unpacker.git git clone git@github.com:CybercentreCanada/assemblyline-service-unpacme.git git clone git@github.com:CybercentreCanada/assemblyline-service-vipermonkey.git git clone git@github.com:CybercentreCanada/assemblyline-service-virustotal-dynamic.git git clone git@github.com:CybercentreCanada/assemblyline-service-virustotal-static.git git clone git@github.com:CybercentreCanada/assemblyline-service-XLMMacroDeobfuscator.git git clone git@github.com:CybercentreCanada/assemblyline-service-yara.git Git via HTTPS Use HTTPS if you don't have your GitHub account configured with an SSH key git clone https://github.com/CybercentreCanada/assemblyline-service-antivirus.git git clone https://github.com/CybercentreCanada/assemblyline-service-apkaye.git git clone https://github.com/CybercentreCanada/assemblyline-service-avclass.git git clone https://github.com/CybercentreCanada/assemblyline-service-beaver.git git clone https://github.com/CybercentreCanada/assemblyline-service-characterize.git git clone https://github.com/CybercentreCanada/assemblyline-service-configextractor.git git clone https://github.com/CybercentreCanada/assemblyline-service-cuckoo.git git clone https://github.com/CybercentreCanada/assemblyline-service-deobfuscripter.git git clone https://github.com/CybercentreCanada/assemblyline-service-emlparser.git git clone https://github.com/CybercentreCanada/assemblyline-service-espresso.git git clone https://github.com/CybercentreCanada/assemblyline-service-extract.git git clone https://github.com/CybercentreCanada/assemblyline-service-floss.git git clone https://github.com/CybercentreCanada/assemblyline-service-frankenstrings.git git clone https://github.com/CybercentreCanada/assemblyline-service-iparse.git git clone https://github.com/CybercentreCanada/assemblyline-service-metadefender.git git clone https://github.com/CybercentreCanada/assemblyline-service-metapeek.git git clone https://github.com/CybercentreCanada/assemblyline-service-oletools.git git clone https://github.com/CybercentreCanada/assemblyline-service-pdfid.git git clone https://github.com/CybercentreCanada/assemblyline-service-peepdf.git git clone https://github.com/CybercentreCanada/assemblyline-service-pefile.git git clone https://github.com/CybercentreCanada/assemblyline-service-pixaxe.git git clone https://github.com/CybercentreCanada/assemblyline-service-safelist.git git clone https://github.com/CybercentreCanada/assemblyline-service-sigma.git git clone https://github.com/CybercentreCanada/assemblyline-service-suricata.git git clone https://github.com/CybercentreCanada/assemblyline-service-swiffer.git git clone https://github.com/CybercentreCanada/assemblyline-service-torrentslicer.git git clone https://github.com/CybercentreCanada/assemblyline-service-unpacker.git git clone https://github.com/CybercentreCanada/assemblyline-service-unpacme.git git clone https://github.com/CybercentreCanada/assemblyline-service-vipermonkey.git git clone https://github.com/CybercentreCanada/assemblyline-service-virustotal-dynamic.git git clone https://github.com/CybercentreCanada/assemblyline-service-virustotal-static.git git clone https://github.com/CybercentreCanada/assemblyline-service-XLMMacroDeobfuscator.git git clone https://github.com/CybercentreCanada/assemblyline-service-yara.git","title":"Services (optional)"},{"location":"developer_manual/env/pycharm/remote_development/#setup-pycharm-for-core","text":"Start with loading the core directory in Pycharm: Load core folder Load Pycharm Professional Choose whatever configuration option you want until the Welcome screen Click the Open button Choose the ~/git/alv4 directory The setup of the remote deployment interpreter: Setup core remote interpreter Click Files -> Settings Select Project: alv4 -> Python Interpreter Click the cog wheel on the top right -> Add Select SSH Interpreter -> New Configuration Host: IP or DNS name of your target VM Username: username of the user on the target VM Port: 22 unless you changed it... Click Next Put your target VM password in the box, check Save password , and click Next In the next window, do the following: For the interpreter box , click the little folder and select your core venv ( /home/YOUR_TARGET_USER/venv/core/bin/python3.9 ) For the Sync folders box, click the little folder and for the remote path set the path to: /home/YOUR_TARGET_USER/git/alv4 then click OK (ensure target directory has write permissions for all users) Make sure Automatically upload files to the server is checked Make sure Execute code using this interpreter with root privileges via sudo is checked Hit Finish Click Ok Let it load the interpreter and do the transfers Finally link Docker for remote management: Setup Docker remote management Click Files -> Settings Select build, Execution, Deployment -> Docker Click the little + on top left Select TCP Socket In engine API URL put: https://TARGET_VM_IP:2376 In Certificates folder, click the little folder and browse to ~/docker_certs directory Click OK","title":"Setup PyCharm for core"},{"location":"developer_manual/env/pycharm/remote_development/#setup-pycharm-for-service-optional","text":"Start with loading the core directory in PyCharm: Load services folder From your core PyCharm window open the File menu then click Open Choose the ~/git/services directory Select New Window The setup of the remote deployment interpreter: Setup services remote interpreter Click Files -> Settings Select Project: services -> Python Interpreter Click the cog wheel on the top right -> Add Select SSH Interpreter -> New Configuration Host: IP or DNS name of your target VM Username: username of the user on the target VM Port: 22 unless you changed it... Click Next Put your target VM password in the box, check Save password , and click Next In the next window, do the following: For the interpreter box , click the little folder and select your core venv ( /home/YOUR_TARGET_USER/venv/services/bin/python3.9 ) For the Sync folders box, click the little folder and for the remote path set the path to: /home/YOUR_TARGET_USER/git/services then click OK (ensure target directory has write permissions for all users) Make sure Automatically upload files to the server is checked Make sure Execute code using this interpreter with root privileges via sudo is checked Hit Finish Click Ok Let it load the interpreter and do the transfers","title":"Setup PyCharm for service (optional)"},{"location":"developer_manual/env/pycharm/remote_development/#use-pycharm","text":"Now that your remote development VM is set up you should read the use PyCharm documentation to get yourself started.","title":"Use Pycharm"},{"location":"developer_manual/env/pycharm/use_pycharm/","text":"Use PyCharm \u00b6 Here are some pointers on how to run most of the core components live in PyCharm when either your local or remote development VM is ready. Load docker-compose files \u00b6 Minimal Dependencies \u00b6 When dependencies are loaded, you can launch any core components and their dependencies should be satisfied. PyCharm Professional In your project file viewer Browse to assemblyline-base/dev/depends Right-click on docker-compose-minimal.yml Select Create 'depends/docker-compose...' Click OK You'll notice on the top right that a new Compose deployment has been created for dependencies. Hit the play button next to it to launch the containers. Tip From now on you can just select that Compose deployment for dependencies from the dropdown up top and hit the run button to launch it. It is good practice to Edit the configuration and give it a proper name. Docker-compose (PyCharm Community) In a new terminal on the VM, run the following commands: cd ~/git/alv4/assemblyline-base/dev/depends/ sudo docker-compose -f docker-compose-minimal.yml up Tip This will take over the terminal and the logs will be displayed there. Hit ctrl-c when you want to shut the containers down. If you close the terminal and the containers keep running, run the following commands to shut down the containers: cd ~/git/alv4/assemblyline-base/dev/depends/ sudo docker-compose -f docker-compose-minimal.yml down Core services \u00b6 Core services depend on the dependencies in the docker-compose file. When the core services are loaded you should be able to point a browser at https://IP_OF_VM and you'll be greeted with a working version of Assemblyline with no services configured. Note Default admin user credentials are: Username: admin Password: admin PyCharm Professional In your project file viewer Browse to assemblyline-base/dev/core Right-click on docker-compose.yml Select Create 'core: Compose Deploy...' Click OK You'll notice on the top right that a new Compose deployment has been created for core services, hit the play button next to it to launch the containers. Tip From now on you can just select that Compose deployment for core components from the dropdown up top and hit the run button to launch it. It is good practice to Edit the configuration and give it a proper name. Docker-compose (PyCharm Community) In a new terminal on the VM, run the following commands: cd ~/git/alv4/assemblyline-base/dev/core/ sudo docker-compose up Tip This will take over the terminal and the logs will be displayed there. Hit ctrl-c when you want to shut the containers down. If you close the terminal and the containers keep running, run the following commands to shut down the containers: cd ~/git/alv4/assemblyline-base/dev/core/ sudo docker-compose down Load a single container live \u00b6 Loading a single container is very useful to test a newly create production service container or to launch the frontend while the backend is being debugged. You can easily load any container live in your Assemblyline Dev environment by following these instructions: Note For this demo we will assume that you want to run the service container from the developing an Assemblyline service documentation. PyCharm Professional Click the Run menu then select Edit Configurations... Click the + button at the top left` Select \"Docker Image\" In the Name field at the top, set the name to: Sample Service - Container Set the Image ID to: testing/assemblyline-service-sample Set the Container Name to SampleService Click the modify options and select Environment variables Add the following environment variable: SERVICE_API_HOST=http://172.17.0.1:5003 Click the modify options and select Run options Add the following Run option : -network host Click OK Tip From now on you can just select the Sample Service - Container run configuration from the dropdown up top and hit the run button to launch it. Docker (PyCharm Community) Pycharm Community does not have support for managing Docker containers, therefore you will have to run your containers using a shell . In a new terminal on the VM, run the following commands: docker run --env SERVICE_API_HOST = http://172.17.0.1:5003 --network = host --name SampleService testing/assemblyline-service-sample Tip This will take over the terminal and the logs will be displayed there. Hit ctrl-c when you want to shut the containers down. Run components live from PyCharm \u00b6 Core Components \u00b6 Most of the core components are going to be as easy to run as finding the component main file then hit Run or Debug ... All core components require you to run the Minimal Dependencies docker-compose file before launching them. Launching the API Server Find the UI main file in the project files browser assemblyline-ui/assemblyline_ui/app.py Right-click on it Select either Run 'app' or Debug 'app' Live Services \u00b6 The main exception to very easy-to-run components is debugging services live in the system. Services require you to run two separate components to process files. The two components talk via named pipes in the /tmp folder. Inside the Docker container, this is very easy but debugging this live requires some sacrifices. Limitations You can only run one live debugging service at the time. Depending on where you stop them and how you stop them, they don't fully clean up after themselves. They require extra configuration, it's not just \"click-and-go\". We will show you how to run the Sample service live in the system created in the developing and Assemblyline service documentation. This requires you to run the full infrastructure using the docker-compose files before executing the steps. ( Minimal dependencies and Core services ) Important Run the following example inside the PyCharm window pointing to the services ( ~/git/services ) Example Setup Task Handler run configuration if it does not exist: Click the \"Run\" menu then select \"Edit Configurations...\" Click the + button at the top left Click Python The first option in the configuration tab is a drop-down that says Script path: click it and choose Module Name: In the box beside it, write: assemblyline_service_client.task_handler In the name box at the top, write Task Handler Click the OK Button Now if you click the green play button beside the newly created Task Handler run configuration, Task Handler will be waiting for the service to start. Setup the new service configuration: Click the \"Run\" menu then select \"Edit Configurations...\" Click the + button at the top left Click Python The first option in the configuration tab is a drop-down that says Script path: click it and choose Module Name: In the box beside it, write: assemblyline_v4_service.run_service Add ;SERVICE_PATH=sample.Sample to the Environment variables Set the working directory to: assemblyline-service-sample by pressing the little folder on the right and browsing to that directory In the name box at the top, write Sample Service - LIVE Click OK Now that dropdown near the top says Sample Service - LIVE , click the play or the bug button to either Run or Debug the service In the Run or Debug window, the service will be stuck at Waiting for receive task named pipe to be ready... . This is because task_handler shut down after registering the service for the first time. Select task_handler from the dropdown near the top and click the play or bug button beside it again. Now both Sample Service - LIVE and Task Handler will stay up and poll for tasks from the service server.","title":"Use PyCharm"},{"location":"developer_manual/env/pycharm/use_pycharm/#use-pycharm","text":"Here are some pointers on how to run most of the core components live in PyCharm when either your local or remote development VM is ready.","title":"Use PyCharm"},{"location":"developer_manual/env/pycharm/use_pycharm/#load-docker-compose-files","text":"","title":"Load docker-compose files"},{"location":"developer_manual/env/pycharm/use_pycharm/#minimal-dependencies","text":"When dependencies are loaded, you can launch any core components and their dependencies should be satisfied. PyCharm Professional In your project file viewer Browse to assemblyline-base/dev/depends Right-click on docker-compose-minimal.yml Select Create 'depends/docker-compose...' Click OK You'll notice on the top right that a new Compose deployment has been created for dependencies. Hit the play button next to it to launch the containers. Tip From now on you can just select that Compose deployment for dependencies from the dropdown up top and hit the run button to launch it. It is good practice to Edit the configuration and give it a proper name. Docker-compose (PyCharm Community) In a new terminal on the VM, run the following commands: cd ~/git/alv4/assemblyline-base/dev/depends/ sudo docker-compose -f docker-compose-minimal.yml up Tip This will take over the terminal and the logs will be displayed there. Hit ctrl-c when you want to shut the containers down. If you close the terminal and the containers keep running, run the following commands to shut down the containers: cd ~/git/alv4/assemblyline-base/dev/depends/ sudo docker-compose -f docker-compose-minimal.yml down","title":"Minimal Dependencies"},{"location":"developer_manual/env/pycharm/use_pycharm/#core-services","text":"Core services depend on the dependencies in the docker-compose file. When the core services are loaded you should be able to point a browser at https://IP_OF_VM and you'll be greeted with a working version of Assemblyline with no services configured. Note Default admin user credentials are: Username: admin Password: admin PyCharm Professional In your project file viewer Browse to assemblyline-base/dev/core Right-click on docker-compose.yml Select Create 'core: Compose Deploy...' Click OK You'll notice on the top right that a new Compose deployment has been created for core services, hit the play button next to it to launch the containers. Tip From now on you can just select that Compose deployment for core components from the dropdown up top and hit the run button to launch it. It is good practice to Edit the configuration and give it a proper name. Docker-compose (PyCharm Community) In a new terminal on the VM, run the following commands: cd ~/git/alv4/assemblyline-base/dev/core/ sudo docker-compose up Tip This will take over the terminal and the logs will be displayed there. Hit ctrl-c when you want to shut the containers down. If you close the terminal and the containers keep running, run the following commands to shut down the containers: cd ~/git/alv4/assemblyline-base/dev/core/ sudo docker-compose down","title":"Core services"},{"location":"developer_manual/env/pycharm/use_pycharm/#load-a-single-container-live","text":"Loading a single container is very useful to test a newly create production service container or to launch the frontend while the backend is being debugged. You can easily load any container live in your Assemblyline Dev environment by following these instructions: Note For this demo we will assume that you want to run the service container from the developing an Assemblyline service documentation. PyCharm Professional Click the Run menu then select Edit Configurations... Click the + button at the top left` Select \"Docker Image\" In the Name field at the top, set the name to: Sample Service - Container Set the Image ID to: testing/assemblyline-service-sample Set the Container Name to SampleService Click the modify options and select Environment variables Add the following environment variable: SERVICE_API_HOST=http://172.17.0.1:5003 Click the modify options and select Run options Add the following Run option : -network host Click OK Tip From now on you can just select the Sample Service - Container run configuration from the dropdown up top and hit the run button to launch it. Docker (PyCharm Community) Pycharm Community does not have support for managing Docker containers, therefore you will have to run your containers using a shell . In a new terminal on the VM, run the following commands: docker run --env SERVICE_API_HOST = http://172.17.0.1:5003 --network = host --name SampleService testing/assemblyline-service-sample Tip This will take over the terminal and the logs will be displayed there. Hit ctrl-c when you want to shut the containers down.","title":"Load a single container live"},{"location":"developer_manual/env/pycharm/use_pycharm/#run-components-live-from-pycharm","text":"","title":"Run components live from PyCharm"},{"location":"developer_manual/env/pycharm/use_pycharm/#core-components","text":"Most of the core components are going to be as easy to run as finding the component main file then hit Run or Debug ... All core components require you to run the Minimal Dependencies docker-compose file before launching them. Launching the API Server Find the UI main file in the project files browser assemblyline-ui/assemblyline_ui/app.py Right-click on it Select either Run 'app' or Debug 'app'","title":"Core Components"},{"location":"developer_manual/env/pycharm/use_pycharm/#live-services","text":"The main exception to very easy-to-run components is debugging services live in the system. Services require you to run two separate components to process files. The two components talk via named pipes in the /tmp folder. Inside the Docker container, this is very easy but debugging this live requires some sacrifices. Limitations You can only run one live debugging service at the time. Depending on where you stop them and how you stop them, they don't fully clean up after themselves. They require extra configuration, it's not just \"click-and-go\". We will show you how to run the Sample service live in the system created in the developing and Assemblyline service documentation. This requires you to run the full infrastructure using the docker-compose files before executing the steps. ( Minimal dependencies and Core services ) Important Run the following example inside the PyCharm window pointing to the services ( ~/git/services ) Example Setup Task Handler run configuration if it does not exist: Click the \"Run\" menu then select \"Edit Configurations...\" Click the + button at the top left Click Python The first option in the configuration tab is a drop-down that says Script path: click it and choose Module Name: In the box beside it, write: assemblyline_service_client.task_handler In the name box at the top, write Task Handler Click the OK Button Now if you click the green play button beside the newly created Task Handler run configuration, Task Handler will be waiting for the service to start. Setup the new service configuration: Click the \"Run\" menu then select \"Edit Configurations...\" Click the + button at the top left Click Python The first option in the configuration tab is a drop-down that says Script path: click it and choose Module Name: In the box beside it, write: assemblyline_v4_service.run_service Add ;SERVICE_PATH=sample.Sample to the Environment variables Set the working directory to: assemblyline-service-sample by pressing the little folder on the right and browsing to that directory In the name box at the top, write Sample Service - LIVE Click OK Now that dropdown near the top says Sample Service - LIVE , click the play or the bug button to either Run or Debug the service In the Run or Debug window, the service will be stuck at Waiting for receive task named pipe to be ready... . This is because task_handler shut down after registering the service for the first time. Select task_handler from the dropdown near the top and click the play or bug button beside it again. Now both Sample Service - LIVE and Task Handler will stay up and poll for tasks from the service server.","title":"Live Services"},{"location":"developer_manual/env/vscode/setup_script/","text":"Setup script \u00b6 Assemblyline's VSCode installation is entirely scripted. It will set up the following things for you: Install VSCode via snap (Optional) Install AL4 development dependencies Clone all core component sources from GitHub Clone all service sources from GitHub (Optional) Create a virtual Python environment for core component development Create a virtual Python environment for service development (optional) Create Run targets inside VSCode for all core components and other important scripts Create Tasks inside VSCode for development using Docker-Compose Setup our code formatting standards Deploy a local Docker registry on port 32000 Note We recommend installing the VSCode extensions needed to use this environment once VSCode is launched in the workspace. Pre-requisites \u00b6 The setup script assumes the following: You are running this on an Ubuntu machine / VM (20.04 and up). VSCode does not have to be running on the same host where you run this script so run the setup script on the target VM of a remote development setup. You have read the setup_vscode.sh script. This script will install and configure packages for ease of use. Important If you are uncomfortable with some of the changes that the script makes, you should comment them out before running the script. Installation instruction \u00b6 Create your repository directory mkdir -p ~/git cd ~/git Clone repository git clone https://github.com/CybercentreCanada/assemblyline-development-setup alv4 Run setup script. Choose the type of development you want to do and on what type of system. Core only (Local) cd alv4 ./setup_vscode.sh -c Core and Services (Local) cd alv4 ./setup_vscode.sh -c -s Core only (Remote) cd alv4 ./setup_vscode.sh Core and Services (Remote) cd alv4 ./setup_vscode.sh -s Important When running the setup script for the Core and Services installation you will get two dev folders: ~/git/alv4 and ~/git/services . The reason for this is that we want to make sure that service Python dependencies don't interfere with core component dependencies. Therefore, two separate venv are created with different sets of dependencies. The service venv will point to the core components' live code to install assemblyline-base , assemblyline-core , assemblyline-v4-service , and assemblyline-client . That way, any modification you do to the core package code will be reflected in your service instantly. Post-installation instructions \u00b6 When the installation is complete, you will be asked to reboot the VM. This is required for sudo-less Docker to work. After the VM has finished rebooted, you can use a shell to open VSCode: code ~/git/alv4 Note If you've installed the services, you should open another VSCode window pointing to the services folder. code ~/git/services Install recommended extensions \u00b6 To take full advantage of this setup, we strongly advise installing the recommended extensions when prompted or typing @recommended in the Extensions tab . Start using VSCode \u00b6 You can now refer to the \" use VSCode \" instructions to get you started using the VSCode environment with Assemblyline.","title":"Setup script"},{"location":"developer_manual/env/vscode/setup_script/#setup-script","text":"Assemblyline's VSCode installation is entirely scripted. It will set up the following things for you: Install VSCode via snap (Optional) Install AL4 development dependencies Clone all core component sources from GitHub Clone all service sources from GitHub (Optional) Create a virtual Python environment for core component development Create a virtual Python environment for service development (optional) Create Run targets inside VSCode for all core components and other important scripts Create Tasks inside VSCode for development using Docker-Compose Setup our code formatting standards Deploy a local Docker registry on port 32000 Note We recommend installing the VSCode extensions needed to use this environment once VSCode is launched in the workspace.","title":"Setup script"},{"location":"developer_manual/env/vscode/setup_script/#pre-requisites","text":"The setup script assumes the following: You are running this on an Ubuntu machine / VM (20.04 and up). VSCode does not have to be running on the same host where you run this script so run the setup script on the target VM of a remote development setup. You have read the setup_vscode.sh script. This script will install and configure packages for ease of use. Important If you are uncomfortable with some of the changes that the script makes, you should comment them out before running the script.","title":"Pre-requisites"},{"location":"developer_manual/env/vscode/setup_script/#installation-instruction","text":"Create your repository directory mkdir -p ~/git cd ~/git Clone repository git clone https://github.com/CybercentreCanada/assemblyline-development-setup alv4 Run setup script. Choose the type of development you want to do and on what type of system. Core only (Local) cd alv4 ./setup_vscode.sh -c Core and Services (Local) cd alv4 ./setup_vscode.sh -c -s Core only (Remote) cd alv4 ./setup_vscode.sh Core and Services (Remote) cd alv4 ./setup_vscode.sh -s Important When running the setup script for the Core and Services installation you will get two dev folders: ~/git/alv4 and ~/git/services . The reason for this is that we want to make sure that service Python dependencies don't interfere with core component dependencies. Therefore, two separate venv are created with different sets of dependencies. The service venv will point to the core components' live code to install assemblyline-base , assemblyline-core , assemblyline-v4-service , and assemblyline-client . That way, any modification you do to the core package code will be reflected in your service instantly.","title":"Installation instruction"},{"location":"developer_manual/env/vscode/setup_script/#post-installation-instructions","text":"When the installation is complete, you will be asked to reboot the VM. This is required for sudo-less Docker to work. After the VM has finished rebooted, you can use a shell to open VSCode: code ~/git/alv4 Note If you've installed the services, you should open another VSCode window pointing to the services folder. code ~/git/services","title":"Post-installation instructions"},{"location":"developer_manual/env/vscode/setup_script/#install-recommended-extensions","text":"To take full advantage of this setup, we strongly advise installing the recommended extensions when prompted or typing @recommended in the Extensions tab .","title":"Install recommended extensions"},{"location":"developer_manual/env/vscode/setup_script/#start-using-vscode","text":"You can now refer to the \" use VSCode \" instructions to get you started using the VSCode environment with Assemblyline.","title":"Start using VSCode"},{"location":"developer_manual/env/vscode/use_vscode/","text":"Use VSCode \u00b6 Once you are done running the setup script , your installation of VSCode will be ready to run and debug any components of Assemblyline and most launch targets are already pre-configured. This page will point you in the right direction to perform some of the more common tasks that you'll have to do when developing an aspect of Assemblyline. Running Tasks \u00b6 After all recommended extensions are finished installing in VSCode, the task button on the sidebar should be revealed and will give you quick access to the most important tasks in the system. These tasks are split into 3 categories: Container - Run a single container for a single task Docker-compose - Execute a set of containers for a specific task dependency Pytest dependencies - Run necessary dependencies to run tests Tip You can edit the task list by modifying the .vscode/tasks.json . The default task.json file can be found in the assemblyline-development-setup repository. Container tasks \u00b6 A container task executes one specific container in the system. Two of these tasks are predefined: Frontend \u00b6 The Frontend task is used to run the User Interface of Assemblyline. It is only useful for when you launch the Assemblyline API Server in debug mode and are NOT using the core components task. Assemblyline's frontend is now built using ReactJS and is served via NPM serve which is why it is not part of this setup. Refer to the frontend development page for more information on how to do development on the Assemblyline frontend. ResultSample \u00b6 The ResultSample task was created to show the developers how to run newly created service containers in the system. Deep dive in the ResultSample Task This is what the JSON block for executing the ResultSample service in VSCode looks like: ... { \"label\" : \"Container - ResultSample\" , \"type\" : \"shell\" , \"options\" : { \"env\" : { \"LOCAL_IP\" : \"172.17.0.1\" } }, \"command\" : \"docker run --env SERVICE_API_HOST=http://${LOCAL_IP}:5003 --network=host cccs/assemblyline-service-resultsample\" , \"runOptions\" : { \"instanceLimit\" : 1 } }, ... Essentially, this runs the docker run command and specifies where the service server API is located. You can change the LOCAL_IP environment variable if your Docker subnet is different. If you want to make sure Docker's local IP is indeed the default, 172.17.0.1 , just run this command: ip addr show docker0 | grep \"inet \" | awk '{print $2}' | cut -f1 -d \"/\" Docker-compose tasks \u00b6 The docker-compose tasks are used to run sets of predefined dependencies in the system. Dependencies \u00b6 Before trying to run anything, at the bare minimum you will need one of the two Dependencies tasks running. Dependencies (Basic) will run the bare minimum set of containers to start components in the system: Elasticsearch, Redis, Minio, and Nginx. Dependencies (Basic + Kibana) will run the same containers as the Basic task but will add Kibana, Filebeat, and APM so that you can have access to the Kibana dashboard and debug your system more efficiently. Core components \u00b6 The core components docker-compose task runs all Assemblyline core components: Service server Frontend API Server Socket IO Server Alerter Expiry Metrics Heartbeats Statistics Workflow Plumber Dispatcher Ingester Tip If you are wondering what each component does, you should read the Infrastructure documentation which gives a brief description of each component. The core components task will also add two test users to the system: uid password is_admin apikey user user no devkey:user admin admin yes devkey:admin Scaler and Updater \u00b6 This task is an extra core component task that will launch Updater and Scaler. This task requires you to run one of the dependency tasks as well as the core components task to run properly. Scaler and Updater have been separated from the core components task because they interfere with running services live in the system. You are unlikely to ever run this task unless you are working on an issue loading containers from Scaler or Updater. Services \u00b6 This task will register all services to the system and allow them to be instantiated by Scaler and Updater. You are unlikely to run this task unless you want a working dev system that can scan files just like an appliance can. Pytest dependencies \u00b6 The Pytest dependencies tasks are used to set up the environment properly to be able to run the tests like if you were building the packages. There are 4 possible dependencies for the tests: Base - To run the tests for the assemblyline-base repository Core - To run the tests for the assemblyline-core repository Service Server - To run the tests for the assemblyline-service-server repository UI or All - To run the tests for the assemblyline-ui repository or any other repository for that matter Tip In most cases, if you are running tests you can use UI or All tasks because all the tests will work with it, but you might want to use the other tasks to save on resources and on dependency loading time. Launching components in debug mode \u00b6 Using debug mode on all of Assemblyline's components will probably be the single most useful thing that is pre-configured by the setup script . All Assemblyline's components get a launch target out of the box to simplify your debugging needs. Simply click the Run/Debug quick access side menu, select the component that you want to launch, and click the play button. The configured launch targets have been pre-fixed with category to help you identify what they do: Core - Core components unrelated to services Data - Script that generates random data in the system Service Server - Service API server Service - Service Live debugging UI - Assemblyline API Servers Warning To be able to successfully run these components, you will have to run at minimum the Dependencies (Basic) tasks Core, Service Server, and UI \u00b6 The core, service server, and UI categories of launch targets are mostly self-explanatory. They will launch in debug mode any of the core components from the system. Tip If you are wondering what each component does, you should read the Infrastructure which gives a brief description of every single one of them. CLI - Command Line Interface \u00b6 This launch target from the core category launches an interactive console that will let perform a specific task in the system. You can use the help command to find out what possible commands are available. Warning The commands that are found in this interactive console can be very dangerous and most of them should not be run on a production system. Data Launch target \u00b6 The launch targets in the data category are used to create random data in the system. Create default users \u00b6 This launch target will generate the same default user as the Core components tasks does: uid password is_admin apikey user user no devkey:user admin admin yes devkey:admin Generate random data \u00b6 In addition to creating the default users, this task will also use the random data generator to fill the different indexes of the system with data that has been randomly generated. This is especially useful when testing APIs or Frontend which require you to have data in the different indices. Running live services \u00b6 Running services in a live Assemblyline dev environment is the most important thing to know while building a service. It will allow you to send files via the user interface and put breakpoints in your service to catch files as they come through for processing. To run a service live in the system you will need to launch two components: Task Handler Your service Warning Here are a few things to consider before running a service live in debug mode: You need to start both the Dependencies (Basic) and the Core components tasks before starting your service to ensure that it will receive files. You can have one Task handler instance running at the time therefore can only debug one service at the time. Task handler communicates with the service via fixed named pipes in the /tmp directory which is the reason for that limitation. The first time you start a service, you'll have to start the task handler and your service twice because the service stops itself after registering in the DB. You should launch your service from the VSCode window pointing to your services git folder ( ~/git/services ) and for it to exist, you should have run the setup_script.sh with the -s option. Create a launch target for your service \u00b6 To add a launch target for your service, you will have to modify the .vscode/launch.json file in your ~/git/services directory. You can mimic the Safelist launch target as a baseline. Demo Safelist launch target ... { \"name\" : \"[Service] Safelist - LIVE\" , \"type\" : \"python\" , \"request\" : \"launch\" , \"module\" : \"assemblyline_v4_service.run_service\" , \"env\" : { \"SERVICE_PATH\" : \"safelist.Safelist\" }, \"console\" : \"internalConsole\" , \"cwd\" : \"${workspaceFolder}/assemblyline-service-safelist\" }, ... The only three things you will have to change for launching your service are the following The name of the launch target so you can pick it from the list later The environment variable SERVICE_PATH . This should be the Python path to the service class relative to your current working directory. The current working directory. This should be the directory where your service code is, a new directory you've created for your service inside the ~/git/services folder. Launch the service \u00b6 Launching the service will take place again in the VSCode window dedicated to services. From the Run/Debug quick access sidebar, you will launch the Task handler first then the launch target for your service that you've just created. Your service should now be running in live mode! Running Tests \u00b6 After running the setup script for VSCode, the testing interface from the VSCode Python extension will be shown in your quick access side menu and will present you with all available unit tests. You can then simply click the test or group of tests you want to run and hit the play button to run the tests. Warning Do not forget to first launch the appropriate Pytest Dependency from the tasks otherwise all the tests will fail. No tests available? If the testing interface is not showing any tests, load up the 'UI and All' Pytest Dependency task and hit the \"Reload tests\" button.","title":"Use VSCode"},{"location":"developer_manual/env/vscode/use_vscode/#use-vscode","text":"Once you are done running the setup script , your installation of VSCode will be ready to run and debug any components of Assemblyline and most launch targets are already pre-configured. This page will point you in the right direction to perform some of the more common tasks that you'll have to do when developing an aspect of Assemblyline.","title":"Use VSCode"},{"location":"developer_manual/env/vscode/use_vscode/#running-tasks","text":"After all recommended extensions are finished installing in VSCode, the task button on the sidebar should be revealed and will give you quick access to the most important tasks in the system. These tasks are split into 3 categories: Container - Run a single container for a single task Docker-compose - Execute a set of containers for a specific task dependency Pytest dependencies - Run necessary dependencies to run tests Tip You can edit the task list by modifying the .vscode/tasks.json . The default task.json file can be found in the assemblyline-development-setup repository.","title":"Running Tasks"},{"location":"developer_manual/env/vscode/use_vscode/#container-tasks","text":"A container task executes one specific container in the system. Two of these tasks are predefined:","title":"Container tasks"},{"location":"developer_manual/env/vscode/use_vscode/#frontend","text":"The Frontend task is used to run the User Interface of Assemblyline. It is only useful for when you launch the Assemblyline API Server in debug mode and are NOT using the core components task. Assemblyline's frontend is now built using ReactJS and is served via NPM serve which is why it is not part of this setup. Refer to the frontend development page for more information on how to do development on the Assemblyline frontend.","title":"Frontend"},{"location":"developer_manual/env/vscode/use_vscode/#resultsample","text":"The ResultSample task was created to show the developers how to run newly created service containers in the system. Deep dive in the ResultSample Task This is what the JSON block for executing the ResultSample service in VSCode looks like: ... { \"label\" : \"Container - ResultSample\" , \"type\" : \"shell\" , \"options\" : { \"env\" : { \"LOCAL_IP\" : \"172.17.0.1\" } }, \"command\" : \"docker run --env SERVICE_API_HOST=http://${LOCAL_IP}:5003 --network=host cccs/assemblyline-service-resultsample\" , \"runOptions\" : { \"instanceLimit\" : 1 } }, ... Essentially, this runs the docker run command and specifies where the service server API is located. You can change the LOCAL_IP environment variable if your Docker subnet is different. If you want to make sure Docker's local IP is indeed the default, 172.17.0.1 , just run this command: ip addr show docker0 | grep \"inet \" | awk '{print $2}' | cut -f1 -d \"/\"","title":"ResultSample"},{"location":"developer_manual/env/vscode/use_vscode/#docker-compose-tasks","text":"The docker-compose tasks are used to run sets of predefined dependencies in the system.","title":"Docker-compose tasks"},{"location":"developer_manual/env/vscode/use_vscode/#dependencies","text":"Before trying to run anything, at the bare minimum you will need one of the two Dependencies tasks running. Dependencies (Basic) will run the bare minimum set of containers to start components in the system: Elasticsearch, Redis, Minio, and Nginx. Dependencies (Basic + Kibana) will run the same containers as the Basic task but will add Kibana, Filebeat, and APM so that you can have access to the Kibana dashboard and debug your system more efficiently.","title":"Dependencies"},{"location":"developer_manual/env/vscode/use_vscode/#core-components","text":"The core components docker-compose task runs all Assemblyline core components: Service server Frontend API Server Socket IO Server Alerter Expiry Metrics Heartbeats Statistics Workflow Plumber Dispatcher Ingester Tip If you are wondering what each component does, you should read the Infrastructure documentation which gives a brief description of each component. The core components task will also add two test users to the system: uid password is_admin apikey user user no devkey:user admin admin yes devkey:admin","title":"Core components"},{"location":"developer_manual/env/vscode/use_vscode/#scaler-and-updater","text":"This task is an extra core component task that will launch Updater and Scaler. This task requires you to run one of the dependency tasks as well as the core components task to run properly. Scaler and Updater have been separated from the core components task because they interfere with running services live in the system. You are unlikely to ever run this task unless you are working on an issue loading containers from Scaler or Updater.","title":"Scaler and Updater"},{"location":"developer_manual/env/vscode/use_vscode/#services","text":"This task will register all services to the system and allow them to be instantiated by Scaler and Updater. You are unlikely to run this task unless you want a working dev system that can scan files just like an appliance can.","title":"Services"},{"location":"developer_manual/env/vscode/use_vscode/#pytest-dependencies","text":"The Pytest dependencies tasks are used to set up the environment properly to be able to run the tests like if you were building the packages. There are 4 possible dependencies for the tests: Base - To run the tests for the assemblyline-base repository Core - To run the tests for the assemblyline-core repository Service Server - To run the tests for the assemblyline-service-server repository UI or All - To run the tests for the assemblyline-ui repository or any other repository for that matter Tip In most cases, if you are running tests you can use UI or All tasks because all the tests will work with it, but you might want to use the other tasks to save on resources and on dependency loading time.","title":"Pytest dependencies"},{"location":"developer_manual/env/vscode/use_vscode/#launching-components-in-debug-mode","text":"Using debug mode on all of Assemblyline's components will probably be the single most useful thing that is pre-configured by the setup script . All Assemblyline's components get a launch target out of the box to simplify your debugging needs. Simply click the Run/Debug quick access side menu, select the component that you want to launch, and click the play button. The configured launch targets have been pre-fixed with category to help you identify what they do: Core - Core components unrelated to services Data - Script that generates random data in the system Service Server - Service API server Service - Service Live debugging UI - Assemblyline API Servers Warning To be able to successfully run these components, you will have to run at minimum the Dependencies (Basic) tasks","title":"Launching components in debug mode"},{"location":"developer_manual/env/vscode/use_vscode/#core-service-server-and-ui","text":"The core, service server, and UI categories of launch targets are mostly self-explanatory. They will launch in debug mode any of the core components from the system. Tip If you are wondering what each component does, you should read the Infrastructure which gives a brief description of every single one of them.","title":"Core, Service Server, and UI"},{"location":"developer_manual/env/vscode/use_vscode/#cli-command-line-interface","text":"This launch target from the core category launches an interactive console that will let perform a specific task in the system. You can use the help command to find out what possible commands are available. Warning The commands that are found in this interactive console can be very dangerous and most of them should not be run on a production system.","title":"CLI - Command Line Interface"},{"location":"developer_manual/env/vscode/use_vscode/#data-launch-target","text":"The launch targets in the data category are used to create random data in the system.","title":"Data Launch target"},{"location":"developer_manual/env/vscode/use_vscode/#create-default-users","text":"This launch target will generate the same default user as the Core components tasks does: uid password is_admin apikey user user no devkey:user admin admin yes devkey:admin","title":"Create default users"},{"location":"developer_manual/env/vscode/use_vscode/#generate-random-data","text":"In addition to creating the default users, this task will also use the random data generator to fill the different indexes of the system with data that has been randomly generated. This is especially useful when testing APIs or Frontend which require you to have data in the different indices.","title":"Generate random data"},{"location":"developer_manual/env/vscode/use_vscode/#running-live-services","text":"Running services in a live Assemblyline dev environment is the most important thing to know while building a service. It will allow you to send files via the user interface and put breakpoints in your service to catch files as they come through for processing. To run a service live in the system you will need to launch two components: Task Handler Your service Warning Here are a few things to consider before running a service live in debug mode: You need to start both the Dependencies (Basic) and the Core components tasks before starting your service to ensure that it will receive files. You can have one Task handler instance running at the time therefore can only debug one service at the time. Task handler communicates with the service via fixed named pipes in the /tmp directory which is the reason for that limitation. The first time you start a service, you'll have to start the task handler and your service twice because the service stops itself after registering in the DB. You should launch your service from the VSCode window pointing to your services git folder ( ~/git/services ) and for it to exist, you should have run the setup_script.sh with the -s option.","title":"Running live services"},{"location":"developer_manual/env/vscode/use_vscode/#create-a-launch-target-for-your-service","text":"To add a launch target for your service, you will have to modify the .vscode/launch.json file in your ~/git/services directory. You can mimic the Safelist launch target as a baseline. Demo Safelist launch target ... { \"name\" : \"[Service] Safelist - LIVE\" , \"type\" : \"python\" , \"request\" : \"launch\" , \"module\" : \"assemblyline_v4_service.run_service\" , \"env\" : { \"SERVICE_PATH\" : \"safelist.Safelist\" }, \"console\" : \"internalConsole\" , \"cwd\" : \"${workspaceFolder}/assemblyline-service-safelist\" }, ... The only three things you will have to change for launching your service are the following The name of the launch target so you can pick it from the list later The environment variable SERVICE_PATH . This should be the Python path to the service class relative to your current working directory. The current working directory. This should be the directory where your service code is, a new directory you've created for your service inside the ~/git/services folder.","title":"Create a launch target for your service"},{"location":"developer_manual/env/vscode/use_vscode/#launch-the-service","text":"Launching the service will take place again in the VSCode window dedicated to services. From the Run/Debug quick access sidebar, you will launch the Task handler first then the launch target for your service that you've just created. Your service should now be running in live mode!","title":"Launch the service"},{"location":"developer_manual/env/vscode/use_vscode/#running-tests","text":"After running the setup script for VSCode, the testing interface from the VSCode Python extension will be shown in your quick access side menu and will present you with all available unit tests. You can then simply click the test or group of tests you want to run and hit the play button to run the tests. Warning Do not forget to first launch the appropriate Pytest Dependency from the tasks otherwise all the tests will fail. No tests available? If the testing interface is not showing any tests, load up the 'UI and All' Pytest Dependency task and hit the \"Reload tests\" button.","title":"Running Tests"},{"location":"developer_manual/frontend/frontend/","text":"Assemblyline frontend development \u00b6 This documentation will show you how to set up your environment for Assemblyline frontend development. Install development environment prerequisites \u00b6 Clone the UI frontend code \u00b6 cd ~/git git clone https://github.com/CybercentreCanada/assemblyline-ui-frontend.git Install NodeJS (Ubuntu) \u00b6 Follow these simple commands to install NodeJS curl -sL https://deb.nodesource.com/setup_14.x | sudo -E bash - sudo apt-get install -y nodejs Install NPM dependencies \u00b6 Go to your assemblyline-ui-frontend directory and type: npm install Install Docker (Ubuntu) \u00b6 Follow these simple commands to get Docker running on your machine: # Add Docker repository sudo apt-get update sudo apt-get install -y apt-transport-https ca-certificates curl gnupg-agent software-properties-common curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - sudo apt-key fingerprint 0EBFCD88 sudo add-apt-repository \"deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\" # Install Docker sudo apt-get install -y docker-ce docker-ce-cli containerd.io # Test Docker installation sudo docker run hello-world Install docker-compose \u00b6 Installing docker-compose is done the same way on all Linux distributions. Follow these simple instructions: # Install docker-compose sudo curl -L \"https://github.com/docker/compose/releases/download/1.28.5/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker-compose sudo chmod +x /usr/local/bin/docker-compose # Test docker-compose installation docker-compose --version For reference, here are the instructions on Docker\u2019s website: https://docs.docker.com/compose/install/ Configure the development environment \u00b6 Setup Webpack for debugging behind a proxy \u00b6 Create a file named .env.local at the root of the assemblyline-ui-frontend directory. The file should only contain the following where <YOUR_IP> is replaced by your development computer IP. HOST=<YOUR_IP>.nip.io Setup docker-compose environment \u00b6 Setup IP routing \u00b6 Create a file in the docker directory named .env . This file should only contain the following where <YOUR_IP> is replaced by your development computer IP. EXTERNAL_IP=<YOUR_IP> Setup Assemblyline configuration file \u00b6 From the docker directory, copy the file config.yml.template to config.yml in the same directory. Change the <YOUR_IP> in the newly created config.yml file to the IP of your development machine. Setup Assemblyline classification engine file \u00b6 From the docker directory, copy the file classification.yml.template to classification.yml in the same directory. Change the enforce value to true in the classification.yml file to turn on the classification engine. Launch the dev environment \u00b6 Dependencies \u00b6 Go to the docker directory and run the following command to launch the Assemblyline database and user interface. docker-compose up Frontend \u00b6 Use the npm start script to launch the frontend. Once dependencies and frontend started \u00b6 Access the dev frontend at the following link: https://<YOUR_IP>.nip.io","title":"Frontend"},{"location":"developer_manual/frontend/frontend/#assemblyline-frontend-development","text":"This documentation will show you how to set up your environment for Assemblyline frontend development.","title":"Assemblyline frontend development"},{"location":"developer_manual/frontend/frontend/#install-development-environment-prerequisites","text":"","title":"Install development environment prerequisites"},{"location":"developer_manual/frontend/frontend/#clone-the-ui-frontend-code","text":"cd ~/git git clone https://github.com/CybercentreCanada/assemblyline-ui-frontend.git","title":"Clone the UI frontend code"},{"location":"developer_manual/frontend/frontend/#install-nodejs-ubuntu","text":"Follow these simple commands to install NodeJS curl -sL https://deb.nodesource.com/setup_14.x | sudo -E bash - sudo apt-get install -y nodejs","title":"Install NodeJS (Ubuntu)"},{"location":"developer_manual/frontend/frontend/#install-npm-dependencies","text":"Go to your assemblyline-ui-frontend directory and type: npm install","title":"Install NPM dependencies"},{"location":"developer_manual/frontend/frontend/#install-docker-ubuntu","text":"Follow these simple commands to get Docker running on your machine: # Add Docker repository sudo apt-get update sudo apt-get install -y apt-transport-https ca-certificates curl gnupg-agent software-properties-common curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - sudo apt-key fingerprint 0EBFCD88 sudo add-apt-repository \"deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\" # Install Docker sudo apt-get install -y docker-ce docker-ce-cli containerd.io # Test Docker installation sudo docker run hello-world","title":"Install Docker (Ubuntu)"},{"location":"developer_manual/frontend/frontend/#install-docker-compose","text":"Installing docker-compose is done the same way on all Linux distributions. Follow these simple instructions: # Install docker-compose sudo curl -L \"https://github.com/docker/compose/releases/download/1.28.5/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker-compose sudo chmod +x /usr/local/bin/docker-compose # Test docker-compose installation docker-compose --version For reference, here are the instructions on Docker\u2019s website: https://docs.docker.com/compose/install/","title":"Install docker-compose"},{"location":"developer_manual/frontend/frontend/#configure-the-development-environment","text":"","title":"Configure the development environment"},{"location":"developer_manual/frontend/frontend/#setup-webpack-for-debugging-behind-a-proxy","text":"Create a file named .env.local at the root of the assemblyline-ui-frontend directory. The file should only contain the following where <YOUR_IP> is replaced by your development computer IP. HOST=<YOUR_IP>.nip.io","title":"Setup Webpack for debugging behind a proxy"},{"location":"developer_manual/frontend/frontend/#setup-docker-compose-environment","text":"","title":"Setup docker-compose environment"},{"location":"developer_manual/frontend/frontend/#setup-ip-routing","text":"Create a file in the docker directory named .env . This file should only contain the following where <YOUR_IP> is replaced by your development computer IP. EXTERNAL_IP=<YOUR_IP>","title":"Setup IP routing"},{"location":"developer_manual/frontend/frontend/#setup-assemblyline-configuration-file","text":"From the docker directory, copy the file config.yml.template to config.yml in the same directory. Change the <YOUR_IP> in the newly created config.yml file to the IP of your development machine.","title":"Setup Assemblyline configuration file"},{"location":"developer_manual/frontend/frontend/#setup-assemblyline-classification-engine-file","text":"From the docker directory, copy the file classification.yml.template to classification.yml in the same directory. Change the enforce value to true in the classification.yml file to turn on the classification engine.","title":"Setup Assemblyline classification engine file"},{"location":"developer_manual/frontend/frontend/#launch-the-dev-environment","text":"","title":"Launch the dev environment"},{"location":"developer_manual/frontend/frontend/#dependencies","text":"Go to the docker directory and run the following command to launch the Assemblyline database and user interface. docker-compose up","title":"Dependencies"},{"location":"developer_manual/frontend/frontend/#frontend","text":"Use the npm start script to launch the frontend.","title":"Frontend"},{"location":"developer_manual/frontend/frontend/#once-dependencies-and-frontend-started","text":"Access the dev frontend at the following link: https://<YOUR_IP>.nip.io","title":"Once dependencies and frontend started"},{"location":"developer_manual/services/adding_a_service_updater/","text":"Adding a Service Updater \u00b6 This documentation builds on Developing an Assemblyline service in the event where you have a service that's dependent on signatures for analysis (ie. YARA or Suricata rules) Build your updater \u00b6 You will need to subclass the ServiceUpdater and implement/override functions as deemed necessary. Refer to ServiceUpdater class for more details. Let's say the following code is written in update_server.py in the root of your service directory: from assemblyline.odm.models.signature import Signature from assemblyline_v4_service.updater.updater import ServiceUpdater class SampleUpdateServer ( ServiceUpdater ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) def import_update ( self , files_sha256 , client , source , default_classification ): -> None # Purpose: Used to import a set of signatures from a source into Assemblyline for signature management # Inputs: # files_sha256: A list of tuples containing file paths and their respective sha256 # client: An Assemblyline client used to interact with the API on behalf of a service account # source: The name of the source # default_classification: The default classification given to a signature if none is provided signatures = [] for file , _ in files_sha256 : # Iterate over all the files retrieved by source and upload them as signatures in Assemblyline with open ( file , 'r' ) as fh : signatures . append ( Signature ( dict ( classification = default_classification , data = fh . read (), name = f 'sample_signature { len ( signatures ) } ' , source = source_name , status = 'DEPLOYED' , type = 'sample' ))) client . signatures . add_update_many ( signatures ) self . log . info ( f 'Successfully imported { len ( signatures ) } signatures' ) return def is_valid ( self , file_path ) -> bool : # Purpose: Used to determine if the file associated is 'valid' to be processed as a signature # Inputs: # file_path: Path to a signature file from an external source return super () . is_valid ( file_path ) #Returns true always if __name__ == '__main__' : with SampleUpdateServer ( default_pattern = \"*.json\" ) as server : server . serve_forever () Add it to the manifest! \u00b6 In addition to your service manifest, you would append the following: Warning The updater dependency needs to be named 'updates' for the service to recognize it as an updater rather than a normal dependency container. !!! critical Updaters need to run_as_core which allows them to run at the same level as other core containers. # Adding your dependency called 'updates' and specify the command the container should run dependencies : updates : container : allow_internet_access : true command : [ \"python\" , \"-m\" , \"update_server\" ] image : ${REGISTRY}testing/assemblyline-service-sample:latest ports : [ \"5003\" ] run_as_core : True # Update configuration block update_config : # list of source object from where to fetch files for update and what will be the name of those files on disk sources : - uri : https://file-examples-com.github.io/uploads/2017/02/file_example_JSON_1kb.json name : sample_1kb_file # interval in seconds at which the updater dependency runs update_interval_seconds : 300 # Should the downloaded files be used to create signatures in the system generates_signatures : true","title":"Adding a Service Updater"},{"location":"developer_manual/services/adding_a_service_updater/#adding-a-service-updater","text":"This documentation builds on Developing an Assemblyline service in the event where you have a service that's dependent on signatures for analysis (ie. YARA or Suricata rules)","title":"Adding a Service Updater"},{"location":"developer_manual/services/adding_a_service_updater/#build-your-updater","text":"You will need to subclass the ServiceUpdater and implement/override functions as deemed necessary. Refer to ServiceUpdater class for more details. Let's say the following code is written in update_server.py in the root of your service directory: from assemblyline.odm.models.signature import Signature from assemblyline_v4_service.updater.updater import ServiceUpdater class SampleUpdateServer ( ServiceUpdater ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) def import_update ( self , files_sha256 , client , source , default_classification ): -> None # Purpose: Used to import a set of signatures from a source into Assemblyline for signature management # Inputs: # files_sha256: A list of tuples containing file paths and their respective sha256 # client: An Assemblyline client used to interact with the API on behalf of a service account # source: The name of the source # default_classification: The default classification given to a signature if none is provided signatures = [] for file , _ in files_sha256 : # Iterate over all the files retrieved by source and upload them as signatures in Assemblyline with open ( file , 'r' ) as fh : signatures . append ( Signature ( dict ( classification = default_classification , data = fh . read (), name = f 'sample_signature { len ( signatures ) } ' , source = source_name , status = 'DEPLOYED' , type = 'sample' ))) client . signatures . add_update_many ( signatures ) self . log . info ( f 'Successfully imported { len ( signatures ) } signatures' ) return def is_valid ( self , file_path ) -> bool : # Purpose: Used to determine if the file associated is 'valid' to be processed as a signature # Inputs: # file_path: Path to a signature file from an external source return super () . is_valid ( file_path ) #Returns true always if __name__ == '__main__' : with SampleUpdateServer ( default_pattern = \"*.json\" ) as server : server . serve_forever ()","title":"Build your updater"},{"location":"developer_manual/services/adding_a_service_updater/#add-it-to-the-manifest","text":"In addition to your service manifest, you would append the following: Warning The updater dependency needs to be named 'updates' for the service to recognize it as an updater rather than a normal dependency container. !!! critical Updaters need to run_as_core which allows them to run at the same level as other core containers. # Adding your dependency called 'updates' and specify the command the container should run dependencies : updates : container : allow_internet_access : true command : [ \"python\" , \"-m\" , \"update_server\" ] image : ${REGISTRY}testing/assemblyline-service-sample:latest ports : [ \"5003\" ] run_as_core : True # Update configuration block update_config : # list of source object from where to fetch files for update and what will be the name of those files on disk sources : - uri : https://file-examples-com.github.io/uploads/2017/02/file_example_JSON_1kb.json name : sample_1kb_file # interval in seconds at which the updater dependency runs update_interval_seconds : 300 # Should the downloaded files be used to create signatures in the system generates_signatures : true","title":"Add it to the manifest!"},{"location":"developer_manual/services/developing_an_assemblyline_service/","text":"Developing an Assemblyline service \u00b6 This guide has been created for developers who are looking to develop services for Assemblyline. It is aimed at individuals with general software development knowledge and basic Python skills. In-depth knowledge of the Assemblyline framework is not required to develop a service. Pre-requisites \u00b6 Before getting started, ensure you have read through the setup environment documentation and created the appropriate development environment to perform service development. Build your first service \u00b6 This section will guide you through the bare minimum steps required to create a running, but functionally useless service. Each sub-section below outlines the steps required for each of the different files required to create an Assemblyline service. All files created in the following sub-sections must be placed in a common directory. Important For this documentation, we will assume that your new service directory is located at ~/git/services/assemblyline-service-sample Service Python code \u00b6 In your service directory, you will first start by creating your service's python file. Let's use sample.py . Put the following code in your service's file: ~/git/services/assemblyline-service-sample/sample.py from assemblyline_v4_service.common.base import ServiceBase from assemblyline_v4_service.common.result import Result , ResultSection class Sample ( ServiceBase ): def __init__ ( self , config = None ): super ( Sample , self ) . __init__ ( config ) def start ( self ): # ================================================================== # On Startup actions: # Your service might have to do some warming up on startup to make things faster self . log . info ( f \"start() from { self . service_attributes . name } service called\" ) def execute ( self , request ): # ================================================================== # Execute a request: # Every time your service receives a new file to scan, the execute function is called # This is where you should execute your processing code. # For this example, we will only generate results ... # ================================================================== # 1. Create a result object where all the result sections will be saved to result = Result () # 2. Create a section to be displayed for this result text_section = ResultSection ( 'Example of a default section' ) # 2.1. Add lines to your section text_section . add_line ( \"This is a line displayed in the body of the section\" ) # 2.2. Your section can generate a score. To do this it needs to fire a heuristic. # We will fire heuristic #1 text_section . set_heuristic ( 1 ) # 2.3. Your section can add tags, we will add a fake one text_section . add_tag ( \"network.static.domain\" , \"cyber.gc.ca\" ) # 3. Make sure you add your section to the result result . add_section ( text_section ) # 4. Wrap-up: Save your result object back into the request request . result = result Service manifest YAML \u00b6 Now that you have a better understanding of the python portion of a service. We will create the associated manifest file that holds the different configurations for the service. In your service directory, you will add the YAML configuration file service_manifest.yml with the following content: ~/git/services/assembyline-service-sample/service_manifest.yml # Name of the service name : Sample # Version of the service version : 4.0.0.dev0 # Description of the service description : ALv4 Sample service from the documentation # Regex defining the types of files the service accepts and rejects accepts : .* rejects : empty # At which stage the service should run (one of FILTER, EXTRACT, CORE, SECONDARY, POST) # NOTE: Stages are executed in the order defined in the list stage : CORE # Which category the service is part of (one of Antivirus, Dynamic Analysis, External, Extraction, Filtering, Networking, Static Analysis) category : Static Analysis # Does the service require access to the file to perform its task # If set to false, the service will only have access to the file metadata (e.g. Hashes, size, type, ...) file_required : true # Maximum execution time the service has before it's considered to be timed out timeout : 10 # is the service enabled by default enabled : true # Service heuristic blocks: List of heuristic objects that define the different heuristics used in the service heuristics : - description : This is a demo heuristic filetype : \"*\" heur_id : 1 name : Demo score : 100 # Docker configuration block which defines: # - the name of the docker container that will be created # - CPU and ram allocation by the container docker_config : image : ${REGISTRY}testing/assemblyline-service-sample:latest cpu_cores : 1.0 ram_mb : 1024 Important The service_manifest.yml has a lot more configurable parameters that you might be required to change depending on the service you are building. You should get familiar with the complete list by reading the service manifest advanced documentation. Dockerfile \u00b6 Finally, the last file needed to complete your assemblyline service is the Dockerfile that is used to create its Docker container. In your service directory, create a file named Dockerfile with the following content: ~/git/services/assemblyline-service-sample/Dockerfile FROM cccs/assemblyline-v4-service-base:stable # Python path to the service class from your service directory # The following example refers to the class \"Sample\" from the \"sample.py\" file ENV SERVICE_PATH sample.Sample # Install any service dependencies here # For example: RUN apt-get update && apt-get install -y libyaml-dev # RUN pip install utils # Switch to assemblyline user USER assemblyline # Copy Sample service code WORKDIR /opt/al_service COPY . . The Dockerfile is required to build a Docker container. When developing a Docker container for an Assemblyline service, the following must be ensured: The parent image must be cccs/assemblyline-v4-service-base:stable so you are using a stable build of service base. An environment variable named SERVICE_PATH must be set whose value defines the Python module path to the main service class which inherits from the ServiceBase class. Any dependency installation must be completed as the root user, which is set by default in the parent image. Once all dependency installations have been completed, you must change the user to assemblyline . The service code and any dependency files must be copied to the /opt/al_service directory. Conclusion \u00b6 After you've completed creating your first service, your ~/git/services/assemblyline-service-sample directory should have the following files at a minimum: . \u251c\u2500\u2500 Dockerfile \u251c\u2500\u2500 result_sample.py \u2514\u2500\u2500 service_manifest.yml","title":"Developing a service"},{"location":"developer_manual/services/developing_an_assemblyline_service/#developing-an-assemblyline-service","text":"This guide has been created for developers who are looking to develop services for Assemblyline. It is aimed at individuals with general software development knowledge and basic Python skills. In-depth knowledge of the Assemblyline framework is not required to develop a service.","title":"Developing an Assemblyline service"},{"location":"developer_manual/services/developing_an_assemblyline_service/#pre-requisites","text":"Before getting started, ensure you have read through the setup environment documentation and created the appropriate development environment to perform service development.","title":"Pre-requisites"},{"location":"developer_manual/services/developing_an_assemblyline_service/#build-your-first-service","text":"This section will guide you through the bare minimum steps required to create a running, but functionally useless service. Each sub-section below outlines the steps required for each of the different files required to create an Assemblyline service. All files created in the following sub-sections must be placed in a common directory. Important For this documentation, we will assume that your new service directory is located at ~/git/services/assemblyline-service-sample","title":"Build your first service"},{"location":"developer_manual/services/developing_an_assemblyline_service/#service-python-code","text":"In your service directory, you will first start by creating your service's python file. Let's use sample.py . Put the following code in your service's file: ~/git/services/assemblyline-service-sample/sample.py from assemblyline_v4_service.common.base import ServiceBase from assemblyline_v4_service.common.result import Result , ResultSection class Sample ( ServiceBase ): def __init__ ( self , config = None ): super ( Sample , self ) . __init__ ( config ) def start ( self ): # ================================================================== # On Startup actions: # Your service might have to do some warming up on startup to make things faster self . log . info ( f \"start() from { self . service_attributes . name } service called\" ) def execute ( self , request ): # ================================================================== # Execute a request: # Every time your service receives a new file to scan, the execute function is called # This is where you should execute your processing code. # For this example, we will only generate results ... # ================================================================== # 1. Create a result object where all the result sections will be saved to result = Result () # 2. Create a section to be displayed for this result text_section = ResultSection ( 'Example of a default section' ) # 2.1. Add lines to your section text_section . add_line ( \"This is a line displayed in the body of the section\" ) # 2.2. Your section can generate a score. To do this it needs to fire a heuristic. # We will fire heuristic #1 text_section . set_heuristic ( 1 ) # 2.3. Your section can add tags, we will add a fake one text_section . add_tag ( \"network.static.domain\" , \"cyber.gc.ca\" ) # 3. Make sure you add your section to the result result . add_section ( text_section ) # 4. Wrap-up: Save your result object back into the request request . result = result","title":"Service Python code"},{"location":"developer_manual/services/developing_an_assemblyline_service/#service-manifest-yaml","text":"Now that you have a better understanding of the python portion of a service. We will create the associated manifest file that holds the different configurations for the service. In your service directory, you will add the YAML configuration file service_manifest.yml with the following content: ~/git/services/assembyline-service-sample/service_manifest.yml # Name of the service name : Sample # Version of the service version : 4.0.0.dev0 # Description of the service description : ALv4 Sample service from the documentation # Regex defining the types of files the service accepts and rejects accepts : .* rejects : empty # At which stage the service should run (one of FILTER, EXTRACT, CORE, SECONDARY, POST) # NOTE: Stages are executed in the order defined in the list stage : CORE # Which category the service is part of (one of Antivirus, Dynamic Analysis, External, Extraction, Filtering, Networking, Static Analysis) category : Static Analysis # Does the service require access to the file to perform its task # If set to false, the service will only have access to the file metadata (e.g. Hashes, size, type, ...) file_required : true # Maximum execution time the service has before it's considered to be timed out timeout : 10 # is the service enabled by default enabled : true # Service heuristic blocks: List of heuristic objects that define the different heuristics used in the service heuristics : - description : This is a demo heuristic filetype : \"*\" heur_id : 1 name : Demo score : 100 # Docker configuration block which defines: # - the name of the docker container that will be created # - CPU and ram allocation by the container docker_config : image : ${REGISTRY}testing/assemblyline-service-sample:latest cpu_cores : 1.0 ram_mb : 1024 Important The service_manifest.yml has a lot more configurable parameters that you might be required to change depending on the service you are building. You should get familiar with the complete list by reading the service manifest advanced documentation.","title":"Service manifest YAML"},{"location":"developer_manual/services/developing_an_assemblyline_service/#dockerfile","text":"Finally, the last file needed to complete your assemblyline service is the Dockerfile that is used to create its Docker container. In your service directory, create a file named Dockerfile with the following content: ~/git/services/assemblyline-service-sample/Dockerfile FROM cccs/assemblyline-v4-service-base:stable # Python path to the service class from your service directory # The following example refers to the class \"Sample\" from the \"sample.py\" file ENV SERVICE_PATH sample.Sample # Install any service dependencies here # For example: RUN apt-get update && apt-get install -y libyaml-dev # RUN pip install utils # Switch to assemblyline user USER assemblyline # Copy Sample service code WORKDIR /opt/al_service COPY . . The Dockerfile is required to build a Docker container. When developing a Docker container for an Assemblyline service, the following must be ensured: The parent image must be cccs/assemblyline-v4-service-base:stable so you are using a stable build of service base. An environment variable named SERVICE_PATH must be set whose value defines the Python module path to the main service class which inherits from the ServiceBase class. Any dependency installation must be completed as the root user, which is set by default in the parent image. Once all dependency installations have been completed, you must change the user to assemblyline . The service code and any dependency files must be copied to the /opt/al_service directory.","title":"Dockerfile"},{"location":"developer_manual/services/developing_an_assemblyline_service/#conclusion","text":"After you've completed creating your first service, your ~/git/services/assemblyline-service-sample directory should have the following files at a minimum: . \u251c\u2500\u2500 Dockerfile \u251c\u2500\u2500 result_sample.py \u2514\u2500\u2500 service_manifest.yml","title":"Conclusion"},{"location":"developer_manual/services/run_your_service/","text":"Run your service \u00b6 This section of the service documentation will show you how to run your service in 3 different ways: Standalone mode directly on a file Live code on an Assemblyline system Production container on an Assemblyline system Important This documentation assumes the following: You have read through the setup environment documentation and created the appropriate development environment to perform service development. You have completed the developing an Assemblyline service documentation and your service is in the ~/git/services/assemblyline-service-sample directory on the machine where your IDE runs. Whether you are using VSCode or PyCharm as your IDE, you have a virtual environment dedicated to running services either located at ~/git/services/venv or ~/venv/services on the VM where the code runs Standalone mode \u00b6 To test an Assemblyline service in standalone mode, the run_service_once module from the assemblyline-v4-service package can be used to run a single task through the service for testing. Running the Sample service \u00b6 Load the virtual environment source ~/git/services/venv/bin/activate Ensure the current working directory is the root of the service directory of the service to be run. cd ~/git/services/assemblyline-service-sample From a terminal, run the run_service_once module, specifying the service path for the service to run and the path to the file to scan. For this example, we will have the service scan itself. python -m assemblyline_v4_service.dev.run_service_once sample.Sample sample.py The run_service_once module creates a directory at the same spot where the file is found with the service name that scanned the file appended to it. In the previous example, the output of the service should be located at ~/git/services/assemblyline-service-sample/sample.py_sample . The directory will contain a result.json file containing the result from the service. You can view the result.json file using the following command: cat ~/git/services/assemblyline-service-sample/sample.py_sample/result.json | json_pp It will look something like this: ~/git/services/assemblyline-service-sample/sample.py_sample/result.json pretty printed { \"classification\" : \"TLP:W\" , \"drop_file\" : false , \"response\" : { \"extracted\" : [], \"milestones\" : { \"service_completed\" : \"2021-09-15T15:52:47.024909Z\" , \"service_started\" : \"2021-09-15T15:52:47.024761Z\" }, \"service_context\" : null , \"service_debug_info\" : null , \"service_name\" : \"sample\" , \"service_tool_version\" : null , \"service_version\" : \"1\" , \"supplementary\" : [] }, \"result\" : { \"score\" : 100 , \"sections\" : [ { \"auto_collapse\" : false , \"body\" : \"This is a line displayed in the body of the section\" , \"body_format\" : \"TEXT\" , \"classification\" : \"TLP:W\" , \"depth\" : 0 , \"heuristic\" : { \"attack_ids\" : [], \"frequency\" : 1 , \"heur_id\" : 1 , \"score\" : 100 , \"score_map\" : {}, \"signatures\" : {} }, \"tags\" : { \"network\" : { \"static\" : { \"domain\" : [ \"cyber.gc.ca\" ] } } }, \"title_text\" : \"Example of a default section\" , \"zeroize_on_tag_safe\" : false } ] }, \"sha256\" : \"dab595d88c22eba68e831e072471b02206d12cb29173c708c606416ecf50b942\" , \"temp_submission_data\" : {} } Live mode \u00b6 The following technique is how to hook in a service to an Assemblyline development instance so you can perform live debugging and you can send files to your service using the Assemblyline UI. The way to run a service in debug mode will differ depending on if you've been using VSCode or PyCharm as your IDE. Follow the appropriate documentation for your current setup: Run a service LIVE in VSCode Run a service LIVE in PyCharm Important You will need to adjust the documentation according to: The correct name for your service ( Sample ) The correct service python module for your service ( sample.Sample ) The correct working directory for your service ( ~/git/services/assemblyline-service-sample ) Run from a shell \u00b6 If you don't plan on doing any debugging and you just want to run the service live in your development environment, you can just spin up two shells and run Task Handler in one and your Sample service in the other. Task Handler \u00b6 # Load your service virtual environment source ~/git/services/venv/bin/activate # Run task handler python -m assemblyline_service_client.task_handler Sample service \u00b6 # Load your service virtual environment source ~/git/services/venv/bin/activate # Go to your service directory cd ~/git/services/assemblyline-service-sample # Run your service SERVICE_PATH = sample.Sample python -m assemblyline_v4_service.run_service Production container mode \u00b6 When you are confident your service is stable enough, it is time to test it in its final form: A Docker container. Build the container \u00b6 Change working directory to root of the service: cd ~/git/services/assemblyline-service-sample Run the docker build command and tag the container with the same name that the container name has in your service manifest docker build -t testing/assemblyline-service-sample . Run the container LIVE \u00b6 The way to run a container LIVE in your development environment differs depending on if you've been using VSCode or PyCharm as your IDE. Follow the appropriate documentation for your current setup: Run a single container in VSCode Run a single container in PyCharm Important You will need to adjust the documentation according to: The correct name for your service ( Sample ) The correct container name ( testing/assemblyline-service-sample ) Run container from a shell \u00b6 If you don't want to use the IDE to test your production container, you can always run it straight from a shell. Use the following command to run it: docker run --env SERVICE_API_HOST = http:// ` ip addr show docker0 | grep \"inet \" | awk '{print $2}' | cut -f1 -d \"/\" ` :5003 --network = host --name SampleService testing/assemblyline-service-sample Add the container to your deployment \u00b6 Note For the scaler and updater to be able to use your service container, they must be able to get it from a docker registry. You can either use DockerHub, a work central registry, or a local registry. Push the container to your local registry \u00b6 Tip A local docker registry should have already been installed during the development environment setup . If not, you can start one by simply running this command: sudo docker run -dp 32000 :5000 --restart = always --name registry registry Use the following to push your sample service to the local registry: docker tag testing/assemblyline-service-sample localhost:32000/testing/assemblyline-service-sample docker push --all-tags localhost:32000/testing/assemblyline-service-sample Add the service in the management interface \u00b6 Using your web browser, go to the service management page: https://localhost/admin/services (Replace localhost by your VM's IP) Click the Add service button Paste the entire content of the service_manifest.yml file from your service directory in the text box If you are using the local registry from this documentation, change the ${REGISTRY} from the content of service_manifest.yml to 172.17.0.1:32000/ Click the Add button Your service information has been added to the system. The scaler component should automatically start a container of your newly created service.","title":"Run your service"},{"location":"developer_manual/services/run_your_service/#run-your-service","text":"This section of the service documentation will show you how to run your service in 3 different ways: Standalone mode directly on a file Live code on an Assemblyline system Production container on an Assemblyline system Important This documentation assumes the following: You have read through the setup environment documentation and created the appropriate development environment to perform service development. You have completed the developing an Assemblyline service documentation and your service is in the ~/git/services/assemblyline-service-sample directory on the machine where your IDE runs. Whether you are using VSCode or PyCharm as your IDE, you have a virtual environment dedicated to running services either located at ~/git/services/venv or ~/venv/services on the VM where the code runs","title":"Run your service"},{"location":"developer_manual/services/run_your_service/#standalone-mode","text":"To test an Assemblyline service in standalone mode, the run_service_once module from the assemblyline-v4-service package can be used to run a single task through the service for testing.","title":"Standalone mode"},{"location":"developer_manual/services/run_your_service/#running-the-sample-service","text":"Load the virtual environment source ~/git/services/venv/bin/activate Ensure the current working directory is the root of the service directory of the service to be run. cd ~/git/services/assemblyline-service-sample From a terminal, run the run_service_once module, specifying the service path for the service to run and the path to the file to scan. For this example, we will have the service scan itself. python -m assemblyline_v4_service.dev.run_service_once sample.Sample sample.py The run_service_once module creates a directory at the same spot where the file is found with the service name that scanned the file appended to it. In the previous example, the output of the service should be located at ~/git/services/assemblyline-service-sample/sample.py_sample . The directory will contain a result.json file containing the result from the service. You can view the result.json file using the following command: cat ~/git/services/assemblyline-service-sample/sample.py_sample/result.json | json_pp It will look something like this: ~/git/services/assemblyline-service-sample/sample.py_sample/result.json pretty printed { \"classification\" : \"TLP:W\" , \"drop_file\" : false , \"response\" : { \"extracted\" : [], \"milestones\" : { \"service_completed\" : \"2021-09-15T15:52:47.024909Z\" , \"service_started\" : \"2021-09-15T15:52:47.024761Z\" }, \"service_context\" : null , \"service_debug_info\" : null , \"service_name\" : \"sample\" , \"service_tool_version\" : null , \"service_version\" : \"1\" , \"supplementary\" : [] }, \"result\" : { \"score\" : 100 , \"sections\" : [ { \"auto_collapse\" : false , \"body\" : \"This is a line displayed in the body of the section\" , \"body_format\" : \"TEXT\" , \"classification\" : \"TLP:W\" , \"depth\" : 0 , \"heuristic\" : { \"attack_ids\" : [], \"frequency\" : 1 , \"heur_id\" : 1 , \"score\" : 100 , \"score_map\" : {}, \"signatures\" : {} }, \"tags\" : { \"network\" : { \"static\" : { \"domain\" : [ \"cyber.gc.ca\" ] } } }, \"title_text\" : \"Example of a default section\" , \"zeroize_on_tag_safe\" : false } ] }, \"sha256\" : \"dab595d88c22eba68e831e072471b02206d12cb29173c708c606416ecf50b942\" , \"temp_submission_data\" : {} }","title":"Running the Sample service"},{"location":"developer_manual/services/run_your_service/#live-mode","text":"The following technique is how to hook in a service to an Assemblyline development instance so you can perform live debugging and you can send files to your service using the Assemblyline UI. The way to run a service in debug mode will differ depending on if you've been using VSCode or PyCharm as your IDE. Follow the appropriate documentation for your current setup: Run a service LIVE in VSCode Run a service LIVE in PyCharm Important You will need to adjust the documentation according to: The correct name for your service ( Sample ) The correct service python module for your service ( sample.Sample ) The correct working directory for your service ( ~/git/services/assemblyline-service-sample )","title":"Live mode"},{"location":"developer_manual/services/run_your_service/#run-from-a-shell","text":"If you don't plan on doing any debugging and you just want to run the service live in your development environment, you can just spin up two shells and run Task Handler in one and your Sample service in the other.","title":"Run from a shell"},{"location":"developer_manual/services/run_your_service/#task-handler","text":"# Load your service virtual environment source ~/git/services/venv/bin/activate # Run task handler python -m assemblyline_service_client.task_handler","title":"Task Handler"},{"location":"developer_manual/services/run_your_service/#sample-service","text":"# Load your service virtual environment source ~/git/services/venv/bin/activate # Go to your service directory cd ~/git/services/assemblyline-service-sample # Run your service SERVICE_PATH = sample.Sample python -m assemblyline_v4_service.run_service","title":"Sample service"},{"location":"developer_manual/services/run_your_service/#production-container-mode","text":"When you are confident your service is stable enough, it is time to test it in its final form: A Docker container.","title":"Production container mode"},{"location":"developer_manual/services/run_your_service/#build-the-container","text":"Change working directory to root of the service: cd ~/git/services/assemblyline-service-sample Run the docker build command and tag the container with the same name that the container name has in your service manifest docker build -t testing/assemblyline-service-sample .","title":"Build the container"},{"location":"developer_manual/services/run_your_service/#run-the-container-live","text":"The way to run a container LIVE in your development environment differs depending on if you've been using VSCode or PyCharm as your IDE. Follow the appropriate documentation for your current setup: Run a single container in VSCode Run a single container in PyCharm Important You will need to adjust the documentation according to: The correct name for your service ( Sample ) The correct container name ( testing/assemblyline-service-sample )","title":"Run the container LIVE"},{"location":"developer_manual/services/run_your_service/#run-container-from-a-shell","text":"If you don't want to use the IDE to test your production container, you can always run it straight from a shell. Use the following command to run it: docker run --env SERVICE_API_HOST = http:// ` ip addr show docker0 | grep \"inet \" | awk '{print $2}' | cut -f1 -d \"/\" ` :5003 --network = host --name SampleService testing/assemblyline-service-sample","title":"Run container from a shell"},{"location":"developer_manual/services/run_your_service/#add-the-container-to-your-deployment","text":"Note For the scaler and updater to be able to use your service container, they must be able to get it from a docker registry. You can either use DockerHub, a work central registry, or a local registry.","title":"Add the container to your deployment"},{"location":"developer_manual/services/run_your_service/#push-the-container-to-your-local-registry","text":"Tip A local docker registry should have already been installed during the development environment setup . If not, you can start one by simply running this command: sudo docker run -dp 32000 :5000 --restart = always --name registry registry Use the following to push your sample service to the local registry: docker tag testing/assemblyline-service-sample localhost:32000/testing/assemblyline-service-sample docker push --all-tags localhost:32000/testing/assemblyline-service-sample","title":"Push the container to your local registry"},{"location":"developer_manual/services/run_your_service/#add-the-service-in-the-management-interface","text":"Using your web browser, go to the service management page: https://localhost/admin/services (Replace localhost by your VM's IP) Click the Add service button Paste the entire content of the service_manifest.yml file from your service directory in the text box If you are using the local registry from this documentation, change the ${REGISTRY} from the content of service_manifest.yml to 172.17.0.1:32000/ Click the Add button Your service information has been added to the system. The scaler component should automatically start a container of your newly created service.","title":"Add the service in the management interface"},{"location":"developer_manual/services/advanced/request/","text":"Request class \u00b6 The Request object is the parameter received by the service execute function. It holds information about the task to be processed by the service. You can view the source for the class here: Request class source Class variables \u00b6 The following table describes all of the Request object variables which the service can use. Variable name Description deep_scan Returns whether the file should be deep-scanned or not. Deep-scanning usually takes more time and is better suited for files that are sent manually. file_contents Returns the raw byte contents of the file to be scanned. file_name Returns the name of the file (as submitted by the user) to be scanned. file_path Returns the path to the file to be scanned. The service can use this path directly to access the file. file_type Returns the Assemblyline-style file type of the file to be scanned. max_extracted Returns the maximum number of files that are allowed to be extracted by a service. By default this is set to 500. md5 Returns the MD5 hash of the file to be scanned. result Used to get and set the current result. sha1 Returns the SHA1 hash of the file to be scanned. sha256 Returns the SHA256 hash of the file to be scanned. sid ID of the submission being scanned. task The original task object used to create this request. You can find more information there about the request (metadata submitted, files already extracted by other services, tags already generated by other services and more...) temp_submission_data Can be used to get and set temporary submission data which is passed onto subsequent tasks resulting from adding extracted files. Class functions \u00b6 The following table describes the Request object functions which the service can use. add_extracted() \u00b6 This function adds a file extracted by the service to the result. The extracted file will also be scanned through a set of services, as if it had been originally submitted. This function can take the following parameters: path : Complete path to the file name : Display name of the file description : Descriptive text about the file classification : Optional classification of the file Example Excerpt from Assemblyline ResultSample service: result_sample.py ... # ================================================================== # Re-Submitting files to the system # Adding extracted files will have them resubmitted to the system for analysis ... fd , temp_path = tempfile . mkstemp ( dir = self . working_directory ) with os . fdopen ( fd , \"wb\" ) as myfile : myfile . write ( b \"CLASSIFIED!!!__\" + data . encode ()) request . add_extracted ( temp_path , \"classified.doc\" , \"Classified file ... don't look\" , classification = cl_engine . RESTRICTED ) ... add_supplementary() \u00b6 This function adds a supplementary file generated by the service to the result. The supplementary file is uploaded for the user's informational use only and is not scanned. This function can take the following parameters: path : Complete path to the file name : Display name of the file description : Descriptive text about the file classification : Optional classification of the file Example Excerpt from Assemblyline ResultSample service: result_sample.py ... # ================================================================== # Supplementary files # Adding supplementary files will save them on the datastore for future # reference but won't reprocess those files. fd , temp_path = tempfile . mkstemp ( dir = self . working_directory ) with os . fdopen ( fd , \"w\" ) as myfile : myfile . write ( json . dumps ( urls )) request . add_supplementary ( temp_path , \"urls.json\" , \"These are urls as a JSON file\" ) ... drop() \u00b6 When called, the task will be dropped and will not be processed further by other remaining service(s). Example Excerpt from Assemblyline Safelist service: safelist.py ... # Stop processing, the file is safe request . drop () ... get_param() \u00b6 Retrieve a submission parameter for the task. This function can take the following parameter: name : name of the submission parameter to retrieve Example Excerpt from Assemblyline Extract service: extract.py ... def execute ( self , request : ServiceRequest ): ... continue_after_extract = request . get_param ( 'continue_after_extract' ) ... set_service_context() \u00b6 Set the context of the service which ran the file. For example, if the service ran an AntiVirus engine on the file, then the AntiVirus definition version would be the service context. This function can take the following parameters: context : Service context as string Example Excerpt from Assemblyline Metadefender service: metadefender.py ... def execute ( self , request : ServiceRequest ): ... request . set_service_context ( f \"Definition Time Range: { self . nodes [ self . current_node ][ 'oldest_dat' ] } - \" f \" { self . nodes [ self . current_node ][ 'newest_dat' ] } \" ) ...","title":"Request Class"},{"location":"developer_manual/services/advanced/request/#request-class","text":"The Request object is the parameter received by the service execute function. It holds information about the task to be processed by the service. You can view the source for the class here: Request class source","title":"Request class"},{"location":"developer_manual/services/advanced/request/#class-variables","text":"The following table describes all of the Request object variables which the service can use. Variable name Description deep_scan Returns whether the file should be deep-scanned or not. Deep-scanning usually takes more time and is better suited for files that are sent manually. file_contents Returns the raw byte contents of the file to be scanned. file_name Returns the name of the file (as submitted by the user) to be scanned. file_path Returns the path to the file to be scanned. The service can use this path directly to access the file. file_type Returns the Assemblyline-style file type of the file to be scanned. max_extracted Returns the maximum number of files that are allowed to be extracted by a service. By default this is set to 500. md5 Returns the MD5 hash of the file to be scanned. result Used to get and set the current result. sha1 Returns the SHA1 hash of the file to be scanned. sha256 Returns the SHA256 hash of the file to be scanned. sid ID of the submission being scanned. task The original task object used to create this request. You can find more information there about the request (metadata submitted, files already extracted by other services, tags already generated by other services and more...) temp_submission_data Can be used to get and set temporary submission data which is passed onto subsequent tasks resulting from adding extracted files.","title":"Class variables"},{"location":"developer_manual/services/advanced/request/#class-functions","text":"The following table describes the Request object functions which the service can use.","title":"Class functions"},{"location":"developer_manual/services/advanced/request/#add_extracted","text":"This function adds a file extracted by the service to the result. The extracted file will also be scanned through a set of services, as if it had been originally submitted. This function can take the following parameters: path : Complete path to the file name : Display name of the file description : Descriptive text about the file classification : Optional classification of the file Example Excerpt from Assemblyline ResultSample service: result_sample.py ... # ================================================================== # Re-Submitting files to the system # Adding extracted files will have them resubmitted to the system for analysis ... fd , temp_path = tempfile . mkstemp ( dir = self . working_directory ) with os . fdopen ( fd , \"wb\" ) as myfile : myfile . write ( b \"CLASSIFIED!!!__\" + data . encode ()) request . add_extracted ( temp_path , \"classified.doc\" , \"Classified file ... don't look\" , classification = cl_engine . RESTRICTED ) ...","title":"add_extracted()"},{"location":"developer_manual/services/advanced/request/#add_supplementary","text":"This function adds a supplementary file generated by the service to the result. The supplementary file is uploaded for the user's informational use only and is not scanned. This function can take the following parameters: path : Complete path to the file name : Display name of the file description : Descriptive text about the file classification : Optional classification of the file Example Excerpt from Assemblyline ResultSample service: result_sample.py ... # ================================================================== # Supplementary files # Adding supplementary files will save them on the datastore for future # reference but won't reprocess those files. fd , temp_path = tempfile . mkstemp ( dir = self . working_directory ) with os . fdopen ( fd , \"w\" ) as myfile : myfile . write ( json . dumps ( urls )) request . add_supplementary ( temp_path , \"urls.json\" , \"These are urls as a JSON file\" ) ...","title":"add_supplementary()"},{"location":"developer_manual/services/advanced/request/#drop","text":"When called, the task will be dropped and will not be processed further by other remaining service(s). Example Excerpt from Assemblyline Safelist service: safelist.py ... # Stop processing, the file is safe request . drop () ...","title":"drop()"},{"location":"developer_manual/services/advanced/request/#get_param","text":"Retrieve a submission parameter for the task. This function can take the following parameter: name : name of the submission parameter to retrieve Example Excerpt from Assemblyline Extract service: extract.py ... def execute ( self , request : ServiceRequest ): ... continue_after_extract = request . get_param ( 'continue_after_extract' ) ...","title":"get_param()"},{"location":"developer_manual/services/advanced/request/#set_service_context","text":"Set the context of the service which ran the file. For example, if the service ran an AntiVirus engine on the file, then the AntiVirus definition version would be the service context. This function can take the following parameters: context : Service context as string Example Excerpt from Assemblyline Metadefender service: metadefender.py ... def execute ( self , request : ServiceRequest ): ... request . set_service_context ( f \"Definition Time Range: { self . nodes [ self . current_node ][ 'oldest_dat' ] } - \" f \" { self . nodes [ self . current_node ][ 'newest_dat' ] } \" ) ...","title":"set_service_context()"},{"location":"developer_manual/services/advanced/result/","text":"Result class \u00b6 All services in Assemblyline must create a Result object containing the different ResultSections which encapsulate their findings. This Result object must then be set as the Request's result parameter which will then be saved in the database to show to the user. You can view the source for the class here: Result class source Class variables \u00b6 Even though the Result class is critical for the service, it does not have many variables that should be used by the service. Variable Name Description sections List of all the ResultSection objects that have been added to the Result object so far. Class functions \u00b6 There is only one function that the service writer should ever use in a Result object. add_section() \u00b6 This function allows the service to add a ResultSection object to the current Result object. It can take the following parameters: section : The ResultSection object to add to the list on_top : (Optional) Boolean value that indicates if the section should be on top of the other sections or not Example Excerpt from Assemblyline ResultSample service: result_sample.py ... kv_body = { \"a_str\" : \"Some string\" , \"a_bool\" : False , \"an_int\" : 102 , } kv_section = ResultSection ( 'Example of a KEY_VALUE section' , body_format = BODY_FORMAT . KEY_VALUE , body = json . dumps ( kv_body )) result . add_section ( kv_section ) ... Tips on writing good results \u00b6 Here's a few tips on writing services: DO's Make sure your results are easily readable for the user Include only necessary information Collapse sections that are less important by default If your service has information that can be tagged, don't forget to add those tags Choose a ResultSection type that will represent your data well DON'T's If your service has nothing to say, do not create ResultSection objects that contain that the service found nothing. There are a lot of shortcuts that the system takes with empty results and this will circumvent them. Don't overwhelm the users with information or they will stop reading it and just skim through Don't resubmit too many embedded files when you are not sure if those files are worth analyzing, otherwise the results will be hard to read and will take a very long time to execute.","title":"Result Class"},{"location":"developer_manual/services/advanced/result/#result-class","text":"All services in Assemblyline must create a Result object containing the different ResultSections which encapsulate their findings. This Result object must then be set as the Request's result parameter which will then be saved in the database to show to the user. You can view the source for the class here: Result class source","title":"Result class"},{"location":"developer_manual/services/advanced/result/#class-variables","text":"Even though the Result class is critical for the service, it does not have many variables that should be used by the service. Variable Name Description sections List of all the ResultSection objects that have been added to the Result object so far.","title":"Class variables"},{"location":"developer_manual/services/advanced/result/#class-functions","text":"There is only one function that the service writer should ever use in a Result object.","title":"Class functions"},{"location":"developer_manual/services/advanced/result/#add_section","text":"This function allows the service to add a ResultSection object to the current Result object. It can take the following parameters: section : The ResultSection object to add to the list on_top : (Optional) Boolean value that indicates if the section should be on top of the other sections or not Example Excerpt from Assemblyline ResultSample service: result_sample.py ... kv_body = { \"a_str\" : \"Some string\" , \"a_bool\" : False , \"an_int\" : 102 , } kv_section = ResultSection ( 'Example of a KEY_VALUE section' , body_format = BODY_FORMAT . KEY_VALUE , body = json . dumps ( kv_body )) result . add_section ( kv_section ) ...","title":"add_section()"},{"location":"developer_manual/services/advanced/result/#tips-on-writing-good-results","text":"Here's a few tips on writing services: DO's Make sure your results are easily readable for the user Include only necessary information Collapse sections that are less important by default If your service has information that can be tagged, don't forget to add those tags Choose a ResultSection type that will represent your data well DON'T's If your service has nothing to say, do not create ResultSection objects that contain that the service found nothing. There are a lot of shortcuts that the system takes with empty results and this will circumvent them. Don't overwhelm the users with information or they will stop reading it and just skim through Don't resubmit too many embedded files when you are not sure if those files are worth analyzing, otherwise the results will be hard to read and will take a very long time to execute.","title":"Tips on writing good results"},{"location":"developer_manual/services/advanced/result_section/","text":"ResultSection class \u00b6 A ResultSection is bascally part of a service result that encapsulates a certain type of information that your service needs to convey to the user. For example, if you have a service that extracts networking indicators as well as process lists, you should put network indicators in their own section and then the process list in another. Result sections have the following properties: They have different types that will display information in different manners They can attach a heuristic which will add a maliciousness score to the result section They can tag important pieces of information about a file They can contain subsections which are just sections inside of another section They can have a classification which allows the API to redact partial results from a service depending on the user You can view the source for the class here: ResultSection class source Class variables \u00b6 The ResultSection class includes many instance variables which can be used to shape the way the section will be shown to the user. The following table describes all of the variables of the ResultSection class. Variable Name Description parent Parent ResultSection object (Only if the section is a child of another) subsections List of children ResultSection objects body Body of the section. Can take multiple forms depending on the section type : List of strings, string, JSON blob... classification The classification level of the current section body_format The types of body of the current section (Default: TEXT) tags Dictionary containing the different tags that have been added to the section heuristic Current heuristic assigned to the section zeroize_on_tag_safe Should the section be forced to a score of 0 if all tags found in it are marked as Safelisted? (Default: False) auto_collapse Should the section be displayed in collapsed mode when first rendered in the UI? (Default: False) zeroize_on_sig_safe Should the section be forced to a score of 0 if all heuristic signatures found in it are marked as Safelisted? (Default: True) Class functions \u00b6 __init__() \u00b6 The constructor of the ResultSection object allows you to set all variables from the start. Parameters: title_text : (Required) Title of the section body : Body of the section classification : Classification of the section body_format : Type of body heuristic : Heuristic assigned to the section tags : Dictionary of tags assigned to the section parent : Parent of the section (either another section or the Result object) zeroize_on_tag_safe : Should the section be forced to a score of 0 if all tags found in it are marked as Safelisted? auto_collapse : Should the section be displayed in collapsed mode when first rendered in the UI? zeroize_on_sig_safe : Should the section be forced to a score of 0 if all heuristic signatures found in it are marked as Safelisted? Example Excerpt from the Assemblyline ResultSample service: result_sample.py ... # The classification of a section can be set to any valid classification for your system section_color_map = ResultSection ( \"Example of colormap result section\" , body_format = BODY_FORMAT . GRAPH_DATA , body = json . dumps ( color_map_data ), classification = cl_engine . RESTRICTED ) result . add_section ( section_color_map ) ... add_line() \u00b6 This function allows the service to add a line to the body of a ResultSection object. Parameters: text : A string containing the line to add to the body Example Excerpt from the Assemblyline ResultSample service: result_sample.py ... text_section = ResultSection ( 'Example of a default section' ) # You can add lines to your section one at a time # Here we will generate a random line text_section . add_line ( get_random_phrase ()) ... add_lines() \u00b6 This function allows the service to add multiple lines to the body of a ResultSection object. Parameters: lines : List of string to add as multiple lines in the body Example Excerpt from the Assemblyline ResultSample service: result_sample.py ... text_section = ResultSection ( 'Example of a default section' ) ... # Or your can add them from a list # Here we will generate random amount of random lines text_section . add_lines ([ get_random_phrase () for _ in range ( random . randint ( 1 , 5 ))]) ... add_subsection() \u00b6 This function allows the service to add a subsection to the current ResultSection object. Parameters: subsection : The ResultSection object to add as a subsection on_top : (Optional) Boolean value that indicates if the section should be on top of the other sections or not Example Excerpt from the Assemblyline ResultSample service: result_sample.py ... url_sub_section = ResultSection ( 'Example of a url sub-section with multiple links' , body = json . dumps ( urls ), body_format = BODY_FORMAT . URL , heuristic = url_heuristic , classification = cl_engine . RESTRICTED ) ... url_sub_sub_section = ResultSection ( 'example of a two level deep sub-section' , body = json . dumps ( ips ), body_format = BODY_FORMAT . URL ) ... # Since url_sub_sub_section is a sub-section of url_sub_section # we will add it as a sub-section of url_sub_section not to the main result itself url_sub_section . add_subsection ( url_sub_sub_section ) ... add_tag() \u00b6 This function allows the service writer to add a tag to the ResultSection Parameters: type : Type of tag value : Value of the tag Example Excerpt from the Assemblyline ResultSample service: result_sample.py ... # You can tag data to a section. Tagging is used to quickly find defining information about a file text_section . add_tag ( \"attribution.implant\" , \"ResultSample\" ) ... set_body() \u00b6 Set the body and the body format of a section Parameters: body : New body value body_format : (Optional) Type of body - Default: TEXT set_heuristic() \u00b6 Set a heuristic for a current section/subsection. A heuristic is required to assign a score to a result section/subsection. Parameters: heur_id : Heuristic ID as set in the service manifest attack_id : (optional) Attack ID related to the heuristic signature : (optional) Signature name that triggered the heuristic Example Excerpt from the Assemblyline ResultSample service: result_sample.py ... # If the section needs to affect the score of the file you need to set a heuristic # Here we will pick one at random # In addition to adding a heuristic, we will associate a signature to the heuristic. # We're doing this by adding the signature name to the heuristic. (Here we use a random name) text_section . set_heuristic ( 3 , signature = \"sig_one\" ) ... Section types \u00b6 These are all result section types that Assemblyline supports. You can see a screenshot of each section as well as the code that was used to generate the actual section. TEXT \u00b6 Code used to generate the TEXT section Excerpt from the Assemblyline ResultSample service: result_sample.py ... # ================================================================== # Standard text section: BODY_FORMAT.TEXT - DEFAULT # Text sections basically just dump the text to the screen... # The scores of all sections will be SUMed in the service result # The Result classification will be the highest classification found in the sections text_section = ResultSection ( 'Example of a default section' ) # You can add lines to your section one at a time # Here we will generate a random line text_section . add_line ( get_random_phrase ()) # Or your can add them from a list # Here we will generate a random amount of random lines text_section . add_lines ([ get_random_phrase () for _ in range ( random . randint ( 1 , 5 ))]) # You can tag data to a section. Tagging is used to quickly find defining information about a file text_section . add_tag ( \"attribution.implant\" , \"ResultSample\" ) # If the section needs to affect the score of the file you need to set a heuristics # Here we will pick one at random # In addition to adding a heuristic, we will associate a signature to the heuristic. # We're doing this by adding the signature name to the heuristic. (Here we use a random name) text_section . set_heuristic ( 3 , signature = \"sig_one\" ) # You can attach ATT&CK IDs to heuristics after they where defined text_section . heuristic . add_attack_id ( random . choice ( list ( software_map . keys ()))) text_section . heuristic . add_attack_id ( random . choice ( list ( attack_map . keys ()))) text_section . heuristic . add_attack_id ( random . choice ( list ( group_map . keys ()))) text_section . heuristic . add_attack_id ( random . choice ( list ( revoke_map . keys ()))) # Same thing for the signatures, they can be added to a heuristic after the fact and you can even say how # many time the signature fired by setting its frequency. If you call add_signature_id twice with the # same signature, this will also increase the frequency of the signature. text_section . heuristic . add_signature_id ( \"sig_two\" , score = 20 , frequency = 2 ) text_section . heuristic . add_signature_id ( \"sig_two\" , score = 20 , frequency = 3 ) text_section . heuristic . add_signature_id ( \"sig_three\" ) text_section . heuristic . add_signature_id ( \"sig_three\" ) text_section . heuristic . add_signature_id ( \"sig_four\" , score = 0 ) # The heuristic for text_section should have the following properties: # 1. 1 ATT&CK ID: T1066 # 2. 4 signatures: sig_one, sig_two, sig_three and sig_four # 3. Signature frequencies are cumulative, therefore they will be as follows: # - sig_one = 1 # - sig_two = 5 # - sig_three = 2 # - sig_four = 1 # 4. The score used by each heuristic is driven by the following rules: signature_score_map is the highest # priority, then score value for the add_signature_id is in second place and finally the default # heuristic score is used. The score used to calculate the total score for the text_section is # as follows: # - sig_one: 10 -> heuristic default score # - sig_two: 20 -> score provided by the function add_signature_id # - sig_three: 30 -> score provided by the heuristic map # - sig_four: 40 -> score provided by the heuristic map because it's higher priority than the # function score # 5. Total section score is then: 1x10 + 5x20 + 2x30 + 1x40 = 210 # Make sure you add your section to the result result . add_section ( text_section ) ... MEMORY_DUMP \u00b6 Code used to generate the MEMORY_DUMP section Excerpt from the Assemblyline ResultSample service: result_sample.py ... # ================================================================== # Memory dump section: BODY_FORMAT.MEMORY_DUMP # Dump whatever string content you have into a <pre/> html tag so you can do your own formatting data = hexdump ( b \"This is some random text that we will format as an hexdump and you'll see \" b \"that the hexdump formatting will be preserved by the memory dump section!\" ) memdump_section = ResultSection ( 'Example of a memory dump section' , body_format = BODY_FORMAT . MEMORY_DUMP , body = data ) memdump_section . set_heuristic ( random . randint ( 1 , 4 )) result . add_section ( memdump_section ) ... GRAPH_DATA \u00b6 Code used to generate the GRAPH_DATA section Excerpt from the Assemblyline ResultSample service: result_sample.py ... # ================================================================== # Color map Section: BODY_FORMAT.GRAPH_DATA # Creates a color map bar using a minimum and maximum domain # e.g. We are using this section to display the entropy distribution in some services cmap_min = 0 cmap_max = 20 color_map_data = { 'type' : 'colormap' , 'data' : { 'domain' : [ cmap_min , cmap_max ], 'values' : [ random . random () * cmap_max for _ in range ( 50 )] } } # The classification of a section can be set to any valid classification for your system section_color_map = ResultSection ( \"Example of colormap result section\" , body_format = BODY_FORMAT . GRAPH_DATA , body = json . dumps ( color_map_data ), classification = cl_engine . RESTRICTED ) result . add_section ( section_color_map ) ... URL \u00b6 Code used to generate the URL section Excerpt from the Assemblyline ResultSample service: result_sample.py ... # ================================================================== # URL section: BODY_FORMAT.URL # Generate a list of clickable URLs using a JSON encoded format # As you can see here, the body of the section can be set directly instead of line by line random_host = get_random_host () url_section = ResultSection ( 'Example of a simple url section' , body_format = BODY_FORMAT . URL , body = json . dumps ({ \"name\" : \"Random url!\" , \"url\" : f \"https:// { random_host } /\" })) # Since URLs are very important features, we can tag those features in the system so that they are easy to find # Tags are defined by a type and a value url_section . add_tag ( \"network.static.domain\" , random_host ) # You may also want to provide a list of URLs! # Also, no need to provide a name, the URL link will be displayed host1 = get_random_host () host2 = get_random_host () urls = [ { \"url\" : f \"https:// { host1 } /\" }, { \"url\" : f \"https:// { host2 } /\" }] # A heuristic can fire more then once without being associated to a signature url_heuristic = Heuristic ( 4 , frequency = len ( urls )) url_sub_section = ResultSection ( 'Example of a URL sub-section with multiple links' , body = json . dumps ( urls ), body_format = BODY_FORMAT . URL , heuristic = url_heuristic , classification = cl_engine . RESTRICTED ) url_sub_section . add_tag ( \"network.static.domain\" , host1 ) url_sub_section . add_tag ( \"network.dynamic.domain\" , host2 ) # Since url_sub_section is a subsection of url_section # we will add it as a subsection of url_section, not to the main result itself url_section . add_subsection ( url_sub_section ) result . add_section ( url_section ) ... JSON \u00b6 Code used to generate the JSON section Excerpt from the Assemblyline ResultSample service: result_sample.py ... # ================================================================== # JSON section: # Re-use the JSON editor we use for administration (https://github.com/josdejong/jsoneditor) # to display a tree view of JSON results. # NB: Use this sparingly! As a service developer you should do your best to include important # results as their own result sections. # The body argument must be a json dump of a python dictionary json_body = { \"a_str\" : \"Some string\" , \"a_list\" : [ \"a\" , \"b\" , \"c\" ], \"a_bool\" : False , \"an_int\" : 102 , \"a_dict\" : { \"list_of_dict\" : [ { \"d1_key\" : \"val\" , \"d1_key2\" : \"val2\" }, { \"d2_key\" : \"val\" , \"d2_key2\" : \"val2\" } ], \"bool\" : True } } json_section = ResultSection ( 'Example of a JSON section' , body_format = BODY_FORMAT . JSON , body = json . dumps ( json_body )) result . add_section ( json_section ) ... KEY_VALUE \u00b6 Code used to generate the KEY_VALUE section Excerpt from Assemblyline Result sample service: result_sample.py ... # ================================================================== # KEY_VALUE section: # This section allows the service writer to list a bunch of key/value pairs to be displayed in the UI # while also providing easy to parse data for auto mated tools. # NB: You should definitely use this over a JSON body type since this one will be displayed correctly # in the UI for the user # The body argument must be a json dumps of a dictionary (only str, int, and booleans are allowed) kv_body = { \"a_str\" : \"Some string\" , \"a_bool\" : False , \"an_int\" : 102 , } kv_section = ResultSection ( 'Example of a KEY_VALUE section' , body_format = BODY_FORMAT . KEY_VALUE , body = json . dumps ( kv_body )) result . add_section ( kv_section ) ... PROCESS_TREE \u00b6 Code used to generate the section Excerpt from Assemblyline Result sample service: result_sample.py ... # ================================================================== # PROCESS_TREE section: # This section allows the service writer to list a bunch of dictionary objects that have nested lists # of dictionaries to be displayed in the UI. Each dictionary object represents a process, and therefore # each dictionary must have be of the following format: # { # \"process_pid\": int, # \"process_name\": str, # \"command_line\": str, # \"children\": [] NB: This list either is empty or contains more dictionaries that have the same # structure # } nc_body = [ { \"process_pid\" : 123 , \"process_name\" : \"evil.exe\" , \"command_line\" : \"C: \\\\ evil.exe\" , \"signatures\" : {}, \"children\" : [ { \"process_pid\" : 321 , \"process_name\" : \"takeovercomputer.exe\" , \"command_line\" : \"C: \\\\ Temp \\\\ takeovercomputer.exe -f do_bad_stuff\" , \"signatures\" : { \"one\" : 250 }, \"children\" : [ { \"process_pid\" : 456 , \"process_name\" : \"evenworsethanbefore.exe\" , \"command_line\" : \"C: \\\\ Temp \\\\ evenworsethanbefore.exe -f change_reg_key_cuz_im_bad\" , \"signatures\" : { \"one\" : 10 , \"two\" : 10 , \"three\" : 10 }, \"children\" : [] }, { \"process_pid\" : 234 , \"process_name\" : \"badfile.exe\" , \"command_line\" : \"C: \\\\ badfile.exe -k nothing_to_see_here\" , \"signatures\" : { \"one\" : 1000 , \"two\" : 10 , \"three\" : 10 , \"four\" : 10 , \"five\" : 10 }, \"children\" : [] } ] }, { \"process_pid\" : 345 , \"process_name\" : \"benignexe.exe\" , \"command_line\" : \"C: \\\\ benignexe.exe -f \\\" just kidding, i'm evil \\\" \" , \"signatures\" : { \"one\" : 2000 }, \"children\" : [] } ] }, { \"process_pid\" : 987 , \"process_name\" : \"runzeroday.exe\" , \"command_line\" : \"C: \\\\ runzeroday.exe -f insert_bad_spelling\" , \"signatures\" : {}, \"children\" : [] } ] nc_section = ResultSection ( 'Example of a PROCESS_TREE section' , body_format = BODY_FORMAT . PROCESS_TREE , body = json . dumps ( nc_body )) result . add_section ( nc_section ) ... TABLE \u00b6 Code used to generate the TABLE section Excerpt from Assemblyline Result sample service: result_sample.py ... # ================================================================== # TABLE section: # This section allows the service writer to have their content displayed in a table format in the UI # The body argument must be a list [] of dict {} objects. A dict object can have a key value pair # where the value is a flat nested dictionary, and this nested dictionary will be displayed as a nested # table within a cell. table_body = [ { \"a_str\" : \"Some string1\" , \"extra_column_here\" : \"confirmed\" , \"a_bool\" : False , \"an_int\" : 101 , }, { \"a_str\" : \"Some string2\" , \"a_bool\" : True , \"an_int\" : 102 , }, { \"a_str\" : \"Some string3\" , \"a_bool\" : False , \"an_int\" : 103 , }, { \"a_str\" : \"Some string4\" , \"a_bool\" : None , \"an_int\" : - 1000000000000000000 , \"extra_column_there\" : \"confirmed\" , \"nested_table\" : { \"a_str\" : \"Some string3\" , \"a_bool\" : False , \"nested_table_thats_too_deep\" : { \"a_str\" : \"Some string3\" , \"a_bool\" : False , \"an_int\" : 103 , }, }, }, ] table_section = ResultSection ( 'Example of a TABLE section' , body_format = BODY_FORMAT . TABLE , body = json . dumps ( table_body )) result . add_section ( table_section ) ...","title":"ResultSection Class"},{"location":"developer_manual/services/advanced/result_section/#resultsection-class","text":"A ResultSection is bascally part of a service result that encapsulates a certain type of information that your service needs to convey to the user. For example, if you have a service that extracts networking indicators as well as process lists, you should put network indicators in their own section and then the process list in another. Result sections have the following properties: They have different types that will display information in different manners They can attach a heuristic which will add a maliciousness score to the result section They can tag important pieces of information about a file They can contain subsections which are just sections inside of another section They can have a classification which allows the API to redact partial results from a service depending on the user You can view the source for the class here: ResultSection class source","title":"ResultSection class"},{"location":"developer_manual/services/advanced/result_section/#class-variables","text":"The ResultSection class includes many instance variables which can be used to shape the way the section will be shown to the user. The following table describes all of the variables of the ResultSection class. Variable Name Description parent Parent ResultSection object (Only if the section is a child of another) subsections List of children ResultSection objects body Body of the section. Can take multiple forms depending on the section type : List of strings, string, JSON blob... classification The classification level of the current section body_format The types of body of the current section (Default: TEXT) tags Dictionary containing the different tags that have been added to the section heuristic Current heuristic assigned to the section zeroize_on_tag_safe Should the section be forced to a score of 0 if all tags found in it are marked as Safelisted? (Default: False) auto_collapse Should the section be displayed in collapsed mode when first rendered in the UI? (Default: False) zeroize_on_sig_safe Should the section be forced to a score of 0 if all heuristic signatures found in it are marked as Safelisted? (Default: True)","title":"Class variables"},{"location":"developer_manual/services/advanced/result_section/#class-functions","text":"","title":"Class functions"},{"location":"developer_manual/services/advanced/result_section/#__init__","text":"The constructor of the ResultSection object allows you to set all variables from the start. Parameters: title_text : (Required) Title of the section body : Body of the section classification : Classification of the section body_format : Type of body heuristic : Heuristic assigned to the section tags : Dictionary of tags assigned to the section parent : Parent of the section (either another section or the Result object) zeroize_on_tag_safe : Should the section be forced to a score of 0 if all tags found in it are marked as Safelisted? auto_collapse : Should the section be displayed in collapsed mode when first rendered in the UI? zeroize_on_sig_safe : Should the section be forced to a score of 0 if all heuristic signatures found in it are marked as Safelisted? Example Excerpt from the Assemblyline ResultSample service: result_sample.py ... # The classification of a section can be set to any valid classification for your system section_color_map = ResultSection ( \"Example of colormap result section\" , body_format = BODY_FORMAT . GRAPH_DATA , body = json . dumps ( color_map_data ), classification = cl_engine . RESTRICTED ) result . add_section ( section_color_map ) ...","title":"__init__()"},{"location":"developer_manual/services/advanced/result_section/#add_line","text":"This function allows the service to add a line to the body of a ResultSection object. Parameters: text : A string containing the line to add to the body Example Excerpt from the Assemblyline ResultSample service: result_sample.py ... text_section = ResultSection ( 'Example of a default section' ) # You can add lines to your section one at a time # Here we will generate a random line text_section . add_line ( get_random_phrase ()) ...","title":"add_line()"},{"location":"developer_manual/services/advanced/result_section/#add_lines","text":"This function allows the service to add multiple lines to the body of a ResultSection object. Parameters: lines : List of string to add as multiple lines in the body Example Excerpt from the Assemblyline ResultSample service: result_sample.py ... text_section = ResultSection ( 'Example of a default section' ) ... # Or your can add them from a list # Here we will generate random amount of random lines text_section . add_lines ([ get_random_phrase () for _ in range ( random . randint ( 1 , 5 ))]) ...","title":"add_lines()"},{"location":"developer_manual/services/advanced/result_section/#add_subsection","text":"This function allows the service to add a subsection to the current ResultSection object. Parameters: subsection : The ResultSection object to add as a subsection on_top : (Optional) Boolean value that indicates if the section should be on top of the other sections or not Example Excerpt from the Assemblyline ResultSample service: result_sample.py ... url_sub_section = ResultSection ( 'Example of a url sub-section with multiple links' , body = json . dumps ( urls ), body_format = BODY_FORMAT . URL , heuristic = url_heuristic , classification = cl_engine . RESTRICTED ) ... url_sub_sub_section = ResultSection ( 'example of a two level deep sub-section' , body = json . dumps ( ips ), body_format = BODY_FORMAT . URL ) ... # Since url_sub_sub_section is a sub-section of url_sub_section # we will add it as a sub-section of url_sub_section not to the main result itself url_sub_section . add_subsection ( url_sub_sub_section ) ...","title":"add_subsection()"},{"location":"developer_manual/services/advanced/result_section/#add_tag","text":"This function allows the service writer to add a tag to the ResultSection Parameters: type : Type of tag value : Value of the tag Example Excerpt from the Assemblyline ResultSample service: result_sample.py ... # You can tag data to a section. Tagging is used to quickly find defining information about a file text_section . add_tag ( \"attribution.implant\" , \"ResultSample\" ) ...","title":"add_tag()"},{"location":"developer_manual/services/advanced/result_section/#set_body","text":"Set the body and the body format of a section Parameters: body : New body value body_format : (Optional) Type of body - Default: TEXT","title":"set_body()"},{"location":"developer_manual/services/advanced/result_section/#set_heuristic","text":"Set a heuristic for a current section/subsection. A heuristic is required to assign a score to a result section/subsection. Parameters: heur_id : Heuristic ID as set in the service manifest attack_id : (optional) Attack ID related to the heuristic signature : (optional) Signature name that triggered the heuristic Example Excerpt from the Assemblyline ResultSample service: result_sample.py ... # If the section needs to affect the score of the file you need to set a heuristic # Here we will pick one at random # In addition to adding a heuristic, we will associate a signature to the heuristic. # We're doing this by adding the signature name to the heuristic. (Here we use a random name) text_section . set_heuristic ( 3 , signature = \"sig_one\" ) ...","title":"set_heuristic()"},{"location":"developer_manual/services/advanced/result_section/#section-types","text":"These are all result section types that Assemblyline supports. You can see a screenshot of each section as well as the code that was used to generate the actual section.","title":"Section types"},{"location":"developer_manual/services/advanced/result_section/#text","text":"Code used to generate the TEXT section Excerpt from the Assemblyline ResultSample service: result_sample.py ... # ================================================================== # Standard text section: BODY_FORMAT.TEXT - DEFAULT # Text sections basically just dump the text to the screen... # The scores of all sections will be SUMed in the service result # The Result classification will be the highest classification found in the sections text_section = ResultSection ( 'Example of a default section' ) # You can add lines to your section one at a time # Here we will generate a random line text_section . add_line ( get_random_phrase ()) # Or your can add them from a list # Here we will generate a random amount of random lines text_section . add_lines ([ get_random_phrase () for _ in range ( random . randint ( 1 , 5 ))]) # You can tag data to a section. Tagging is used to quickly find defining information about a file text_section . add_tag ( \"attribution.implant\" , \"ResultSample\" ) # If the section needs to affect the score of the file you need to set a heuristics # Here we will pick one at random # In addition to adding a heuristic, we will associate a signature to the heuristic. # We're doing this by adding the signature name to the heuristic. (Here we use a random name) text_section . set_heuristic ( 3 , signature = \"sig_one\" ) # You can attach ATT&CK IDs to heuristics after they where defined text_section . heuristic . add_attack_id ( random . choice ( list ( software_map . keys ()))) text_section . heuristic . add_attack_id ( random . choice ( list ( attack_map . keys ()))) text_section . heuristic . add_attack_id ( random . choice ( list ( group_map . keys ()))) text_section . heuristic . add_attack_id ( random . choice ( list ( revoke_map . keys ()))) # Same thing for the signatures, they can be added to a heuristic after the fact and you can even say how # many time the signature fired by setting its frequency. If you call add_signature_id twice with the # same signature, this will also increase the frequency of the signature. text_section . heuristic . add_signature_id ( \"sig_two\" , score = 20 , frequency = 2 ) text_section . heuristic . add_signature_id ( \"sig_two\" , score = 20 , frequency = 3 ) text_section . heuristic . add_signature_id ( \"sig_three\" ) text_section . heuristic . add_signature_id ( \"sig_three\" ) text_section . heuristic . add_signature_id ( \"sig_four\" , score = 0 ) # The heuristic for text_section should have the following properties: # 1. 1 ATT&CK ID: T1066 # 2. 4 signatures: sig_one, sig_two, sig_three and sig_four # 3. Signature frequencies are cumulative, therefore they will be as follows: # - sig_one = 1 # - sig_two = 5 # - sig_three = 2 # - sig_four = 1 # 4. The score used by each heuristic is driven by the following rules: signature_score_map is the highest # priority, then score value for the add_signature_id is in second place and finally the default # heuristic score is used. The score used to calculate the total score for the text_section is # as follows: # - sig_one: 10 -> heuristic default score # - sig_two: 20 -> score provided by the function add_signature_id # - sig_three: 30 -> score provided by the heuristic map # - sig_four: 40 -> score provided by the heuristic map because it's higher priority than the # function score # 5. Total section score is then: 1x10 + 5x20 + 2x30 + 1x40 = 210 # Make sure you add your section to the result result . add_section ( text_section ) ...","title":"TEXT"},{"location":"developer_manual/services/advanced/result_section/#memory_dump","text":"Code used to generate the MEMORY_DUMP section Excerpt from the Assemblyline ResultSample service: result_sample.py ... # ================================================================== # Memory dump section: BODY_FORMAT.MEMORY_DUMP # Dump whatever string content you have into a <pre/> html tag so you can do your own formatting data = hexdump ( b \"This is some random text that we will format as an hexdump and you'll see \" b \"that the hexdump formatting will be preserved by the memory dump section!\" ) memdump_section = ResultSection ( 'Example of a memory dump section' , body_format = BODY_FORMAT . MEMORY_DUMP , body = data ) memdump_section . set_heuristic ( random . randint ( 1 , 4 )) result . add_section ( memdump_section ) ...","title":"MEMORY_DUMP"},{"location":"developer_manual/services/advanced/result_section/#graph_data","text":"Code used to generate the GRAPH_DATA section Excerpt from the Assemblyline ResultSample service: result_sample.py ... # ================================================================== # Color map Section: BODY_FORMAT.GRAPH_DATA # Creates a color map bar using a minimum and maximum domain # e.g. We are using this section to display the entropy distribution in some services cmap_min = 0 cmap_max = 20 color_map_data = { 'type' : 'colormap' , 'data' : { 'domain' : [ cmap_min , cmap_max ], 'values' : [ random . random () * cmap_max for _ in range ( 50 )] } } # The classification of a section can be set to any valid classification for your system section_color_map = ResultSection ( \"Example of colormap result section\" , body_format = BODY_FORMAT . GRAPH_DATA , body = json . dumps ( color_map_data ), classification = cl_engine . RESTRICTED ) result . add_section ( section_color_map ) ...","title":"GRAPH_DATA"},{"location":"developer_manual/services/advanced/result_section/#url","text":"Code used to generate the URL section Excerpt from the Assemblyline ResultSample service: result_sample.py ... # ================================================================== # URL section: BODY_FORMAT.URL # Generate a list of clickable URLs using a JSON encoded format # As you can see here, the body of the section can be set directly instead of line by line random_host = get_random_host () url_section = ResultSection ( 'Example of a simple url section' , body_format = BODY_FORMAT . URL , body = json . dumps ({ \"name\" : \"Random url!\" , \"url\" : f \"https:// { random_host } /\" })) # Since URLs are very important features, we can tag those features in the system so that they are easy to find # Tags are defined by a type and a value url_section . add_tag ( \"network.static.domain\" , random_host ) # You may also want to provide a list of URLs! # Also, no need to provide a name, the URL link will be displayed host1 = get_random_host () host2 = get_random_host () urls = [ { \"url\" : f \"https:// { host1 } /\" }, { \"url\" : f \"https:// { host2 } /\" }] # A heuristic can fire more then once without being associated to a signature url_heuristic = Heuristic ( 4 , frequency = len ( urls )) url_sub_section = ResultSection ( 'Example of a URL sub-section with multiple links' , body = json . dumps ( urls ), body_format = BODY_FORMAT . URL , heuristic = url_heuristic , classification = cl_engine . RESTRICTED ) url_sub_section . add_tag ( \"network.static.domain\" , host1 ) url_sub_section . add_tag ( \"network.dynamic.domain\" , host2 ) # Since url_sub_section is a subsection of url_section # we will add it as a subsection of url_section, not to the main result itself url_section . add_subsection ( url_sub_section ) result . add_section ( url_section ) ...","title":"URL"},{"location":"developer_manual/services/advanced/result_section/#json","text":"Code used to generate the JSON section Excerpt from the Assemblyline ResultSample service: result_sample.py ... # ================================================================== # JSON section: # Re-use the JSON editor we use for administration (https://github.com/josdejong/jsoneditor) # to display a tree view of JSON results. # NB: Use this sparingly! As a service developer you should do your best to include important # results as their own result sections. # The body argument must be a json dump of a python dictionary json_body = { \"a_str\" : \"Some string\" , \"a_list\" : [ \"a\" , \"b\" , \"c\" ], \"a_bool\" : False , \"an_int\" : 102 , \"a_dict\" : { \"list_of_dict\" : [ { \"d1_key\" : \"val\" , \"d1_key2\" : \"val2\" }, { \"d2_key\" : \"val\" , \"d2_key2\" : \"val2\" } ], \"bool\" : True } } json_section = ResultSection ( 'Example of a JSON section' , body_format = BODY_FORMAT . JSON , body = json . dumps ( json_body )) result . add_section ( json_section ) ...","title":"JSON"},{"location":"developer_manual/services/advanced/result_section/#key_value","text":"Code used to generate the KEY_VALUE section Excerpt from Assemblyline Result sample service: result_sample.py ... # ================================================================== # KEY_VALUE section: # This section allows the service writer to list a bunch of key/value pairs to be displayed in the UI # while also providing easy to parse data for auto mated tools. # NB: You should definitely use this over a JSON body type since this one will be displayed correctly # in the UI for the user # The body argument must be a json dumps of a dictionary (only str, int, and booleans are allowed) kv_body = { \"a_str\" : \"Some string\" , \"a_bool\" : False , \"an_int\" : 102 , } kv_section = ResultSection ( 'Example of a KEY_VALUE section' , body_format = BODY_FORMAT . KEY_VALUE , body = json . dumps ( kv_body )) result . add_section ( kv_section ) ...","title":"KEY_VALUE"},{"location":"developer_manual/services/advanced/result_section/#process_tree","text":"Code used to generate the section Excerpt from Assemblyline Result sample service: result_sample.py ... # ================================================================== # PROCESS_TREE section: # This section allows the service writer to list a bunch of dictionary objects that have nested lists # of dictionaries to be displayed in the UI. Each dictionary object represents a process, and therefore # each dictionary must have be of the following format: # { # \"process_pid\": int, # \"process_name\": str, # \"command_line\": str, # \"children\": [] NB: This list either is empty or contains more dictionaries that have the same # structure # } nc_body = [ { \"process_pid\" : 123 , \"process_name\" : \"evil.exe\" , \"command_line\" : \"C: \\\\ evil.exe\" , \"signatures\" : {}, \"children\" : [ { \"process_pid\" : 321 , \"process_name\" : \"takeovercomputer.exe\" , \"command_line\" : \"C: \\\\ Temp \\\\ takeovercomputer.exe -f do_bad_stuff\" , \"signatures\" : { \"one\" : 250 }, \"children\" : [ { \"process_pid\" : 456 , \"process_name\" : \"evenworsethanbefore.exe\" , \"command_line\" : \"C: \\\\ Temp \\\\ evenworsethanbefore.exe -f change_reg_key_cuz_im_bad\" , \"signatures\" : { \"one\" : 10 , \"two\" : 10 , \"three\" : 10 }, \"children\" : [] }, { \"process_pid\" : 234 , \"process_name\" : \"badfile.exe\" , \"command_line\" : \"C: \\\\ badfile.exe -k nothing_to_see_here\" , \"signatures\" : { \"one\" : 1000 , \"two\" : 10 , \"three\" : 10 , \"four\" : 10 , \"five\" : 10 }, \"children\" : [] } ] }, { \"process_pid\" : 345 , \"process_name\" : \"benignexe.exe\" , \"command_line\" : \"C: \\\\ benignexe.exe -f \\\" just kidding, i'm evil \\\" \" , \"signatures\" : { \"one\" : 2000 }, \"children\" : [] } ] }, { \"process_pid\" : 987 , \"process_name\" : \"runzeroday.exe\" , \"command_line\" : \"C: \\\\ runzeroday.exe -f insert_bad_spelling\" , \"signatures\" : {}, \"children\" : [] } ] nc_section = ResultSection ( 'Example of a PROCESS_TREE section' , body_format = BODY_FORMAT . PROCESS_TREE , body = json . dumps ( nc_body )) result . add_section ( nc_section ) ...","title":"PROCESS_TREE"},{"location":"developer_manual/services/advanced/result_section/#table","text":"Code used to generate the TABLE section Excerpt from Assemblyline Result sample service: result_sample.py ... # ================================================================== # TABLE section: # This section allows the service writer to have their content displayed in a table format in the UI # The body argument must be a list [] of dict {} objects. A dict object can have a key value pair # where the value is a flat nested dictionary, and this nested dictionary will be displayed as a nested # table within a cell. table_body = [ { \"a_str\" : \"Some string1\" , \"extra_column_here\" : \"confirmed\" , \"a_bool\" : False , \"an_int\" : 101 , }, { \"a_str\" : \"Some string2\" , \"a_bool\" : True , \"an_int\" : 102 , }, { \"a_str\" : \"Some string3\" , \"a_bool\" : False , \"an_int\" : 103 , }, { \"a_str\" : \"Some string4\" , \"a_bool\" : None , \"an_int\" : - 1000000000000000000 , \"extra_column_there\" : \"confirmed\" , \"nested_table\" : { \"a_str\" : \"Some string3\" , \"a_bool\" : False , \"nested_table_thats_too_deep\" : { \"a_str\" : \"Some string3\" , \"a_bool\" : False , \"an_int\" : 103 , }, }, }, ] table_section = ResultSection ( 'Example of a TABLE section' , body_format = BODY_FORMAT . TABLE , body = json . dumps ( table_body )) result . add_section ( table_section ) ...","title":"TABLE"},{"location":"developer_manual/services/advanced/service_base/","text":"ServiceBase class \u00b6 All service created for Assemblyline must inherit from the ServiceBase class which can be imported from assemblyline_v4_service.common.base . In this section we will go through the different methods and variables available to you in the ServiceBase class. You can view the source for the class here: ServiceBase class source Class variables \u00b6 The ServiceBase base class includes many instance variables which can be used to access service related information. The following tables describes all of the variables of the ServiceBase class. Variable Name Description config Reference to the service parameters containing values updated by the user for service configuration. dependencies A dictionary containing connection details for service dependencies log Reference to the logger. rules_directory Returns the directory path which contains the current location of your rules rules_hash A hash of the files in the rules_list. Used to invalidate caching if rules change. rules_list Returns a list of directory paths which point to rule files derived from rules_directory service_attributes Service attributes from the service manifest . update_time An integer representing the epoch of when the last update occurred working_directory Returns the directory path which the service can use to temporarily store files during each task execution. Class functions \u00b6 This is the list of all the functions that you can override in your service. They are explained in order of importance and the likelihood at which you will override them. execute() \u00b6 This is where your service processing code lives! The execute function is called every time the service receives a new file to scan. To this function, a single Request object is provided as an input which provides the service all the information available about the file which was requested for scanning. It is inside this function that you will create a Result objects to send your scan results back to the system for storage and display in the user interface. This has to be done before the end of the function execution. Make sure you go through the tips on writing good results before starting to built your service. get_tool_version() \u00b6 The purpose of the get_tool_version function is to the return a string indicating the version of the tools used by the service or a hash of the signatures it uses. The tool version should be updated to reflect changes in the service tools or signatures, so that Assemblyline can rescan files on the new service version if they are submitted again. start() \u00b6 The start function is called when the Assemblyline service is initiated and should be used to prepare your service for task execution. get_api_interface() \u00b6 The purpose of the get_api_interface function is to give the service direct access to the service server component to perform API request to find out if a file or a tag is meant to be safelisted. Warning By using this function, your service will not work using the run_service_once command and will be completely tied to the service server API. stop() \u00b6 The stop function is called when the Assemblyline service is stopped and should be used to cleanup your service. The following functions are used if and only if you're using a dependency that's a service updater named 'updates'. For this reason, we reserve the dependency name 'updates' to be used for service updaters. _load_rules() \u00b6 The _load_rules function is called to process the rules_list in a specific way defined by the service. _clear_rules() \u00b6 The _clear_rules function is optionally called to remove the current ruleset from memory. Requires implementation by the service writer for use. _download_rules() \u00b6 The _download_rules function is called after each _cleanup call to check if there is new updates to be processed. If so, it will attempt to download and use the new ruleset otherwise it will revert to the old ruleset. It will call on _load_rules and _clear_rules during this attempt process.","title":"ServiceBase Class"},{"location":"developer_manual/services/advanced/service_base/#servicebase-class","text":"All service created for Assemblyline must inherit from the ServiceBase class which can be imported from assemblyline_v4_service.common.base . In this section we will go through the different methods and variables available to you in the ServiceBase class. You can view the source for the class here: ServiceBase class source","title":"ServiceBase class"},{"location":"developer_manual/services/advanced/service_base/#class-variables","text":"The ServiceBase base class includes many instance variables which can be used to access service related information. The following tables describes all of the variables of the ServiceBase class. Variable Name Description config Reference to the service parameters containing values updated by the user for service configuration. dependencies A dictionary containing connection details for service dependencies log Reference to the logger. rules_directory Returns the directory path which contains the current location of your rules rules_hash A hash of the files in the rules_list. Used to invalidate caching if rules change. rules_list Returns a list of directory paths which point to rule files derived from rules_directory service_attributes Service attributes from the service manifest . update_time An integer representing the epoch of when the last update occurred working_directory Returns the directory path which the service can use to temporarily store files during each task execution.","title":"Class variables"},{"location":"developer_manual/services/advanced/service_base/#class-functions","text":"This is the list of all the functions that you can override in your service. They are explained in order of importance and the likelihood at which you will override them.","title":"Class functions"},{"location":"developer_manual/services/advanced/service_base/#execute","text":"This is where your service processing code lives! The execute function is called every time the service receives a new file to scan. To this function, a single Request object is provided as an input which provides the service all the information available about the file which was requested for scanning. It is inside this function that you will create a Result objects to send your scan results back to the system for storage and display in the user interface. This has to be done before the end of the function execution. Make sure you go through the tips on writing good results before starting to built your service.","title":"execute()"},{"location":"developer_manual/services/advanced/service_base/#get_tool_version","text":"The purpose of the get_tool_version function is to the return a string indicating the version of the tools used by the service or a hash of the signatures it uses. The tool version should be updated to reflect changes in the service tools or signatures, so that Assemblyline can rescan files on the new service version if they are submitted again.","title":"get_tool_version()"},{"location":"developer_manual/services/advanced/service_base/#start","text":"The start function is called when the Assemblyline service is initiated and should be used to prepare your service for task execution.","title":"start()"},{"location":"developer_manual/services/advanced/service_base/#get_api_interface","text":"The purpose of the get_api_interface function is to give the service direct access to the service server component to perform API request to find out if a file or a tag is meant to be safelisted. Warning By using this function, your service will not work using the run_service_once command and will be completely tied to the service server API.","title":"get_api_interface()"},{"location":"developer_manual/services/advanced/service_base/#stop","text":"The stop function is called when the Assemblyline service is stopped and should be used to cleanup your service. The following functions are used if and only if you're using a dependency that's a service updater named 'updates'. For this reason, we reserve the dependency name 'updates' to be used for service updaters.","title":"stop()"},{"location":"developer_manual/services/advanced/service_base/#_load_rules","text":"The _load_rules function is called to process the rules_list in a specific way defined by the service.","title":"_load_rules()"},{"location":"developer_manual/services/advanced/service_base/#_clear_rules","text":"The _clear_rules function is optionally called to remove the current ruleset from memory. Requires implementation by the service writer for use.","title":"_clear_rules()"},{"location":"developer_manual/services/advanced/service_base/#_download_rules","text":"The _download_rules function is called after each _cleanup call to check if there is new updates to be processed. If so, it will attempt to download and use the new ruleset otherwise it will revert to the old ruleset. It will call on _load_rules and _clear_rules during this attempt process.","title":"_download_rules()"},{"location":"developer_manual/services/advanced/service_manifest/","text":"Service manifest \u00b6 Every service must have a service_manifest.yml file in its root directory. The manifest file presents essential information about the service to the Assemblyline core system, information the Assemblyline core system must have before it can run the service. The table below shows all the elements that the manifest file can contain, including a brief description of each. Field name Value type Required? Description accepts Keyword No Default: .* Regexes applied to Assemblyline style file type string. For example, .* will allow the service to accept all types of files. category Keyword No Default: Static Analysis Which category is the service part of? Must be one of Antivirus , Dynamic Analysis , External , Extraction , Filtering , Networking , or Static Analysis . config Mapping of Any No Dictionary of service configuration variables. The key names can be any Keyword and the value can be of Any type. default_result_classification Classification string No Default: UNRESTRICTED The default classification for the results generated by the service. If no classification is provided for a result section, this default classification is used. dependencies Mapping of Dependency Config No Refer to the dependency config section. description Text No Default: NA Detailed description of the service and its features. disable_cache Boolean No Default: false Should the result cache be disabled for this service? Only disable caching for services that will always provide different results each run. docker_config Docker Config Yes Refer to the Docker Config section. enabled Boolean No Default: false Should the service be enabled by default? file_required Boolean Does the service require access to the file to perform its task? If set to false , the service will only have access to the file metadata (e.g. hashes, size, type, etc.). heuristics List of Heuristic No List of heuristic(s) used in the service for scoring. Refer to the heuristic section. is_external Boolean No Default: false Does the service make API calls to other products not part of the Assemblyline infrastructure (e.g. VirusTotal, ...)? licence_count Integer No Default: 0 Number of concurrent services allowed to run at the same time. name Keyword Yes Name of the service. rejects Keyword No Default: empty|metadata/.* Regexes applied to Assemblyline style file type string. For example, empty|metadata/.* will reject all empty and metadata files. stage Keyword No Default: CORE At which stage should the service run. Must be one of: (1) FILTER , (2) EXTRACT , (3) CORE , (4) SECONDARY , (5) POST . Note that stages are executed in the numbered order shown. submission_params List of Submission Params No List of submission param(s) that define parameters that the user can change about the service for each of its scans. Refer to the submission_params section. timeout Integer No Default: 60 Maximum execution time the service has before the task is timed out. update_config Update Config No Refer to the update config section. version 1 Keyword Yes Version of the service. 1 the version in the manifest must be the same as the image tag in order to successfully pass registration on service update/load. Dependency config \u00b6 Field name Value type Required? Description container Docker Config Yes Refer to Docker Config section. volumes Mapping of Persistent Volume No Refer to the persistent volume section. Docker config \u00b6 Field name Value type Required? Description allow_internet_access Boolean No Default: false Should the container be allowed to access the internet? command List[Keyword] No Command that should be run when the container launches. cpu_cores Float No Default: 1.0 Amount of CPU that should be allocated to the container. environment List of Environment Variable No Refer to the environment variable section. image Keyword Yes Image name always prepended by ${REGISTRY} or ${PRIVATE_REGISTRY} if image not on DockerHub. Append the rest of the image path. Do not put a / between the rest of the image path and registry var. Image name always ends in :$SERVICE_TAG ports List[Keyword] No List of ports to bind from the container. ram_mb Integer No Default: 1024 Amount of RAM in MB that should be allocated to the container. Environment variable \u00b6 Field name Value type Required? Description name Keyword Yes Name of the variable. value Keyword Yes Value of the variable. Heuristic \u00b6 Field name Value type Required? Description attack_id Enum No Mitre's Att&ck matrix ID. classification Classification No Default: UNRESTRICTED description Text Yes Detailed description of the heuristic which addresses the technique used to score. filetype Keyword Yes Regex of the filetype which applies to this heuristic. heur_id Keyword Yes Unique ID for identifying the heuristic. max_score Integer No The maximum score the heuristic can have. name Keyword Yes Short name for the heuristic. score Integer Yes Score that should be applied when this heuristic is set. Persistent volume \u00b6 Field name Value type Required? Description mount_path Keyword Yes Path into the container to mount volume. capacity Keyword Yes Storage capacity required in bytes. storage_class Keyword Yes Submission params \u00b6 Field name Value type Required? Description default Any Yes Default value of the parameter. name Keyword Yes Variable name of the parameter. type Enum Yes Type of variable. Must be one of: bool , int , list , or str . value Any Yes Value of the variable as configured by the user or the default if not configured. Update config \u00b6 Field name Value type Required? Description generates_signatures Boolean No Default: false Should the downloaded files be used to create signatures in the system? sources List of Update Source No List of source(s) from which updates can be downloaded. Refer to the update source section. update_interval_seconds Integer Yes Interval in seconds at which the updater runs. wait_for_update Boolean False Should the service wait for its updater dependency to be running? signature_delimiter Enum Must be of: new_line , double_new_line , pipe , comma , space , none , file , custom Type of delimiter used for signaure downloads. custom_delimiter Keyword Optional Custom signature delimiter to use when signature_delimiter: custom Update source \u00b6 Field name Value type Required? Description headers List of Environment Variable No Refer to the environment variable section. name Keyword Yes Unique name of the source. password Keyword No The password required to access the file. pattern Keyword No Regex pattern to match against the file names of all downloaded files from this source. This is useful when you want to filter out some files from a repo which contains many files. private_key Keyword No Key for accessing file or Git repo. uri Keyword Yes URL of the update file. Some example URL formats are: git@github.com:sample/sample-repo.git , https://file-examples.com/wp-content/uploads/2017/02/zip_2MB.zip . username Keyword No The username required to access the file.","title":"Service manifest"},{"location":"developer_manual/services/advanced/service_manifest/#service-manifest","text":"Every service must have a service_manifest.yml file in its root directory. The manifest file presents essential information about the service to the Assemblyline core system, information the Assemblyline core system must have before it can run the service. The table below shows all the elements that the manifest file can contain, including a brief description of each. Field name Value type Required? Description accepts Keyword No Default: .* Regexes applied to Assemblyline style file type string. For example, .* will allow the service to accept all types of files. category Keyword No Default: Static Analysis Which category is the service part of? Must be one of Antivirus , Dynamic Analysis , External , Extraction , Filtering , Networking , or Static Analysis . config Mapping of Any No Dictionary of service configuration variables. The key names can be any Keyword and the value can be of Any type. default_result_classification Classification string No Default: UNRESTRICTED The default classification for the results generated by the service. If no classification is provided for a result section, this default classification is used. dependencies Mapping of Dependency Config No Refer to the dependency config section. description Text No Default: NA Detailed description of the service and its features. disable_cache Boolean No Default: false Should the result cache be disabled for this service? Only disable caching for services that will always provide different results each run. docker_config Docker Config Yes Refer to the Docker Config section. enabled Boolean No Default: false Should the service be enabled by default? file_required Boolean Does the service require access to the file to perform its task? If set to false , the service will only have access to the file metadata (e.g. hashes, size, type, etc.). heuristics List of Heuristic No List of heuristic(s) used in the service for scoring. Refer to the heuristic section. is_external Boolean No Default: false Does the service make API calls to other products not part of the Assemblyline infrastructure (e.g. VirusTotal, ...)? licence_count Integer No Default: 0 Number of concurrent services allowed to run at the same time. name Keyword Yes Name of the service. rejects Keyword No Default: empty|metadata/.* Regexes applied to Assemblyline style file type string. For example, empty|metadata/.* will reject all empty and metadata files. stage Keyword No Default: CORE At which stage should the service run. Must be one of: (1) FILTER , (2) EXTRACT , (3) CORE , (4) SECONDARY , (5) POST . Note that stages are executed in the numbered order shown. submission_params List of Submission Params No List of submission param(s) that define parameters that the user can change about the service for each of its scans. Refer to the submission_params section. timeout Integer No Default: 60 Maximum execution time the service has before the task is timed out. update_config Update Config No Refer to the update config section. version 1 Keyword Yes Version of the service. 1 the version in the manifest must be the same as the image tag in order to successfully pass registration on service update/load.","title":"Service manifest"},{"location":"developer_manual/services/advanced/service_manifest/#dependency-config","text":"Field name Value type Required? Description container Docker Config Yes Refer to Docker Config section. volumes Mapping of Persistent Volume No Refer to the persistent volume section.","title":"Dependency config"},{"location":"developer_manual/services/advanced/service_manifest/#docker-config","text":"Field name Value type Required? Description allow_internet_access Boolean No Default: false Should the container be allowed to access the internet? command List[Keyword] No Command that should be run when the container launches. cpu_cores Float No Default: 1.0 Amount of CPU that should be allocated to the container. environment List of Environment Variable No Refer to the environment variable section. image Keyword Yes Image name always prepended by ${REGISTRY} or ${PRIVATE_REGISTRY} if image not on DockerHub. Append the rest of the image path. Do not put a / between the rest of the image path and registry var. Image name always ends in :$SERVICE_TAG ports List[Keyword] No List of ports to bind from the container. ram_mb Integer No Default: 1024 Amount of RAM in MB that should be allocated to the container.","title":"Docker config"},{"location":"developer_manual/services/advanced/service_manifest/#environment-variable","text":"Field name Value type Required? Description name Keyword Yes Name of the variable. value Keyword Yes Value of the variable.","title":"Environment variable"},{"location":"developer_manual/services/advanced/service_manifest/#heuristic","text":"Field name Value type Required? Description attack_id Enum No Mitre's Att&ck matrix ID. classification Classification No Default: UNRESTRICTED description Text Yes Detailed description of the heuristic which addresses the technique used to score. filetype Keyword Yes Regex of the filetype which applies to this heuristic. heur_id Keyword Yes Unique ID for identifying the heuristic. max_score Integer No The maximum score the heuristic can have. name Keyword Yes Short name for the heuristic. score Integer Yes Score that should be applied when this heuristic is set.","title":"Heuristic"},{"location":"developer_manual/services/advanced/service_manifest/#persistent-volume","text":"Field name Value type Required? Description mount_path Keyword Yes Path into the container to mount volume. capacity Keyword Yes Storage capacity required in bytes. storage_class Keyword Yes","title":"Persistent volume"},{"location":"developer_manual/services/advanced/service_manifest/#submission-params","text":"Field name Value type Required? Description default Any Yes Default value of the parameter. name Keyword Yes Variable name of the parameter. type Enum Yes Type of variable. Must be one of: bool , int , list , or str . value Any Yes Value of the variable as configured by the user or the default if not configured.","title":"Submission params"},{"location":"developer_manual/services/advanced/service_manifest/#update-config","text":"Field name Value type Required? Description generates_signatures Boolean No Default: false Should the downloaded files be used to create signatures in the system? sources List of Update Source No List of source(s) from which updates can be downloaded. Refer to the update source section. update_interval_seconds Integer Yes Interval in seconds at which the updater runs. wait_for_update Boolean False Should the service wait for its updater dependency to be running? signature_delimiter Enum Must be of: new_line , double_new_line , pipe , comma , space , none , file , custom Type of delimiter used for signaure downloads. custom_delimiter Keyword Optional Custom signature delimiter to use when signature_delimiter: custom","title":"Update config"},{"location":"developer_manual/services/advanced/service_manifest/#update-source","text":"Field name Value type Required? Description headers List of Environment Variable No Refer to the environment variable section. name Keyword Yes Unique name of the source. password Keyword No The password required to access the file. pattern Keyword No Regex pattern to match against the file names of all downloaded files from this source. This is useful when you want to filter out some files from a repo which contains many files. private_key Keyword No Key for accessing file or Git repo. uri Keyword Yes URL of the update file. Some example URL formats are: git@github.com:sample/sample-repo.git , https://file-examples.com/wp-content/uploads/2017/02/zip_2MB.zip . username Keyword No The username required to access the file.","title":"Update source"},{"location":"developer_manual/services/advanced/service_updater_base/","text":"ServiceUpdater class \u00b6 Some services created for Assemblyline will require signatures/rules as part of it's analysis process. ServiceUpdater class which can be imported from assemblyline_v4_service.updater.updater . In this section we will go through the different methods and variables available to you in the ServiceUpdater class. You can view the source for the class here: ServiceUpdater class source Class variables \u00b6 The ServiceUpdater base class includes many instance variables which can be used to access service related information. The following tables describes all of the variables of the ServiceUpdater class. Variable Name Description updater_type The type of updater, typically the service's name lower-cased taken from the SERVICE_PATH environment variable default_pattern The default regex pattern used if a source doesn't provide one (Default: .*) Class functions \u00b6 This is the list of all the functions that you can override in your updater. They are explained in order of importance and the likelihood at which you will override them. Note: the updater are yours to define however you would like for your service, we have implemented the basics that work with our existing services but this does not mean you have to follow what's already defined. Override it! ie. [Safelist] (https://github.com/CybercentreCanada/assemblyline-service-safelist) import_update() [Implementation Required] \u00b6 The import_update function is called to import a set of files into Assemblyline. This involves creating a list of Signature objects and importing them via the Signature API by using the Assemblyline client. do_source_update() [Override Optional] \u00b6 The do_source_update function is called on a separate thread that checks external signature sources for changes. This will then fetch the new ruleset on modification and update the signature set in Assemblyline to make it available to both users and the do_local_update thread. is_valid() [Override Optional] \u00b6 The is_valid function is called to determine if a file from a source is a valid file. The definition of its validity can vary between services but it should be able to answer the question 'Can the service use this?'. do_local_update() [Override Optional] \u00b6 The do_local_update function is called on a separate thread that checks Assemblyline for local changes to signatures such as change in status or additions/removals to the ruleset. This will then fetch the new ruleset on modification and make it available to be served to service instances. Warning Failure to implement import_update() in your service's subclass of ServiceUpdater will render the updater unusable. For an example, see Adding a Service Updater","title":"ServiceUpdater Class"},{"location":"developer_manual/services/advanced/service_updater_base/#serviceupdater-class","text":"Some services created for Assemblyline will require signatures/rules as part of it's analysis process. ServiceUpdater class which can be imported from assemblyline_v4_service.updater.updater . In this section we will go through the different methods and variables available to you in the ServiceUpdater class. You can view the source for the class here: ServiceUpdater class source","title":"ServiceUpdater class"},{"location":"developer_manual/services/advanced/service_updater_base/#class-variables","text":"The ServiceUpdater base class includes many instance variables which can be used to access service related information. The following tables describes all of the variables of the ServiceUpdater class. Variable Name Description updater_type The type of updater, typically the service's name lower-cased taken from the SERVICE_PATH environment variable default_pattern The default regex pattern used if a source doesn't provide one (Default: .*)","title":"Class variables"},{"location":"developer_manual/services/advanced/service_updater_base/#class-functions","text":"This is the list of all the functions that you can override in your updater. They are explained in order of importance and the likelihood at which you will override them. Note: the updater are yours to define however you would like for your service, we have implemented the basics that work with our existing services but this does not mean you have to follow what's already defined. Override it! ie. [Safelist] (https://github.com/CybercentreCanada/assemblyline-service-safelist)","title":"Class functions"},{"location":"developer_manual/services/advanced/service_updater_base/#import_update-implementation-required","text":"The import_update function is called to import a set of files into Assemblyline. This involves creating a list of Signature objects and importing them via the Signature API by using the Assemblyline client.","title":"import_update() [Implementation Required]"},{"location":"developer_manual/services/advanced/service_updater_base/#do_source_update-override-optional","text":"The do_source_update function is called on a separate thread that checks external signature sources for changes. This will then fetch the new ruleset on modification and update the signature set in Assemblyline to make it available to both users and the do_local_update thread.","title":"do_source_update() [Override Optional]"},{"location":"developer_manual/services/advanced/service_updater_base/#is_valid-override-optional","text":"The is_valid function is called to determine if a file from a source is a valid file. The definition of its validity can vary between services but it should be able to answer the question 'Can the service use this?'.","title":"is_valid() [Override Optional]"},{"location":"developer_manual/services/advanced/service_updater_base/#do_local_update-override-optional","text":"The do_local_update function is called on a separate thread that checks Assemblyline for local changes to signatures such as change in status or additions/removals to the ruleset. This will then fetch the new ruleset on modification and make it available to be served to service instances. Warning Failure to implement import_update() in your service's subclass of ServiceUpdater will render the updater unusable. For an example, see Adding a Service Updater","title":"do_local_update() [Override Optional]"},{"location":"developer_manual/services/advanced/service_updater_upgrade/","text":"How-to: Upgrade Service Updater for Assemblyline 4.1+ \u00b6 For this tutorial, we'll take an existing official service that was ported to 4.1 as an example Sigma 4.0.1.stable10 . Where do I start? \u00b6 As a starting point, you would create an instance of the ServiceUpdater class. We recommend setting a default_pattern (default: *) as this is a fallback if a source didn't provide a pattern when searching for acceptable signature files. In Sigma's case, its signatures are typically in .yml format. from assemblyline_v4_service.updater.updater import ServiceUpdater class SigmaUpdateServer ( ServiceUpdater ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) def import_update ( self , files_sha256 , client , source , default_classification ): -> None pass def is_valid ( self , file_path ) -> bool : return super () . is_valid ( file_path ) #Returns true always if __name__ == '__main__' : with SigmaUpdateServer ( default_pattern = \"*.yml\" ) as server : server . serve_forever () What do I need to keep from my old updater? \u00b6 Importing into Assemblyine \u00b6 In most cases, the means of importing your signatures into Assemblyline's Signature API will still be needed. As with the following within the if block, you'll be able to copy-paste that section of code into a import_update() (with obvious changes to variables names to match the definition). Failure to implement a import_update() will raise a NotImplementedError indicating this is a must for an updater. Starting at line 259 of sigma_updater.py: if files_sha256 : LOGGER . info ( \"Found new Sigma rule files to process!\" ) sigma_importer = SigmaImporter ( al_client , logger = LOGGER ) for source , source_val in files_sha256 . items (): total_imported = 0 default_classification = source_default_classification [ source ] for file in source_val . keys (): try : total_imported += sigma_importer . import_file ( file , source , default_classification = default_classification ) except ValueError : LOGGER . warning ( f \" { file } failed to import due to a Sigma error\" ) except ComposerError : LOGGER . warning ( f \" { file } failed to import due to a YAML-parsing error\" ) LOGGER . info ( f \" { total_imported } signatures were imported for source { source } \" ) else : LOGGER . info ( 'No new Sigma rule files to process' ) In the new updater: from assemblyline.common import forge from assemblyline_v4_service.updater.updater import ServiceUpdater from sigma_.sigma_importer import SigmaImporter from pysigma.exceptions import UnsupportedFeature from yaml.composer import ComposerError classification = forge . get_classification () class SigmaUpdateServer ( ServiceUpdater ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) def import_update ( self , files_sha256 , client , source , default_classification = classification . UNRESTRICTED ): -> None sigma_importer = SigmaImporter ( client , logger = self . log ) total_imported = 0 for file , _ in files_sha256 : try : total_imported += sigma_importer . import_file ( file , source , default_classification ) except ValueError : self . log . warning ( f \" { file } failed to import due to a Sigma error\" ) except ComposerError : self . log . warning ( f \" { file } failed to import due to a YAML-parsing error\" ) except UnsupportedFeature as e : self . log . warning ( f ' { file } | { e } ' ) self . log . info ( f \" { total_imported } signatures were imported for source { source } \" ) def is_valid ( self , file_path ) -> bool : return super () . is_valid ( file_path ) #Returns true always if __name__ == '__main__' : with SigmaUpdateServer ( default_pattern = \"*.yml\" ) as server : server . serve_forever () Validating signature file (Optional) \u00b6 Some updaters might've had a means of validating signature files before adding it as part of the signature collection before import. In Sigma's case, it called val_file , after downloading its signature files by either a URL download or a Git clone. If deemed a valid file for the service, then it would add it as part of the signature collection, otherwise it was ignored. Starting at line 236 of sigma_updater.py: if uri . endswith ( '.git' ): files = git_clone_repo ( source , previous_update = previous_update ) for file , sha256 in files : files_sha256 . setdefault ( source_name , {}) if previous_hash . get ( source_name , {}) . get ( file , None ) != sha256 : try : if val_file ( file ): files_sha256 [ source_name ][ file ] = sha256 else : LOGGER . warning ( f \" { file } was not imported due to failed validation\" ) except UnsupportedFeature as e : LOGGER . warning ( f \" { file } | { e } \" ) else : files = url_download ( source , previous_update = previous_update ) for file , sha256 in files : files_sha256 . setdefault ( source_name , {}) if previous_hash . get ( source_name , {}) . get ( file , None ) != sha256 : if val_file ( file ): files_sha256 [ source_name ][ file ] = sha256 else : LOGGER . warning ( f \" { file } was not imported due to failed validation\" ) In the new updater, you can override the class' is_valid() call to suit your validation needs: from yaml.composer import ComposerError from assemblyline.common import forge from assemblyline_v4_service.updater.updater import ServiceUpdater from sigma_.sigma_importer import SigmaImporter from pysigma.pysigma import val_file from pysigma.exceptions import UnsupportedFeature classification = forge . get_classification () class SigmaUpdateServer ( ServiceUpdater ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) def import_update ( self , files_sha256 , client , source , default_classification = classification . UNRESTRICTED ): sigma_importer = SigmaImporter ( client , logger = self . log ) total_imported = 0 for file , _ in files_sha256 : try : total_imported += sigma_importer . import_file ( file , source , default_classification ) except ValueError : self . log . warning ( f \" { file } failed to import due to a Sigma error\" ) except ComposerError : self . log . warning ( f \" { file } failed to import due to a YAML-parsing error\" ) except UnsupportedFeature as e : self . log . warning ( f ' { file } | { e } ' ) self . log . info ( f \" { total_imported } signatures were imported for source { source } \" ) def is_valid ( self , file_path ) -> bool : try : return val_file ( file_path ) except UnsupportedFeature : return False if __name__ == '__main__' : with SigmaUpdateServer ( default_pattern = \"*.yml\" ) as server : server . serve_forever () Is there anything I need to change for the actual service code? \u00b6 There would be a need to implement a load_rules() since services can use their rules in different ways so standardizing on a method isn't feasible at the moment. In the case of Sigma: def _load_rules ( self ) -> None : temp_list = [] # Patch source_name into signature and import for rule in self . rules_list : with open ( rule , 'r' ) as yaml_fh : file = yaml_fh . read () source_name = rule . split ( '/' )[ - 2 ] patched_rule = f ' { file } \\n signature_source: { source_name } ' temp_list . append ( patched_rule ) self . log . info ( f \"Number of rules to be loaded: { len ( temp_list ) } \" ) for rule in temp_list : try : self . sigma_parser . add_signature ( rule ) except Exception as e : self . log . warning ( f \" { e } | { rule } \" ) self . log . info ( f \"Number of rules successfully loaded: { len ( self . sigma_parser . rules ) } \" ) Note: After each submission, during a _cleanup() , there is a call made to _download_rules() which will ask your updater if there's new signatures to retrieve. If there's nothing new, then you will proceed to use the same signature set. Otherwise, the service wil retrieve the new set and attempt to load it into the service. For this reason, there can be the need to implement a _clear_rules() to ensure service stability if there was a failure while loading the new set that could have a negative impact on analysis. What don't I need to implement and why? \u00b6 do_source_update() \u00b6 Considering most of our services have the same pattern of: 1. Establishing a client for Assemblyline using the service_update_account 2. Iterating over our source list and using either a URL download or Git clone to retrieve signatures 3. (Optional) If there are rules to be updated, modify the rules before import to add extra metadata or validation 4. Importing the rules into Assemblyline We decided to streamline that process for most services while giving service writers the ability to override certain aspects as needed. The service base contains a set of helper functions for retrieving files during the download process. So you'll notice most services don't need to implement their own do_source_update() , they just need to implement an import_update() . Safelist is an example of a service that is an exception to this rule where it does implement its own do_source_update() . do_local_update() \u00b6 This particular function doesn't need to be overridden (but you're free to do so) because its primary function is retrieve the signature set from Assemblyline and make it accessible to service instances when they ask for signatures. This is separate from do_source_update() as that function is meant to retrieve / check for changes from sources outside Assemblyline whereas do_local_update() is checking for internal changes to the signature set (ie. a change in status from 'DEPLOYED' to 'DISABLED'). We invite you to look at the ServiceUpdater code to have a better understanding of how the new service updaters work and how they use the functions you implement ( is_valid , import_update ). Do I have to change the service manifest for 4.1? \u00b6 Yes, very much so. We've gone away from considering the service updater as a special dependency that needs its own config section. We now consider it to just be part of the service's list of dependencies, although we reserve the container name 'updates' to indicate this dependency is an updater to the service. Service Manifest for 4.0: update_config : generates_signatures : true method : run run_options : allow_internet_access : true command : [ \"python\" , \"-m\" , \"sigma_updater\" ] image : ${REGISTRY}cccs/assemblyline-service-sigma:$SERVICE_TAG sources : - name : sigma pattern : .*windows\\/.*\\.yml uri : https://github.com/SigmaHQ/sigma.git update_interval_seconds : 21600 # Quarter-day (every 6 hours) will now become in 4.1: dependencies : updates : container : allow_internet_access : true command : [ \"python\" , \"-m\" , \"sigma_.update_server\" ] image : ${REGISTRY}cccs/assemblyline-service-sigma:$SERVICE_TAG ports : [ \"5003\" ] run_as_core : True update_config : generates_signatures : true sources : - name : sigma pattern : .*windows\\/.*\\.yml uri : https://github.com/SigmaHQ/sigma.git update_interval_seconds : 21600 # Quarter-day (every 6 hours) signature_delimiter : \"file\" Notes: - Source management will still be part of the update_config, but container configuration will be moved to a list of dependencies. - signature_delimiter indicates how signatures should be separated when downloaded from Assemblyline's Signature API (default: double newline). - In Sigma's case, it gets its list of files in the following form: /updates/<random_tempdir>/sigma/<source_name>/<signature_name> where signature_name only contains the signature associated to the name - Under the default delimiter, it would get: /updates/<random_tempdir>/sigma/<source_name> where source_name is a compiled list of all signatures from the source separated by double newlines in a single file (which might be fine for some services like YARA and Suricata) In order for updaters to work, they need to be able to communicate with other core components. So you'll need to enable the dependency to be able to run_as_core , otherwise this could lead to issues where the updater isn't able to resolve to other components like Redis and/or Elasticsearch by name. Setting port(s) helps facilitate communication between the service and its dependency over certain ports and so, as a result, Scaler will create the appropriate NetworkPolicy to ensure communication only between a service and its dependencies.","title":"How-to: Upgrade Service Updater for Assemblyline 4.1+"},{"location":"developer_manual/services/advanced/service_updater_upgrade/#how-to-upgrade-service-updater-for-assemblyline-41","text":"For this tutorial, we'll take an existing official service that was ported to 4.1 as an example Sigma 4.0.1.stable10 .","title":"How-to: Upgrade Service Updater for Assemblyline 4.1+"},{"location":"developer_manual/services/advanced/service_updater_upgrade/#where-do-i-start","text":"As a starting point, you would create an instance of the ServiceUpdater class. We recommend setting a default_pattern (default: *) as this is a fallback if a source didn't provide a pattern when searching for acceptable signature files. In Sigma's case, its signatures are typically in .yml format. from assemblyline_v4_service.updater.updater import ServiceUpdater class SigmaUpdateServer ( ServiceUpdater ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) def import_update ( self , files_sha256 , client , source , default_classification ): -> None pass def is_valid ( self , file_path ) -> bool : return super () . is_valid ( file_path ) #Returns true always if __name__ == '__main__' : with SigmaUpdateServer ( default_pattern = \"*.yml\" ) as server : server . serve_forever ()","title":"Where do I start?"},{"location":"developer_manual/services/advanced/service_updater_upgrade/#what-do-i-need-to-keep-from-my-old-updater","text":"","title":"What do I need to keep from my old updater?"},{"location":"developer_manual/services/advanced/service_updater_upgrade/#importing-into-assemblyine","text":"In most cases, the means of importing your signatures into Assemblyline's Signature API will still be needed. As with the following within the if block, you'll be able to copy-paste that section of code into a import_update() (with obvious changes to variables names to match the definition). Failure to implement a import_update() will raise a NotImplementedError indicating this is a must for an updater. Starting at line 259 of sigma_updater.py: if files_sha256 : LOGGER . info ( \"Found new Sigma rule files to process!\" ) sigma_importer = SigmaImporter ( al_client , logger = LOGGER ) for source , source_val in files_sha256 . items (): total_imported = 0 default_classification = source_default_classification [ source ] for file in source_val . keys (): try : total_imported += sigma_importer . import_file ( file , source , default_classification = default_classification ) except ValueError : LOGGER . warning ( f \" { file } failed to import due to a Sigma error\" ) except ComposerError : LOGGER . warning ( f \" { file } failed to import due to a YAML-parsing error\" ) LOGGER . info ( f \" { total_imported } signatures were imported for source { source } \" ) else : LOGGER . info ( 'No new Sigma rule files to process' ) In the new updater: from assemblyline.common import forge from assemblyline_v4_service.updater.updater import ServiceUpdater from sigma_.sigma_importer import SigmaImporter from pysigma.exceptions import UnsupportedFeature from yaml.composer import ComposerError classification = forge . get_classification () class SigmaUpdateServer ( ServiceUpdater ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) def import_update ( self , files_sha256 , client , source , default_classification = classification . UNRESTRICTED ): -> None sigma_importer = SigmaImporter ( client , logger = self . log ) total_imported = 0 for file , _ in files_sha256 : try : total_imported += sigma_importer . import_file ( file , source , default_classification ) except ValueError : self . log . warning ( f \" { file } failed to import due to a Sigma error\" ) except ComposerError : self . log . warning ( f \" { file } failed to import due to a YAML-parsing error\" ) except UnsupportedFeature as e : self . log . warning ( f ' { file } | { e } ' ) self . log . info ( f \" { total_imported } signatures were imported for source { source } \" ) def is_valid ( self , file_path ) -> bool : return super () . is_valid ( file_path ) #Returns true always if __name__ == '__main__' : with SigmaUpdateServer ( default_pattern = \"*.yml\" ) as server : server . serve_forever ()","title":"Importing into Assemblyine"},{"location":"developer_manual/services/advanced/service_updater_upgrade/#validating-signature-file-optional","text":"Some updaters might've had a means of validating signature files before adding it as part of the signature collection before import. In Sigma's case, it called val_file , after downloading its signature files by either a URL download or a Git clone. If deemed a valid file for the service, then it would add it as part of the signature collection, otherwise it was ignored. Starting at line 236 of sigma_updater.py: if uri . endswith ( '.git' ): files = git_clone_repo ( source , previous_update = previous_update ) for file , sha256 in files : files_sha256 . setdefault ( source_name , {}) if previous_hash . get ( source_name , {}) . get ( file , None ) != sha256 : try : if val_file ( file ): files_sha256 [ source_name ][ file ] = sha256 else : LOGGER . warning ( f \" { file } was not imported due to failed validation\" ) except UnsupportedFeature as e : LOGGER . warning ( f \" { file } | { e } \" ) else : files = url_download ( source , previous_update = previous_update ) for file , sha256 in files : files_sha256 . setdefault ( source_name , {}) if previous_hash . get ( source_name , {}) . get ( file , None ) != sha256 : if val_file ( file ): files_sha256 [ source_name ][ file ] = sha256 else : LOGGER . warning ( f \" { file } was not imported due to failed validation\" ) In the new updater, you can override the class' is_valid() call to suit your validation needs: from yaml.composer import ComposerError from assemblyline.common import forge from assemblyline_v4_service.updater.updater import ServiceUpdater from sigma_.sigma_importer import SigmaImporter from pysigma.pysigma import val_file from pysigma.exceptions import UnsupportedFeature classification = forge . get_classification () class SigmaUpdateServer ( ServiceUpdater ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) def import_update ( self , files_sha256 , client , source , default_classification = classification . UNRESTRICTED ): sigma_importer = SigmaImporter ( client , logger = self . log ) total_imported = 0 for file , _ in files_sha256 : try : total_imported += sigma_importer . import_file ( file , source , default_classification ) except ValueError : self . log . warning ( f \" { file } failed to import due to a Sigma error\" ) except ComposerError : self . log . warning ( f \" { file } failed to import due to a YAML-parsing error\" ) except UnsupportedFeature as e : self . log . warning ( f ' { file } | { e } ' ) self . log . info ( f \" { total_imported } signatures were imported for source { source } \" ) def is_valid ( self , file_path ) -> bool : try : return val_file ( file_path ) except UnsupportedFeature : return False if __name__ == '__main__' : with SigmaUpdateServer ( default_pattern = \"*.yml\" ) as server : server . serve_forever ()","title":"Validating signature file (Optional)"},{"location":"developer_manual/services/advanced/service_updater_upgrade/#is-there-anything-i-need-to-change-for-the-actual-service-code","text":"There would be a need to implement a load_rules() since services can use their rules in different ways so standardizing on a method isn't feasible at the moment. In the case of Sigma: def _load_rules ( self ) -> None : temp_list = [] # Patch source_name into signature and import for rule in self . rules_list : with open ( rule , 'r' ) as yaml_fh : file = yaml_fh . read () source_name = rule . split ( '/' )[ - 2 ] patched_rule = f ' { file } \\n signature_source: { source_name } ' temp_list . append ( patched_rule ) self . log . info ( f \"Number of rules to be loaded: { len ( temp_list ) } \" ) for rule in temp_list : try : self . sigma_parser . add_signature ( rule ) except Exception as e : self . log . warning ( f \" { e } | { rule } \" ) self . log . info ( f \"Number of rules successfully loaded: { len ( self . sigma_parser . rules ) } \" ) Note: After each submission, during a _cleanup() , there is a call made to _download_rules() which will ask your updater if there's new signatures to retrieve. If there's nothing new, then you will proceed to use the same signature set. Otherwise, the service wil retrieve the new set and attempt to load it into the service. For this reason, there can be the need to implement a _clear_rules() to ensure service stability if there was a failure while loading the new set that could have a negative impact on analysis.","title":"Is there anything I need to change for the actual service code?"},{"location":"developer_manual/services/advanced/service_updater_upgrade/#what-dont-i-need-to-implement-and-why","text":"","title":"What don't I need to implement and why?"},{"location":"developer_manual/services/advanced/service_updater_upgrade/#do_source_update","text":"Considering most of our services have the same pattern of: 1. Establishing a client for Assemblyline using the service_update_account 2. Iterating over our source list and using either a URL download or Git clone to retrieve signatures 3. (Optional) If there are rules to be updated, modify the rules before import to add extra metadata or validation 4. Importing the rules into Assemblyline We decided to streamline that process for most services while giving service writers the ability to override certain aspects as needed. The service base contains a set of helper functions for retrieving files during the download process. So you'll notice most services don't need to implement their own do_source_update() , they just need to implement an import_update() . Safelist is an example of a service that is an exception to this rule where it does implement its own do_source_update() .","title":"do_source_update()"},{"location":"developer_manual/services/advanced/service_updater_upgrade/#do_local_update","text":"This particular function doesn't need to be overridden (but you're free to do so) because its primary function is retrieve the signature set from Assemblyline and make it accessible to service instances when they ask for signatures. This is separate from do_source_update() as that function is meant to retrieve / check for changes from sources outside Assemblyline whereas do_local_update() is checking for internal changes to the signature set (ie. a change in status from 'DEPLOYED' to 'DISABLED'). We invite you to look at the ServiceUpdater code to have a better understanding of how the new service updaters work and how they use the functions you implement ( is_valid , import_update ).","title":"do_local_update()"},{"location":"developer_manual/services/advanced/service_updater_upgrade/#do-i-have-to-change-the-service-manifest-for-41","text":"Yes, very much so. We've gone away from considering the service updater as a special dependency that needs its own config section. We now consider it to just be part of the service's list of dependencies, although we reserve the container name 'updates' to indicate this dependency is an updater to the service. Service Manifest for 4.0: update_config : generates_signatures : true method : run run_options : allow_internet_access : true command : [ \"python\" , \"-m\" , \"sigma_updater\" ] image : ${REGISTRY}cccs/assemblyline-service-sigma:$SERVICE_TAG sources : - name : sigma pattern : .*windows\\/.*\\.yml uri : https://github.com/SigmaHQ/sigma.git update_interval_seconds : 21600 # Quarter-day (every 6 hours) will now become in 4.1: dependencies : updates : container : allow_internet_access : true command : [ \"python\" , \"-m\" , \"sigma_.update_server\" ] image : ${REGISTRY}cccs/assemblyline-service-sigma:$SERVICE_TAG ports : [ \"5003\" ] run_as_core : True update_config : generates_signatures : true sources : - name : sigma pattern : .*windows\\/.*\\.yml uri : https://github.com/SigmaHQ/sigma.git update_interval_seconds : 21600 # Quarter-day (every 6 hours) signature_delimiter : \"file\" Notes: - Source management will still be part of the update_config, but container configuration will be moved to a list of dependencies. - signature_delimiter indicates how signatures should be separated when downloaded from Assemblyline's Signature API (default: double newline). - In Sigma's case, it gets its list of files in the following form: /updates/<random_tempdir>/sigma/<source_name>/<signature_name> where signature_name only contains the signature associated to the name - Under the default delimiter, it would get: /updates/<random_tempdir>/sigma/<source_name> where source_name is a compiled list of all signatures from the source separated by double newlines in a single file (which might be fine for some services like YARA and Suricata) In order for updaters to work, they need to be able to communicate with other core components. So you'll need to enable the dependency to be able to run_as_core , otherwise this could lead to issues where the updater isn't able to resolve to other components like Redis and/or Elasticsearch by name. Setting port(s) helps facilitate communication between the service and its dependency over certain ports and so, as a result, Scaler will create the appropriate NetworkPolicy to ensure communication only between a service and its dependencies.","title":"Do I have to change the service manifest for 4.1?"},{"location":"installation/appliance/","text":"Appliance installation (MicroK8s) \u00b6 This is the documentation for an appliance instance of the Assemblyline platform suited for smaller-scale deployments. Since we've used microk8s as the backend for this, the appliance setup can later be scaled to multiple nodes. Setup requirements \u00b6 Caveat The documentation provided here assumes that you are installing your appliance on an Ubuntu-based system and was only tested on Ubuntu 20.04. You might have to change the commands a bit if you use other Linux distributions. The recommended minimum system requirement for this appliance is 6 CPU and 12 GB of Ram. Install pre-requisites: \u00b6 Online Install microk8s: sudo snap install microk8s --classic Install microk8s addons: sudo microk8s enable dns ha-cluster storage metrics-server Install Helm and set it up to use with microk8s: sudo snap install helm --classic sudo mkdir /var/snap/microk8s/current/bin sudo ln -s /snap/bin/helm /var/snap/microk8s/current/bin/helm Install git: sudo apt install git Install ingress controller: sudo microk8s kubectl create ns ingress sudo microk8s helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx sudo microk8s helm repo update sudo microk8s helm install ingress-nginx ingress-nginx/ingress-nginx --set controller.hostPort.enabled = true -n ingress Offline Download offline packages (On an internet-connected system): Create a directory to house everything to be transferred mkdir al_deps cd al_deps Download offline snap packages for snap_pkg in \"microk8s\" \"helm\" \"kubectl\" do sudo snap download $snap_pkg done Fetch helm charts # Clone the Assemblyline helm chart repo git clone https://github.com/CybercentreCanada/assemblyline-helm-chart # Download dependency helm charts helm dependency update assemblyline-helm-chart/assemblyline/ wget https://github.com/kubernetes/ingress-nginx/releases/download/helm-chart-4.0.12/ingress-nginx-4.0.12.tgz Fetch container images # Calico CNI for container_image in \"cni\" \"pod2daemon-flexvol\" \"node\" do docker pull calico/ $container_image :v3.19.1 && docker save calico/ $container_image :v3.19.1 >> calico_ $container_image .tar done docker pull calico/kube-controllers:v3.17.3 && docker save calico/kube-controllers:v3.17.3 >> calico_kube-controllers.tar # NGINX-Ingress, refer to values.yaml in `charts` directory for container_image in \"controller:v1.1.0\" \"kube-webhook-certgen:v1.1.1\" do docker pull k8s.gcr.io/ingress-nginx/ $container_image && docker save k8s.gcr.io/ingress-nginx/ $container_image >> $container_image .tar done # microk8s add-ons, refer to images used in corresponding .yaml files (https://github.com/ubuntu/microk8s/tree/master/microk8s-resources/actions) export ARCH = amd64 # DNS, Storage, coreDNS, metrics-server container images for container_image in \"k8s-dns-kube-dns\" \"k8s-dns-dnsmasq-nanny\" \"k8s-dns-sidecar\" do docker pull gcr.io/google_containers/ $container_image - $ARCH :1.14.7 && docker save gcr.io/google_containers/ $container_image - $ARCH :1.14.7 >> $container_image .tar done docker pull k8s.gcr.io/pause:3.1 && docker save k8s.gcr.io/pause:3.1 >> pause.tar docker pull coredns/coredns:1.8.0 && docker save coredns/coredns:1.8.0 >> coredns.tar docker pull cdkbot/hostpath-provisioner- $ARCH :1.0.0 && docker save cdkbot/hostpath-provisioner- $ARCH :1.0.0 >> storage.tar docker pull k8s.gcr.io/metrics-server/metrics-server:v0.5.2 && docker save k8s.gcr.io/metrics-server/metrics-server:v0.5.2 >> metrics.tar # Assemblyline Core (release: 4.2.stable) for al_image in \"core\" \"ui\" \"ui-frontend\" \"service-server\" \"socketio\" do docker pull cccs/assemblyline- $al_image :4.2.stable && docker save cccs/assemblyline- $al_image :4.2.stable >> al_ $al_image .tar done # Elastic ES_REG = docker.elastic.co ES_VER = 7 .15.0 for beat in \"filebeat\" \"metricbeat\" do docker pull $ES_REG /beats/ $beat : $ES_VER && docker save $ES_REG /beats/ $beat : $ES_VER >> es_ $beat .tar done for es in \"logstash\" \"kibana\" \"elasticsearch\" do docker pull $ES_REG / $es / $es : $ES_VER && docker save $ES_REG / $es / $es : $ES_VER >> es_ $es .tar done # Filestore image (MinIO) for minio_image in \"minio:RELEASE.2021-02-14T04-01-33Z\" \"mc:RELEASE.2021-02-14T04-28-06Z\" do docker pull minio/ $minio_image && docker save minio/ $minio_image >> minio_ $minio_image .tar done # Redis image docker pull redis && docker save redis >> redis.tar (Optional) Container Registry Image docker pull registry && docker save registry >> registry.tar (Optional) Pull official service images for svc_image in apkaye beaver characterize configextractor cuckoo deobfuscripter emlparser espresso extract floss frankenstrings iparse metadefender metapeek oletools pdfid peepdf pefile pixaxe safelist sigma suricata swiffer tagcheck torrentslicer unpacme unpacker vipermonkey virustotal-dynamic virustotal-static xlmmacrodeobfuscator yara do docker pull cccs/assemblyline-service- $svc_image :stable && docker save cccs/assemblyline-service- $svc_image :stable >> $svc_image .tar done Load images into a container registry (Optional) Setup local container registry # Assumes Docker is installed on hosting system for container_image in registry.tar ) : do sudo docker load -i $container_image done # Start up registry container sudo docker run -dp 32000 :5000 --restart = always --name registry registry Load images from disk and push to registry REGISTRY = localhost:32000 # Re-tag images and push to local registry for image in $( docker image ls --format {{ .Repository }} : {{ .Tag }} ) do image_tag = $image if [ $( grep -o '/' <<< $image | wc -l ) -eq 2 ] then image_tag = $( cut -d '/' -f 2 - <<< $image ) fi sudo docker tag $image $REGISTRY / $image_tag && docker push $REGISTRY / $image_tag sudo docker image rm $image sudo docker image rm $REGISTRY / $image_tag done Setup computing host(s): Install microk8s sudo snap ack microk8s_*.assert sudo snap install microk8s_*.snap --classic Modify Container Registry Endpoints sudo vim /var/snap/microk8s/current/args/containerd-template.toml Replace REGISTRY with the domain/port of your container registry (ie. localhost:32000) [plugins.\"io.containerd.grpc.v1.cri\".registry.mirrors] [plugins.\"io.containerd.grpc.v1.cri\".registry.mirrors.\"docker.io\"] endpoint = [ \"https://registry-1.docker.io\" , \"http://<REGISTRY>\" , ] [plugins.\"io.containerd.grpc.v1.cri\".registry.mirrors.\"*\"] endpoint = [ \"http://<REGISTRY>\" ] Restart containerd to acknowledge the changes sudo systemctl restart containerd Enable microk8s add-ons sudo microk8s reset && sudo microk8s enable dns ha-cluster storage metrics-server Fetch kubeconfig for administration cp /var/snap/microk8s/current/credentials/client.config <remote_destination> Install admin tools (separate or same host(s) for computing): sudo snap ack helm_*.assert sudo snap install helm_*.snap --classic sudo snap ack kubectl_*.assert sudo snap install kubectl_*.snap --classic # Copy kubeconfig from cluster and make it accessible for kubectl/helm export KUBECONFIG = $HOME /.kube/config # If installing on computing hosts # sudo mkdir /var/snap/microk8s/current/bin # sudo ln -s /snap/bin/helm /var/snap/microk8s/current/bin/helm # sudo ln -s /snap/bin/kubectl /var/snap/microk8s/current/bin/kubectl # (Optional) Install additional Kubernetes monitoring tools like k9s or Lens Install ingress: # These steps assume you know the digest of the re-tagged images required for the ingress-nginx helm chart kubectl create ns ingress helm install ingress-nginx ./ingress-nginx-*.tgz --set controller.hostPort.enabled = true -n ingress Adding more nodes (optional) Note: This can be done before or after the system is live. Install required addon: sudo microk8s enable ha-cluster From the master node, run: sudo microk8s add-node This will generate a command with a token to be executed on a standby node. On your standby node, ensure the microk8s ha-cluster addon is enabled before running the command from the master to join the cluster. To verify the nodes are connected, run (on any node): sudo microk8s kubectl get nodes Repeat this process for any additional standby nodes that are to be added. For more details, see: Clustering with MicroK8s Get the Assemblyline chart to your administration computer \u00b6 Get the Assemblyline helm charts \u00b6 mkdir ~/git && cd ~/git git clone https://github.com/CybercentreCanada/assemblyline-helm-chart.git Create your personal deployment \u00b6 mkdir ~/git/deployment cp ~/git/assemblyline-helm-chart/appliance/*.yaml ~/git/deployment Setup the charts and secrets \u00b6 The values.yaml file in your deployment directory ~/git/deployment is already pre-configured for use with microk8s as a basic one node minimal appliance. Make sure you go through the file to adjust disk sizes and to turn on/off features to your liking. The secret.yaml file in your deployment directory is preconfigured with default passwords, you should change them. Tip The secrets are used to set up during bootstrap so make sure you change them before deploying the al chart. Warning Be sure to update any values as deemed necessary ie. FQDN Deploy Assemblyline via Helm: \u00b6 Create a namespace for your Assemblyline install \u00b6 For this documentation, we will use al as the namespace. sudo microk8s kubectl create namespace al Deploy the secret to the namespace \u00b6 sudo microk8s kubectl apply -f ~/git/deployment/secrets.yaml --namespace = al From this point on, you don't need the secrets.yaml file anymore. You should delete it so there is no file on disk containing your passwords. rm ~/git/deployment/secrets.yaml Finally, let's deploy Assemblyline's chart: \u00b6 For documentation, we will use assemblyline as the deployment name. sudo microk8s helm install assemblyline ~/git/assemblyline-helm-chart/assemblyline -f ~/git/deployment/values.yaml -n al Warning After you've ran the helm install command, the system has a lot of setting up to do (Creating database indexes, loading service, setting up default accounts, loading signatures ...). Don't expect it to be fully operational for at least the next 15 minutes. Updating the current deployment \u00b6 Once you have your Assemblyline chart deployed through helm, you can change any values in the values.yaml file and upgrade your deployment with the following command: sudo microk8s helm upgrade assemblyline ~/git/assemblyline-helm-chart/assemblyline -f ~/git/deployment/values.yaml -n al (Optional) Get the latest assemblyline-helm-chart Before doing your helm upgrade command, you can get the latest changes that we did to the chart by pulling them. (This could conflict with the changes you've made so be careful while doing this.) cd ~/git/assemblyline-helm-chart && git pull Quality of life improvements \u00b6 Lens IDE \u00b6 If the computer on which your microk8s deployment is installed has a desktop interface, I strongly suggest that you use K8s Lens IDE to manage the system Install Lens \u00b6 sudo snap install kontena-lens --classic Configure Lens \u00b6 After you run Lens for the first time, click the \"Add cluster\" menu/button, select the paste as text tab and paste the output of the following command: sudo microk8s kubectl config view --raw Sudoless access to MicroK8s \u00b6 MicroK8s require you to add sudo in front of every command, you can add your user to the microk8s group so you don't have to. sudo usermod -a -G microk8s $USER sudo chown -f -R $USER ~/.kube You will need to reboot for these changes to take effect Temporarily, you can add the group to your current shell by running the following: newgrp microk8s Alias to Kubectl \u00b6 Since all is running inside microk8s you can create an alias to the kubectl command to make your life easier sudo snap alias microk8s.kubectl kubectl kubectl config set-context --current --namespace=al Alternative Installations \u00b6 We will officially only support microk8s installations for appliances, but you can technically install it on any local Kubernetes frameworks (k3s, minikube, kind...). That said there will be no documentation for these setups, and you will have to modify the values.yaml storage classes to fit with your desired framework.","title":"Appliance installation (MicroK8s)"},{"location":"installation/appliance/#appliance-installation-microk8s","text":"This is the documentation for an appliance instance of the Assemblyline platform suited for smaller-scale deployments. Since we've used microk8s as the backend for this, the appliance setup can later be scaled to multiple nodes.","title":"Appliance installation (MicroK8s)"},{"location":"installation/appliance/#setup-requirements","text":"Caveat The documentation provided here assumes that you are installing your appliance on an Ubuntu-based system and was only tested on Ubuntu 20.04. You might have to change the commands a bit if you use other Linux distributions. The recommended minimum system requirement for this appliance is 6 CPU and 12 GB of Ram.","title":"Setup requirements"},{"location":"installation/appliance/#install-pre-requisites","text":"Online Install microk8s: sudo snap install microk8s --classic Install microk8s addons: sudo microk8s enable dns ha-cluster storage metrics-server Install Helm and set it up to use with microk8s: sudo snap install helm --classic sudo mkdir /var/snap/microk8s/current/bin sudo ln -s /snap/bin/helm /var/snap/microk8s/current/bin/helm Install git: sudo apt install git Install ingress controller: sudo microk8s kubectl create ns ingress sudo microk8s helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx sudo microk8s helm repo update sudo microk8s helm install ingress-nginx ingress-nginx/ingress-nginx --set controller.hostPort.enabled = true -n ingress Offline Download offline packages (On an internet-connected system): Create a directory to house everything to be transferred mkdir al_deps cd al_deps Download offline snap packages for snap_pkg in \"microk8s\" \"helm\" \"kubectl\" do sudo snap download $snap_pkg done Fetch helm charts # Clone the Assemblyline helm chart repo git clone https://github.com/CybercentreCanada/assemblyline-helm-chart # Download dependency helm charts helm dependency update assemblyline-helm-chart/assemblyline/ wget https://github.com/kubernetes/ingress-nginx/releases/download/helm-chart-4.0.12/ingress-nginx-4.0.12.tgz Fetch container images # Calico CNI for container_image in \"cni\" \"pod2daemon-flexvol\" \"node\" do docker pull calico/ $container_image :v3.19.1 && docker save calico/ $container_image :v3.19.1 >> calico_ $container_image .tar done docker pull calico/kube-controllers:v3.17.3 && docker save calico/kube-controllers:v3.17.3 >> calico_kube-controllers.tar # NGINX-Ingress, refer to values.yaml in `charts` directory for container_image in \"controller:v1.1.0\" \"kube-webhook-certgen:v1.1.1\" do docker pull k8s.gcr.io/ingress-nginx/ $container_image && docker save k8s.gcr.io/ingress-nginx/ $container_image >> $container_image .tar done # microk8s add-ons, refer to images used in corresponding .yaml files (https://github.com/ubuntu/microk8s/tree/master/microk8s-resources/actions) export ARCH = amd64 # DNS, Storage, coreDNS, metrics-server container images for container_image in \"k8s-dns-kube-dns\" \"k8s-dns-dnsmasq-nanny\" \"k8s-dns-sidecar\" do docker pull gcr.io/google_containers/ $container_image - $ARCH :1.14.7 && docker save gcr.io/google_containers/ $container_image - $ARCH :1.14.7 >> $container_image .tar done docker pull k8s.gcr.io/pause:3.1 && docker save k8s.gcr.io/pause:3.1 >> pause.tar docker pull coredns/coredns:1.8.0 && docker save coredns/coredns:1.8.0 >> coredns.tar docker pull cdkbot/hostpath-provisioner- $ARCH :1.0.0 && docker save cdkbot/hostpath-provisioner- $ARCH :1.0.0 >> storage.tar docker pull k8s.gcr.io/metrics-server/metrics-server:v0.5.2 && docker save k8s.gcr.io/metrics-server/metrics-server:v0.5.2 >> metrics.tar # Assemblyline Core (release: 4.2.stable) for al_image in \"core\" \"ui\" \"ui-frontend\" \"service-server\" \"socketio\" do docker pull cccs/assemblyline- $al_image :4.2.stable && docker save cccs/assemblyline- $al_image :4.2.stable >> al_ $al_image .tar done # Elastic ES_REG = docker.elastic.co ES_VER = 7 .15.0 for beat in \"filebeat\" \"metricbeat\" do docker pull $ES_REG /beats/ $beat : $ES_VER && docker save $ES_REG /beats/ $beat : $ES_VER >> es_ $beat .tar done for es in \"logstash\" \"kibana\" \"elasticsearch\" do docker pull $ES_REG / $es / $es : $ES_VER && docker save $ES_REG / $es / $es : $ES_VER >> es_ $es .tar done # Filestore image (MinIO) for minio_image in \"minio:RELEASE.2021-02-14T04-01-33Z\" \"mc:RELEASE.2021-02-14T04-28-06Z\" do docker pull minio/ $minio_image && docker save minio/ $minio_image >> minio_ $minio_image .tar done # Redis image docker pull redis && docker save redis >> redis.tar (Optional) Container Registry Image docker pull registry && docker save registry >> registry.tar (Optional) Pull official service images for svc_image in apkaye beaver characterize configextractor cuckoo deobfuscripter emlparser espresso extract floss frankenstrings iparse metadefender metapeek oletools pdfid peepdf pefile pixaxe safelist sigma suricata swiffer tagcheck torrentslicer unpacme unpacker vipermonkey virustotal-dynamic virustotal-static xlmmacrodeobfuscator yara do docker pull cccs/assemblyline-service- $svc_image :stable && docker save cccs/assemblyline-service- $svc_image :stable >> $svc_image .tar done Load images into a container registry (Optional) Setup local container registry # Assumes Docker is installed on hosting system for container_image in registry.tar ) : do sudo docker load -i $container_image done # Start up registry container sudo docker run -dp 32000 :5000 --restart = always --name registry registry Load images from disk and push to registry REGISTRY = localhost:32000 # Re-tag images and push to local registry for image in $( docker image ls --format {{ .Repository }} : {{ .Tag }} ) do image_tag = $image if [ $( grep -o '/' <<< $image | wc -l ) -eq 2 ] then image_tag = $( cut -d '/' -f 2 - <<< $image ) fi sudo docker tag $image $REGISTRY / $image_tag && docker push $REGISTRY / $image_tag sudo docker image rm $image sudo docker image rm $REGISTRY / $image_tag done Setup computing host(s): Install microk8s sudo snap ack microk8s_*.assert sudo snap install microk8s_*.snap --classic Modify Container Registry Endpoints sudo vim /var/snap/microk8s/current/args/containerd-template.toml Replace REGISTRY with the domain/port of your container registry (ie. localhost:32000) [plugins.\"io.containerd.grpc.v1.cri\".registry.mirrors] [plugins.\"io.containerd.grpc.v1.cri\".registry.mirrors.\"docker.io\"] endpoint = [ \"https://registry-1.docker.io\" , \"http://<REGISTRY>\" , ] [plugins.\"io.containerd.grpc.v1.cri\".registry.mirrors.\"*\"] endpoint = [ \"http://<REGISTRY>\" ] Restart containerd to acknowledge the changes sudo systemctl restart containerd Enable microk8s add-ons sudo microk8s reset && sudo microk8s enable dns ha-cluster storage metrics-server Fetch kubeconfig for administration cp /var/snap/microk8s/current/credentials/client.config <remote_destination> Install admin tools (separate or same host(s) for computing): sudo snap ack helm_*.assert sudo snap install helm_*.snap --classic sudo snap ack kubectl_*.assert sudo snap install kubectl_*.snap --classic # Copy kubeconfig from cluster and make it accessible for kubectl/helm export KUBECONFIG = $HOME /.kube/config # If installing on computing hosts # sudo mkdir /var/snap/microk8s/current/bin # sudo ln -s /snap/bin/helm /var/snap/microk8s/current/bin/helm # sudo ln -s /snap/bin/kubectl /var/snap/microk8s/current/bin/kubectl # (Optional) Install additional Kubernetes monitoring tools like k9s or Lens Install ingress: # These steps assume you know the digest of the re-tagged images required for the ingress-nginx helm chart kubectl create ns ingress helm install ingress-nginx ./ingress-nginx-*.tgz --set controller.hostPort.enabled = true -n ingress Adding more nodes (optional) Note: This can be done before or after the system is live. Install required addon: sudo microk8s enable ha-cluster From the master node, run: sudo microk8s add-node This will generate a command with a token to be executed on a standby node. On your standby node, ensure the microk8s ha-cluster addon is enabled before running the command from the master to join the cluster. To verify the nodes are connected, run (on any node): sudo microk8s kubectl get nodes Repeat this process for any additional standby nodes that are to be added. For more details, see: Clustering with MicroK8s","title":"Install pre-requisites:"},{"location":"installation/appliance/#get-the-assemblyline-chart-to-your-administration-computer","text":"","title":"Get the Assemblyline chart to your administration computer"},{"location":"installation/appliance/#get-the-assemblyline-helm-charts","text":"mkdir ~/git && cd ~/git git clone https://github.com/CybercentreCanada/assemblyline-helm-chart.git","title":"Get the Assemblyline helm charts"},{"location":"installation/appliance/#create-your-personal-deployment","text":"mkdir ~/git/deployment cp ~/git/assemblyline-helm-chart/appliance/*.yaml ~/git/deployment","title":"Create your personal deployment"},{"location":"installation/appliance/#setup-the-charts-and-secrets","text":"The values.yaml file in your deployment directory ~/git/deployment is already pre-configured for use with microk8s as a basic one node minimal appliance. Make sure you go through the file to adjust disk sizes and to turn on/off features to your liking. The secret.yaml file in your deployment directory is preconfigured with default passwords, you should change them. Tip The secrets are used to set up during bootstrap so make sure you change them before deploying the al chart. Warning Be sure to update any values as deemed necessary ie. FQDN","title":"Setup the charts and secrets"},{"location":"installation/appliance/#deploy-assemblyline-via-helm","text":"","title":"Deploy Assemblyline via Helm:"},{"location":"installation/appliance/#create-a-namespace-for-your-assemblyline-install","text":"For this documentation, we will use al as the namespace. sudo microk8s kubectl create namespace al","title":"Create a namespace for your Assemblyline install"},{"location":"installation/appliance/#deploy-the-secret-to-the-namespace","text":"sudo microk8s kubectl apply -f ~/git/deployment/secrets.yaml --namespace = al From this point on, you don't need the secrets.yaml file anymore. You should delete it so there is no file on disk containing your passwords. rm ~/git/deployment/secrets.yaml","title":"Deploy the secret to the namespace"},{"location":"installation/appliance/#finally-lets-deploy-assemblylines-chart","text":"For documentation, we will use assemblyline as the deployment name. sudo microk8s helm install assemblyline ~/git/assemblyline-helm-chart/assemblyline -f ~/git/deployment/values.yaml -n al Warning After you've ran the helm install command, the system has a lot of setting up to do (Creating database indexes, loading service, setting up default accounts, loading signatures ...). Don't expect it to be fully operational for at least the next 15 minutes.","title":"Finally, let's deploy Assemblyline's chart:"},{"location":"installation/appliance/#updating-the-current-deployment","text":"Once you have your Assemblyline chart deployed through helm, you can change any values in the values.yaml file and upgrade your deployment with the following command: sudo microk8s helm upgrade assemblyline ~/git/assemblyline-helm-chart/assemblyline -f ~/git/deployment/values.yaml -n al (Optional) Get the latest assemblyline-helm-chart Before doing your helm upgrade command, you can get the latest changes that we did to the chart by pulling them. (This could conflict with the changes you've made so be careful while doing this.) cd ~/git/assemblyline-helm-chart && git pull","title":"Updating the current deployment"},{"location":"installation/appliance/#quality-of-life-improvements","text":"","title":"Quality of life improvements"},{"location":"installation/appliance/#lens-ide","text":"If the computer on which your microk8s deployment is installed has a desktop interface, I strongly suggest that you use K8s Lens IDE to manage the system","title":"Lens IDE"},{"location":"installation/appliance/#install-lens","text":"sudo snap install kontena-lens --classic","title":"Install Lens"},{"location":"installation/appliance/#configure-lens","text":"After you run Lens for the first time, click the \"Add cluster\" menu/button, select the paste as text tab and paste the output of the following command: sudo microk8s kubectl config view --raw","title":"Configure Lens"},{"location":"installation/appliance/#sudoless-access-to-microk8s","text":"MicroK8s require you to add sudo in front of every command, you can add your user to the microk8s group so you don't have to. sudo usermod -a -G microk8s $USER sudo chown -f -R $USER ~/.kube You will need to reboot for these changes to take effect Temporarily, you can add the group to your current shell by running the following: newgrp microk8s","title":"Sudoless access to MicroK8s"},{"location":"installation/appliance/#alias-to-kubectl","text":"Since all is running inside microk8s you can create an alias to the kubectl command to make your life easier sudo snap alias microk8s.kubectl kubectl kubectl config set-context --current --namespace=al","title":"Alias to Kubectl"},{"location":"installation/appliance/#alternative-installations","text":"We will officially only support microk8s installations for appliances, but you can technically install it on any local Kubernetes frameworks (k3s, minikube, kind...). That said there will be no documentation for these setups, and you will have to modify the values.yaml storage classes to fit with your desired framework.","title":"Alternative Installations"},{"location":"installation/appliance_docker/","text":"Appliance installation (Docker) \u00b6 This is the documentation for an appliance instance of the Assemblyline platform suited for very small single machine deployment. Setup requirements \u00b6 Caveat The documentation provided here assumes that you are installing your appliance on one of the following systems: Debian: Ubuntu 20.04 RHEL: RHEL 8.5 You might have to change the commands a bit if you use other Linux distributions. The recommended minimum system requirement for this appliance is 4 CPUs and 8 GB of RAM. Warning If you have more then 16 CPUs and 64 GB of ram, you should consider using the Microk8s appliance instead. Microk8s will be able to auto-scale core components based on load but this docker appliance can only scale services. Install pre-requisites \u00b6 Online Ubuntu Install Docker: sudo apt-get update sudo apt-get install -y apt-transport-https ca-certificates curl gnupg-agent software-properties-common curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - sudo apt-key fingerprint 0EBFCD88 sudo add-apt-repository \"deb [arch=amd64] https://download.docker.com/linux/ubuntu $( lsb_release -cs ) stable\" sudo apt-get install -y docker-ce docker-ce-cli containerd.io Install Docker compose: sudo curl -s -L \"https://github.com/docker/compose/releases/download/1.29.2/docker-compose- $( uname -s ) - $( uname -m ) \" -o /usr/local/bin/docker-compose sudo chmod +x /usr/local/bin/docker-compose sudo curl -s -L https://raw.githubusercontent.com/docker/compose/1.29.2/contrib/completion/bash/docker-compose -o /etc/bash_completion.d/docker-compose RHEL 8.5 Warning Many of the instructions below have been set to force yes and allowerasing for quick implementation. It is recommended that these flags be removed for production environments to avoid impacting production environments by missing key messages and warnings. Step 4 contains a firewall configuration, we strongly advise firewall settings should be managed and reviewed by your organization before deployment. Install Docker: yum update -y --allowerasing yum install -y yum-utils yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo yum install -y docker-ce docker-ce-cli containerd.io --allowerasing systemctl start docker systemctl enable docker Install Docker compose: curl -s -L \"https://github.com/docker/compose/releases/download/1.29.2/docker-compose- $( uname -s ) - $( uname -m ) \" -o /usr/bin/docker-compose chmod +x /usr/bin/docker-compose curl -s -L https://raw.githubusercontent.com/docker/compose/1.29.2/contrib/completion/bash/docker-compose -o /etc/bash_completion.d/docker-compose Upgrade Python3.9: dnf install -y python39 alternatives --set python3 /usr/bin/python3.9 python3 --version Configure firewalld for Docker: sed -i 's/FirewallBackend=nftables/FirewallBackend=iptables/' /etc/firewalld/firewalld.conf firewall-cmd --reload reboot Setup your Assemblyline appliance \u00b6 Download the Assemblyline docker-compose files \u00b6 Online mkdir ~/git cd ~/git git clone https://github.com/CybercentreCanada/assemblyline-docker-compose.git Choose your deployment type \u00b6 Important After this step, we will assume that the commands that you run are from your deployment directory: ~/deployments/assemblyline/ Assemblyline only mkdir ~/deployments cp -R ~/git/assemblyline-docker-compose/minimal_appliance ~/deployments/assemblyline cd ~/deployments/assemblyline Assemblyline with ELK monitoring stack mkdir ~/deployments cp -R ~/git/assemblyline-docker-compose/full_appliance ~/deployments/assemblyline cd ~/deployments/assemblyline Setup your appliance \u00b6 The config/config.yaml file in your deployment directory is already pre-configured for use with docker-compose as a single node appliance. You can review the settings already configured but you should not have anything to change there. The .env file in your deployment directory is preconfigured with default passwords, you should definitely change them. Deploy Assemblyline \u00b6 Create your https certs \u00b6 openssl req -nodes -x509 -newkey rsa:4096 -keyout ~/deployments/assemblyline/config/nginx.key -out ~/deployments/assemblyline/config/nginx.crt -days 365 -subj \"/C=CA/ST=Ontario/L=Ottawa/O=CCCS/CN=assemblyline.local\" Pull necessary docker containers \u00b6 cd ~/deployments/assemblyline sudo docker-compose pull sudo docker-compose build sudo docker-compose -f bootstrap-compose.yaml pull Finally deploy your appliance \u00b6 cd ~/deployments/assemblyline sudo docker-compose up -d sudo docker-compose -f bootstrap-compose.yaml up Info Once the docker-compose command on the bootstrap file complete, your cluster will be ready to use and you can login with the default admin user/password that you've set in your .env file Docker Compose cheat sheet \u00b6 Updating your appliance \u00b6 cd ~/deployments/assemblyline sudo docker-compose pull sudo docker-compose build sudo docker-compose up -d Changing Assemblyline configuration file \u00b6 Edit the cd ~/deployments/assemblyline/config/config.yml then: cd ~/deployments/assemblyline sudo docker-compose restart Check core services logs \u00b6 For core components: cd ~/deployments/assemblyline sudo docker-compose logs Or for a specific component: cd ~/deployments/assemblyline sudo docker-compose logs ui Take down your appliance \u00b6 Tip This will remove all containers related to your appliance but will not remove the volumes so you can bring it back up safely. cd ~/deployments/assemblyline sudo docker-compose stop sudo docker rm --force $( sudo docker ps -a --filter label = app = assemblyline -q ) sudo docker-compose down --remove-orphans Bring your appliance back online \u00b6 cd ~/deployments/assemblyline sudo docker-compose up -d","title":"Appliance installation (Docker)"},{"location":"installation/appliance_docker/#appliance-installation-docker","text":"This is the documentation for an appliance instance of the Assemblyline platform suited for very small single machine deployment.","title":"Appliance installation (Docker)"},{"location":"installation/appliance_docker/#setup-requirements","text":"Caveat The documentation provided here assumes that you are installing your appliance on one of the following systems: Debian: Ubuntu 20.04 RHEL: RHEL 8.5 You might have to change the commands a bit if you use other Linux distributions. The recommended minimum system requirement for this appliance is 4 CPUs and 8 GB of RAM. Warning If you have more then 16 CPUs and 64 GB of ram, you should consider using the Microk8s appliance instead. Microk8s will be able to auto-scale core components based on load but this docker appliance can only scale services.","title":"Setup requirements"},{"location":"installation/appliance_docker/#install-pre-requisites","text":"Online Ubuntu Install Docker: sudo apt-get update sudo apt-get install -y apt-transport-https ca-certificates curl gnupg-agent software-properties-common curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - sudo apt-key fingerprint 0EBFCD88 sudo add-apt-repository \"deb [arch=amd64] https://download.docker.com/linux/ubuntu $( lsb_release -cs ) stable\" sudo apt-get install -y docker-ce docker-ce-cli containerd.io Install Docker compose: sudo curl -s -L \"https://github.com/docker/compose/releases/download/1.29.2/docker-compose- $( uname -s ) - $( uname -m ) \" -o /usr/local/bin/docker-compose sudo chmod +x /usr/local/bin/docker-compose sudo curl -s -L https://raw.githubusercontent.com/docker/compose/1.29.2/contrib/completion/bash/docker-compose -o /etc/bash_completion.d/docker-compose RHEL 8.5 Warning Many of the instructions below have been set to force yes and allowerasing for quick implementation. It is recommended that these flags be removed for production environments to avoid impacting production environments by missing key messages and warnings. Step 4 contains a firewall configuration, we strongly advise firewall settings should be managed and reviewed by your organization before deployment. Install Docker: yum update -y --allowerasing yum install -y yum-utils yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo yum install -y docker-ce docker-ce-cli containerd.io --allowerasing systemctl start docker systemctl enable docker Install Docker compose: curl -s -L \"https://github.com/docker/compose/releases/download/1.29.2/docker-compose- $( uname -s ) - $( uname -m ) \" -o /usr/bin/docker-compose chmod +x /usr/bin/docker-compose curl -s -L https://raw.githubusercontent.com/docker/compose/1.29.2/contrib/completion/bash/docker-compose -o /etc/bash_completion.d/docker-compose Upgrade Python3.9: dnf install -y python39 alternatives --set python3 /usr/bin/python3.9 python3 --version Configure firewalld for Docker: sed -i 's/FirewallBackend=nftables/FirewallBackend=iptables/' /etc/firewalld/firewalld.conf firewall-cmd --reload reboot","title":"Install pre-requisites"},{"location":"installation/appliance_docker/#setup-your-assemblyline-appliance","text":"","title":"Setup your Assemblyline appliance"},{"location":"installation/appliance_docker/#download-the-assemblyline-docker-compose-files","text":"Online mkdir ~/git cd ~/git git clone https://github.com/CybercentreCanada/assemblyline-docker-compose.git","title":"Download the Assemblyline docker-compose files"},{"location":"installation/appliance_docker/#choose-your-deployment-type","text":"Important After this step, we will assume that the commands that you run are from your deployment directory: ~/deployments/assemblyline/ Assemblyline only mkdir ~/deployments cp -R ~/git/assemblyline-docker-compose/minimal_appliance ~/deployments/assemblyline cd ~/deployments/assemblyline Assemblyline with ELK monitoring stack mkdir ~/deployments cp -R ~/git/assemblyline-docker-compose/full_appliance ~/deployments/assemblyline cd ~/deployments/assemblyline","title":"Choose your deployment type"},{"location":"installation/appliance_docker/#setup-your-appliance","text":"The config/config.yaml file in your deployment directory is already pre-configured for use with docker-compose as a single node appliance. You can review the settings already configured but you should not have anything to change there. The .env file in your deployment directory is preconfigured with default passwords, you should definitely change them.","title":"Setup your appliance"},{"location":"installation/appliance_docker/#deploy-assemblyline","text":"","title":"Deploy Assemblyline"},{"location":"installation/appliance_docker/#create-your-https-certs","text":"openssl req -nodes -x509 -newkey rsa:4096 -keyout ~/deployments/assemblyline/config/nginx.key -out ~/deployments/assemblyline/config/nginx.crt -days 365 -subj \"/C=CA/ST=Ontario/L=Ottawa/O=CCCS/CN=assemblyline.local\"","title":"Create your https certs"},{"location":"installation/appliance_docker/#pull-necessary-docker-containers","text":"cd ~/deployments/assemblyline sudo docker-compose pull sudo docker-compose build sudo docker-compose -f bootstrap-compose.yaml pull","title":"Pull necessary docker containers"},{"location":"installation/appliance_docker/#finally-deploy-your-appliance","text":"cd ~/deployments/assemblyline sudo docker-compose up -d sudo docker-compose -f bootstrap-compose.yaml up Info Once the docker-compose command on the bootstrap file complete, your cluster will be ready to use and you can login with the default admin user/password that you've set in your .env file","title":"Finally deploy your appliance"},{"location":"installation/appliance_docker/#docker-compose-cheat-sheet","text":"","title":"Docker Compose cheat sheet"},{"location":"installation/appliance_docker/#updating-your-appliance","text":"cd ~/deployments/assemblyline sudo docker-compose pull sudo docker-compose build sudo docker-compose up -d","title":"Updating your appliance"},{"location":"installation/appliance_docker/#changing-assemblyline-configuration-file","text":"Edit the cd ~/deployments/assemblyline/config/config.yml then: cd ~/deployments/assemblyline sudo docker-compose restart","title":"Changing Assemblyline configuration file"},{"location":"installation/appliance_docker/#check-core-services-logs","text":"For core components: cd ~/deployments/assemblyline sudo docker-compose logs Or for a specific component: cd ~/deployments/assemblyline sudo docker-compose logs ui","title":"Check core services logs"},{"location":"installation/appliance_docker/#take-down-your-appliance","text":"Tip This will remove all containers related to your appliance but will not remove the volumes so you can bring it back up safely. cd ~/deployments/assemblyline sudo docker-compose stop sudo docker rm --force $( sudo docker ps -a --filter label = app = assemblyline -q ) sudo docker-compose down --remove-orphans","title":"Take down your appliance"},{"location":"installation/appliance_docker/#bring-your-appliance-back-online","text":"cd ~/deployments/assemblyline sudo docker-compose up -d","title":"Bring your appliance back online"},{"location":"installation/classification_engine/","text":"Classification engine \u00b6 Assemblyline can do record-based access control for submission, files, results, and even up to individual sections and tags of said results. It was built to support the Government of Canada levels of security and the Traffic Light Protocol but can also be modified at will. Once turned on, the classification engine will do the following changes to the system: Users will have to be assigned a maximum classification level that they can see as well as the groups they are members of Each submission to the system will have to have a classification level The User Interface will: Show the effective classification of each submission, file, result, and result sections Have a dedicated help section that will explain how classification conflicts are resolved Let you pick a classification while submitting a file Automatically hide portions of the result for a user that does not have enough privileges to see them Configuration \u00b6 The classification engine has many parameters that can be customized so you can get record-based access controls that fit your organization. Here is an exhaustive configuration file of the classification engine that explains every single parameter: Exhaustive classification configuration # Turn on/off classification enforcement. When this flag is off, this # completely disables the classification engine, any documents added while # the classification engine is off getting the default unrestricted value enforce : false # When this flag is on, the classification engine will automatically create # groups based on the domain part of a user's email address # EX: # For user with email: test@local.host # The group \"local.host\" will be valid in the system dynamic_groups : false # List of Classification Levels: # This is a graded list; a smaller number is less restricted than a higher number # A user must be allowed a classification level >= to be able to view the data levels : # List of alternate names for the current marking. If a user submits a file with # those markings, the classification will automatically rename it to the value # specified in the name - aliases : - UNRESTRICTED - UNCLASSIFIED - U # Stylesheet applied in the UI for the current classification level css : # Name of the color scheme used for display # Possible values: default, primary, secondary, success, info, warning, error color : default # Deprecated parameters (Use color instead) # These were used in the old UI but are still valid in the new UI because if # the new UI cannot find the color param, it will use the label param and # strips \"label-\" part. banner : alert-default label : label-default text : text-muted # Description of the classification level description : Subject to standard copyright rules, TLP:WHITE information may be distributed without restriction. # Integer value of the Classification level (higher is more classified) lvl : 100 # Long name of the classification level name : TLP:WHITE # Short name of the classification level short_name : TLP:W - aliases : [] css : color : success description : Recipients may share TLP:GREEN information with peers and partner organizations within their sector or community, but not via publicly accessible channels. Information in this category can be circulated widely within a particular community. TLP:GREEN information may not be released outside of the community. lvl : 110 name : TLP:GREEN short_name : TLP:G - aliases : - RESTRICTED css : color : warning description : Recipients may only share TLP:AMBER information with members of their own organization and with clients or customers who need to know the information to protect themselves or prevent further harm. lvl : 120 name : TLP:AMBER short_name : TLP:A # List of required tokens: # A user requesting access to an item must have all the # required tokens the item has, to gain access to it required : # List of alternate names for the token - aliases : [] # Description of the token description : Produced using a commercial tool with limited distribution # Long name for the token name : COMMERCIAL # Short name for the token short_name : CMR # (optional) The minimum classification level an item must have # for this token to be valid. So, because this token has a value # of 120, once it's selected, the classification level automatically # jumps to TPL:A which was set to 120 in the previous section. require_lvl : 120 # List of groups: # A user requesting access to an item must be part of a least # of one the group the item is part of to gain access groups : # List of aliases for the group - aliases : - ANY # (optional) This is a special flag that when set to true if any other groups # are selected in a classification, this group will automatically be selected # as well. auto_select : true # Description of the group description : Employees of CSE # Long name for the group name : CSE # Short name for the group short_name : CSE # (optional) Assuming that this group is the only group selected, this is the # display name that will be used in the classification # NOTE: values must be in the aliases of this group and only this group solitary_display_name : ANY # List of sub-groups: # A user requesting access to an item must be part of a least # of one the sub-group the item is part of to gain access subgroups : # List of aliases for the subgroups - aliases : [] # Description of the sub-group description : Member of Incident Response team # Long name of the sub-group name : IR TEAM # Short name of the sub-group short_name : IR - aliases : [] description : Member of the Canadian Centre for Cyber Security # (optional) This is a special flag that auto select the corresponding # group when this sub-group is selected require_group : CSE name : CCCS short_name : CCCS # (optional) This is a special flag that makes sure that none other then # the corresponding group is selected when this sub-group is selected limited_to_group : CSE # Default restricted classification restricted : TLP:A//CMR # Default unrestricted classification: # When no classification is provided or the classification engine is # disabled, this is the classification value each item will get unrestricted : TLP:W Enabling it in your system \u00b6 By default, the classification engine is disabled in the system, but it can easily be enabled by creating a new config map in Kubernetes. Info For this documentation we will enable a classification engine that supports only the traffic light protocol. In your deployment directory, create a file named objects.yaml with the following content: objects.yaml apiVersion : v1 kind : ConfigMap metadata : name : assemblyline-extra-config data : classification : | enforce: true levels: - aliases: - UNRESTRICTED - U css: color: default description: Subject to standard copyright rules, TLP:WHITE information may be distributed without restriction. lvl: 100 name: TLP:WHITE short_name: TLP:W - aliases: [] css: color: success description: Recipients may share TLP:GREEN information with peers and partner organizations within their sector or community, but not via publicly accessible channels. Information in this category can be circulated widely within a particular community. TLP:GREEN information may not be released outside of the community. lvl: 110 name: TLP:GREEN short_name: TLP:G - aliases: - RESTRICTED - R css: color: warning description: Recipients may only share TLP:AMBER information with members of their own organization and with clients or customers who need to know the information to protect themselves or prevent further harm. lvl: 120 name: TLP:AMBER short_name: TLP:A required: [] groups: [] subgroups: [] restricted: TLP:A unrestricted: TLP:W Use kubectl to apply the objects.yaml file to your system: Appliance sudo microk8s kubectl apply -f ~/git/deployment/objects.yaml --namespace = al Cluster kubectl apply -f <deployment_directory>/objects.yaml --namespace = al Then you must tell your pods to use the classification engine from the newly created config map. Add the following block to the values.yaml or your deployment: Partial values.yaml to load the custom classification.yml file ... coreEnv : - name : CLASSIFICATION_CONFIGMAP value : assemblyline-extra-config - name : CLASSIFICATION_CONFIGMAP_KEY value : classification coreMounts : - name : al-extra-config mountPath : /etc/assemblyline/classification.yml subPath : classification coreVolumes : - name : al-extra-config configMap : name : assemblyline-extra-config ... Finally update your deployment using helm upgrade command : Appliance sudo microk8s helm upgrade assemblyline ~/git/assemblyline-helm-chart/assemblyline -f ~/git/deployment/values.yaml -n al Cluster helm upgrade assemblyline <assemblyline-helm-chart>/assemblyline -f <deployment_directory>/values.yaml -n al Important It takes a while for all the containers to be restarted so be patient and it will eventually show up in the UI.","title":"Classification engine"},{"location":"installation/classification_engine/#classification-engine","text":"Assemblyline can do record-based access control for submission, files, results, and even up to individual sections and tags of said results. It was built to support the Government of Canada levels of security and the Traffic Light Protocol but can also be modified at will. Once turned on, the classification engine will do the following changes to the system: Users will have to be assigned a maximum classification level that they can see as well as the groups they are members of Each submission to the system will have to have a classification level The User Interface will: Show the effective classification of each submission, file, result, and result sections Have a dedicated help section that will explain how classification conflicts are resolved Let you pick a classification while submitting a file Automatically hide portions of the result for a user that does not have enough privileges to see them","title":"Classification engine"},{"location":"installation/classification_engine/#configuration","text":"The classification engine has many parameters that can be customized so you can get record-based access controls that fit your organization. Here is an exhaustive configuration file of the classification engine that explains every single parameter: Exhaustive classification configuration # Turn on/off classification enforcement. When this flag is off, this # completely disables the classification engine, any documents added while # the classification engine is off getting the default unrestricted value enforce : false # When this flag is on, the classification engine will automatically create # groups based on the domain part of a user's email address # EX: # For user with email: test@local.host # The group \"local.host\" will be valid in the system dynamic_groups : false # List of Classification Levels: # This is a graded list; a smaller number is less restricted than a higher number # A user must be allowed a classification level >= to be able to view the data levels : # List of alternate names for the current marking. If a user submits a file with # those markings, the classification will automatically rename it to the value # specified in the name - aliases : - UNRESTRICTED - UNCLASSIFIED - U # Stylesheet applied in the UI for the current classification level css : # Name of the color scheme used for display # Possible values: default, primary, secondary, success, info, warning, error color : default # Deprecated parameters (Use color instead) # These were used in the old UI but are still valid in the new UI because if # the new UI cannot find the color param, it will use the label param and # strips \"label-\" part. banner : alert-default label : label-default text : text-muted # Description of the classification level description : Subject to standard copyright rules, TLP:WHITE information may be distributed without restriction. # Integer value of the Classification level (higher is more classified) lvl : 100 # Long name of the classification level name : TLP:WHITE # Short name of the classification level short_name : TLP:W - aliases : [] css : color : success description : Recipients may share TLP:GREEN information with peers and partner organizations within their sector or community, but not via publicly accessible channels. Information in this category can be circulated widely within a particular community. TLP:GREEN information may not be released outside of the community. lvl : 110 name : TLP:GREEN short_name : TLP:G - aliases : - RESTRICTED css : color : warning description : Recipients may only share TLP:AMBER information with members of their own organization and with clients or customers who need to know the information to protect themselves or prevent further harm. lvl : 120 name : TLP:AMBER short_name : TLP:A # List of required tokens: # A user requesting access to an item must have all the # required tokens the item has, to gain access to it required : # List of alternate names for the token - aliases : [] # Description of the token description : Produced using a commercial tool with limited distribution # Long name for the token name : COMMERCIAL # Short name for the token short_name : CMR # (optional) The minimum classification level an item must have # for this token to be valid. So, because this token has a value # of 120, once it's selected, the classification level automatically # jumps to TPL:A which was set to 120 in the previous section. require_lvl : 120 # List of groups: # A user requesting access to an item must be part of a least # of one the group the item is part of to gain access groups : # List of aliases for the group - aliases : - ANY # (optional) This is a special flag that when set to true if any other groups # are selected in a classification, this group will automatically be selected # as well. auto_select : true # Description of the group description : Employees of CSE # Long name for the group name : CSE # Short name for the group short_name : CSE # (optional) Assuming that this group is the only group selected, this is the # display name that will be used in the classification # NOTE: values must be in the aliases of this group and only this group solitary_display_name : ANY # List of sub-groups: # A user requesting access to an item must be part of a least # of one the sub-group the item is part of to gain access subgroups : # List of aliases for the subgroups - aliases : [] # Description of the sub-group description : Member of Incident Response team # Long name of the sub-group name : IR TEAM # Short name of the sub-group short_name : IR - aliases : [] description : Member of the Canadian Centre for Cyber Security # (optional) This is a special flag that auto select the corresponding # group when this sub-group is selected require_group : CSE name : CCCS short_name : CCCS # (optional) This is a special flag that makes sure that none other then # the corresponding group is selected when this sub-group is selected limited_to_group : CSE # Default restricted classification restricted : TLP:A//CMR # Default unrestricted classification: # When no classification is provided or the classification engine is # disabled, this is the classification value each item will get unrestricted : TLP:W","title":"Configuration"},{"location":"installation/classification_engine/#enabling-it-in-your-system","text":"By default, the classification engine is disabled in the system, but it can easily be enabled by creating a new config map in Kubernetes. Info For this documentation we will enable a classification engine that supports only the traffic light protocol. In your deployment directory, create a file named objects.yaml with the following content: objects.yaml apiVersion : v1 kind : ConfigMap metadata : name : assemblyline-extra-config data : classification : | enforce: true levels: - aliases: - UNRESTRICTED - U css: color: default description: Subject to standard copyright rules, TLP:WHITE information may be distributed without restriction. lvl: 100 name: TLP:WHITE short_name: TLP:W - aliases: [] css: color: success description: Recipients may share TLP:GREEN information with peers and partner organizations within their sector or community, but not via publicly accessible channels. Information in this category can be circulated widely within a particular community. TLP:GREEN information may not be released outside of the community. lvl: 110 name: TLP:GREEN short_name: TLP:G - aliases: - RESTRICTED - R css: color: warning description: Recipients may only share TLP:AMBER information with members of their own organization and with clients or customers who need to know the information to protect themselves or prevent further harm. lvl: 120 name: TLP:AMBER short_name: TLP:A required: [] groups: [] subgroups: [] restricted: TLP:A unrestricted: TLP:W Use kubectl to apply the objects.yaml file to your system: Appliance sudo microk8s kubectl apply -f ~/git/deployment/objects.yaml --namespace = al Cluster kubectl apply -f <deployment_directory>/objects.yaml --namespace = al Then you must tell your pods to use the classification engine from the newly created config map. Add the following block to the values.yaml or your deployment: Partial values.yaml to load the custom classification.yml file ... coreEnv : - name : CLASSIFICATION_CONFIGMAP value : assemblyline-extra-config - name : CLASSIFICATION_CONFIGMAP_KEY value : classification coreMounts : - name : al-extra-config mountPath : /etc/assemblyline/classification.yml subPath : classification coreVolumes : - name : al-extra-config configMap : name : assemblyline-extra-config ... Finally update your deployment using helm upgrade command : Appliance sudo microk8s helm upgrade assemblyline ~/git/assemblyline-helm-chart/assemblyline -f ~/git/deployment/values.yaml -n al Cluster helm upgrade assemblyline <assemblyline-helm-chart>/assemblyline -f <deployment_directory>/values.yaml -n al Important It takes a while for all the containers to be restarted so be patient and it will eventually show up in the UI.","title":"Enabling it in your system"},{"location":"installation/cluster/","text":"Cluster installation \u00b6 Pre-requisites \u00b6 A Kubernetes 1.18+ cluster that has an ingress controller and storage class with read/write many (RWX) support. Assemblyline is known to work with the following Kubernetes providers: Rancher AKS (Azure) EKS (Amazon) GKE (Google) kubectl already configured for your cluster on your machine helm already configured for your cluster on your machine Installation \u00b6 1. Get Assemblyline Helm chart ready \u00b6 Download the latest Assemblyline helm chart Unzip it into a directory of your choice which we will refer to as assemblyline-helm-chart Create a new directory of your choice which will hold your personal deployment configuration. We will refer to it as deployment_directory 2. Create the assemblyline namespace \u00b6 When deploying an Assemblyline instance using our chart, it must be in its own namespace. For this documentation, we will use the al namespace. kubectl create namespace al 3. Setup secrets \u00b6 In the deployment_directory you've just created, create a secrets.yaml file which will contain the different passwords used by Assemblyline. The secrets.yaml file should have the following format apiVersion : v1 kind : Secret metadata : name : assemblyline-system-passwords type : Opaque stringData : datastore-password : logging-password : # If this is the password for backends like azure blob storage, the password may need to be URL-encoded # if it includes non-alphanumeric characters filestore-password : initial-admin-password : Tip Here is an example of secrets.yaml file used for appliance deployments. When you're done setting the different passwords in your secrets.yaml file, upload it to your namespace: kubectl apply -f <deployment_directory>/secrets.yaml --namespace = al Warning From this point on, you will not need the secret.yaml file anymore. You should delete it. 4. Configure your deployment \u00b6 In your deployment_directory , create a values.yaml file which will contain the configuration specific to your deployment. Tip For an exhaustive view of all the possible parameters you can change the values.yaml you've created, refer to the assemblyline-helm-chart/assemblyline/values.yaml file. These are the strict minimum configuration changes you will need to do: Setup the ingress controller by changing the values of: ingressAnnotations.cert-manager.io/issuer: (Name of the issuer in K8s. This is for cert validation) tlsSecretName (Name of the TLS cert in k8s. This is for cert validation) configuration.ui.fqdn (Domain name for your al instance). Setup the storage classes according to your Kubernetes cluster : redisStorageClass (Use SSD backed managed disks) log-storage.volumeClaimTemplate.storageClassName (Use SSD backed managed disks) datastore.volumeClaimTemplate.storageClassName (Use SSD backed managed disks) updateStorageClass (Use standard file sharing disks) persistentStorageClass (Use standard file sharing disks) sharedStorageClass (Use standard file sharing disks) Decide where you want files stored, set the appropriate URI in the configuration.filestore.* fields. You should try to avoid using the internal filestore and use something like Azure blob store, Amazon S3... Enable/disable/configure logging features, (disabled by default). This is an example values.yaml file to get you started # 1. Setup the ingress controller ingressAnnotations : kubernetes.io/ingress.class : \"nginx\" nginx.ingress.kubernetes.io/proxy-body-size : 100M cert-manager.io/issuer : <CHANGE_ME> tlsSecretName : <CHANGE_ME> # 2. Setup the storage classes according to your Kubernetes cluster redisStorageClass : <CHANGE_ME> datastore : volumeClaimTemplate : storageClassName : <CHANGE_ME> log-storage : volumeClaimTemplate : storageClassName : <CHANGE_ME> updateStorageClass : <CHANGE_ME> persistantStorageClass : <CHANGE_ME> sharedStorageClass : <CHANGE_ME> # 3. Decide where you want files stored internalFilestore : false # Un-comment and setup if internal filestore used #filestore: # persistence: # size: 500Gi # StorageClass: <CHANGE_ME> # 4. Enable/disable/configure logging features enableLogging : false enableMetrics : false enableAPM : false internalELKStack : false seperateInternalELKStack : false loggingUsername : elastic loggingTLSVerify : \"none\" # Internal configuration for assemblyline components. See the assemblyline # administration documentation for more details. # https://cybercentrecanada.github.io/assemblyline4_docs/configuration/config_file/ configuration : # 1. Setup the ingress controller submission : max_file_size : 104857600 ui : fqdn : \"localhost\" # 3. Decide where you want files stored filestore : cache : [ \"s3://${INTERNAL_FILESTORE_ACCESS}:${INTERNAL_FILESTORE_KEY}@filestore:9000?s3_bucket=al-cache&use_ssl=False\" ] storage : [ \"s3://${INTERNAL_FILESTORE_ACCESS}:${INTERNAL_FILESTORE_KEY}@filestore:9000?s3_bucket=al-storage&use_ssl=False\" ] # 4. Enable/disable/configure logging features logging : log_level : WARNING 5. Deploy your current configuration \u00b6 Now that you've fully configured your values.yaml file, you can simply deploy it via helm by referencing the default assemblyline helm chart. helm install assemblyline <assemblyline-helm-chart>/assemblyline -f <deployment_directory>/values.yaml -n al Warning After you've ran the helm install command, the system has a lot of setting up to do (Creating database indexes, loading service, setting up default accounts, loading signatures ...). Don't expect it to be fully operational for at least the next 15 minutes. Update your deployment \u00b6 Once you have your Assemblyline chart deployed through helm, you can change any values in the values.yaml file and upgrade your deployment with the following command: helm upgrade assemblyline <assemblyline-helm-chart>/assemblyline -f <deployment_directory>/values.yaml -n al","title":"Cluster installation"},{"location":"installation/cluster/#cluster-installation","text":"","title":"Cluster installation"},{"location":"installation/cluster/#pre-requisites","text":"A Kubernetes 1.18+ cluster that has an ingress controller and storage class with read/write many (RWX) support. Assemblyline is known to work with the following Kubernetes providers: Rancher AKS (Azure) EKS (Amazon) GKE (Google) kubectl already configured for your cluster on your machine helm already configured for your cluster on your machine","title":"Pre-requisites"},{"location":"installation/cluster/#installation","text":"","title":"Installation"},{"location":"installation/cluster/#1-get-assemblyline-helm-chart-ready","text":"Download the latest Assemblyline helm chart Unzip it into a directory of your choice which we will refer to as assemblyline-helm-chart Create a new directory of your choice which will hold your personal deployment configuration. We will refer to it as deployment_directory","title":"1. Get Assemblyline Helm chart ready"},{"location":"installation/cluster/#2-create-the-assemblyline-namespace","text":"When deploying an Assemblyline instance using our chart, it must be in its own namespace. For this documentation, we will use the al namespace. kubectl create namespace al","title":"2. Create the assemblyline namespace"},{"location":"installation/cluster/#3-setup-secrets","text":"In the deployment_directory you've just created, create a secrets.yaml file which will contain the different passwords used by Assemblyline. The secrets.yaml file should have the following format apiVersion : v1 kind : Secret metadata : name : assemblyline-system-passwords type : Opaque stringData : datastore-password : logging-password : # If this is the password for backends like azure blob storage, the password may need to be URL-encoded # if it includes non-alphanumeric characters filestore-password : initial-admin-password : Tip Here is an example of secrets.yaml file used for appliance deployments. When you're done setting the different passwords in your secrets.yaml file, upload it to your namespace: kubectl apply -f <deployment_directory>/secrets.yaml --namespace = al Warning From this point on, you will not need the secret.yaml file anymore. You should delete it.","title":"3. Setup secrets"},{"location":"installation/cluster/#4-configure-your-deployment","text":"In your deployment_directory , create a values.yaml file which will contain the configuration specific to your deployment. Tip For an exhaustive view of all the possible parameters you can change the values.yaml you've created, refer to the assemblyline-helm-chart/assemblyline/values.yaml file. These are the strict minimum configuration changes you will need to do: Setup the ingress controller by changing the values of: ingressAnnotations.cert-manager.io/issuer: (Name of the issuer in K8s. This is for cert validation) tlsSecretName (Name of the TLS cert in k8s. This is for cert validation) configuration.ui.fqdn (Domain name for your al instance). Setup the storage classes according to your Kubernetes cluster : redisStorageClass (Use SSD backed managed disks) log-storage.volumeClaimTemplate.storageClassName (Use SSD backed managed disks) datastore.volumeClaimTemplate.storageClassName (Use SSD backed managed disks) updateStorageClass (Use standard file sharing disks) persistentStorageClass (Use standard file sharing disks) sharedStorageClass (Use standard file sharing disks) Decide where you want files stored, set the appropriate URI in the configuration.filestore.* fields. You should try to avoid using the internal filestore and use something like Azure blob store, Amazon S3... Enable/disable/configure logging features, (disabled by default). This is an example values.yaml file to get you started # 1. Setup the ingress controller ingressAnnotations : kubernetes.io/ingress.class : \"nginx\" nginx.ingress.kubernetes.io/proxy-body-size : 100M cert-manager.io/issuer : <CHANGE_ME> tlsSecretName : <CHANGE_ME> # 2. Setup the storage classes according to your Kubernetes cluster redisStorageClass : <CHANGE_ME> datastore : volumeClaimTemplate : storageClassName : <CHANGE_ME> log-storage : volumeClaimTemplate : storageClassName : <CHANGE_ME> updateStorageClass : <CHANGE_ME> persistantStorageClass : <CHANGE_ME> sharedStorageClass : <CHANGE_ME> # 3. Decide where you want files stored internalFilestore : false # Un-comment and setup if internal filestore used #filestore: # persistence: # size: 500Gi # StorageClass: <CHANGE_ME> # 4. Enable/disable/configure logging features enableLogging : false enableMetrics : false enableAPM : false internalELKStack : false seperateInternalELKStack : false loggingUsername : elastic loggingTLSVerify : \"none\" # Internal configuration for assemblyline components. See the assemblyline # administration documentation for more details. # https://cybercentrecanada.github.io/assemblyline4_docs/configuration/config_file/ configuration : # 1. Setup the ingress controller submission : max_file_size : 104857600 ui : fqdn : \"localhost\" # 3. Decide where you want files stored filestore : cache : [ \"s3://${INTERNAL_FILESTORE_ACCESS}:${INTERNAL_FILESTORE_KEY}@filestore:9000?s3_bucket=al-cache&use_ssl=False\" ] storage : [ \"s3://${INTERNAL_FILESTORE_ACCESS}:${INTERNAL_FILESTORE_KEY}@filestore:9000?s3_bucket=al-storage&use_ssl=False\" ] # 4. Enable/disable/configure logging features logging : log_level : WARNING","title":"4. Configure your deployment"},{"location":"installation/cluster/#5-deploy-your-current-configuration","text":"Now that you've fully configured your values.yaml file, you can simply deploy it via helm by referencing the default assemblyline helm chart. helm install assemblyline <assemblyline-helm-chart>/assemblyline -f <deployment_directory>/values.yaml -n al Warning After you've ran the helm install command, the system has a lot of setting up to do (Creating database indexes, loading service, setting up default accounts, loading signatures ...). Don't expect it to be fully operational for at least the next 15 minutes.","title":"5. Deploy your current configuration"},{"location":"installation/cluster/#update-your-deployment","text":"Once you have your Assemblyline chart deployed through helm, you can change any values in the values.yaml file and upgrade your deployment with the following command: helm upgrade assemblyline <assemblyline-helm-chart>/assemblyline -f <deployment_directory>/values.yaml -n al","title":"Update your deployment"},{"location":"installation/deployment/","text":"Choose your deployment type \u00b6 Assemblyline has two distinctive deployment types: Appliance : Single host deployment Cluster : Multi-host deployment Warning Keep in mind that you will need extra hosts for running external resources such as anti-virus products, or sandboxes (such as Cuckoo Sandbox ). These complementary products are not mandatory but will greatly complement the static analysis and file extraction performed by Assemblyline. Features \u00b6 Both deployments are the same in terms of analysis capabilities however a cluster deployment can be scaled to scan multiple millions of files per day and offer redundancy and failover. If you are deploying in the cloud, a cluster will be easier to deploy (using cloud Kubernetes offerings). However, in a lab or to support an incident response team, any powerful computer with an appliance deployment will be able to process thousands of files a day. Tip If you choose to deploy a MicroK8s appliance, you will be able to scaled it up to a small cluster using multiple machines (nodes) if need be. Deployment features rundown \u00b6 Appliance (Docker) Appliance (MicroK8s) Cluster Support all analysis capabilities Yes Yes Yes Single host installation Yes Yes No Easy step by step installation Yes Yes No Simple to deploy and manage Yes No No Multiple host installation No Optional Yes Auto-scaling of core components No Optional Yes High volume throughput No No Yes Cloud provider support (AKS, EKS, GKE...) No No Yes Redundancy / Failover support No No Yes Installation stack \u00b6 Both clustered and MicroK8s deployments use a very similar stack in the background which allows them to share the same Helm chart. Only small changes in the values.yml file are required to differentiate them. As for the Docker compose appliance deployment, it uses a simpler stack which is easier to maintain and reset but offer less features to scale to high capacity. Installation instructions \u00b6 Now that you know the difference between the two types of deployment, you can refer to their respective installation instruction to get you started. Appliance installation (Docker) Appliance installation (Microk8s) Cluster installation Tip Consider reading the configuration section of the documentation before jumping into the installation instruction. This will help you understand all the different options you can modify during the installation process.","title":"Choose your deployment type"},{"location":"installation/deployment/#choose-your-deployment-type","text":"Assemblyline has two distinctive deployment types: Appliance : Single host deployment Cluster : Multi-host deployment Warning Keep in mind that you will need extra hosts for running external resources such as anti-virus products, or sandboxes (such as Cuckoo Sandbox ). These complementary products are not mandatory but will greatly complement the static analysis and file extraction performed by Assemblyline.","title":"Choose your deployment type"},{"location":"installation/deployment/#features","text":"Both deployments are the same in terms of analysis capabilities however a cluster deployment can be scaled to scan multiple millions of files per day and offer redundancy and failover. If you are deploying in the cloud, a cluster will be easier to deploy (using cloud Kubernetes offerings). However, in a lab or to support an incident response team, any powerful computer with an appliance deployment will be able to process thousands of files a day. Tip If you choose to deploy a MicroK8s appliance, you will be able to scaled it up to a small cluster using multiple machines (nodes) if need be.","title":"Features"},{"location":"installation/deployment/#deployment-features-rundown","text":"Appliance (Docker) Appliance (MicroK8s) Cluster Support all analysis capabilities Yes Yes Yes Single host installation Yes Yes No Easy step by step installation Yes Yes No Simple to deploy and manage Yes No No Multiple host installation No Optional Yes Auto-scaling of core components No Optional Yes High volume throughput No No Yes Cloud provider support (AKS, EKS, GKE...) No No Yes Redundancy / Failover support No No Yes","title":"Deployment features rundown"},{"location":"installation/deployment/#installation-stack","text":"Both clustered and MicroK8s deployments use a very similar stack in the background which allows them to share the same Helm chart. Only small changes in the values.yml file are required to differentiate them. As for the Docker compose appliance deployment, it uses a simpler stack which is easier to maintain and reset but offer less features to scale to high capacity.","title":"Installation stack"},{"location":"installation/deployment/#installation-instructions","text":"Now that you know the difference between the two types of deployment, you can refer to their respective installation instruction to get you started. Appliance installation (Docker) Appliance installation (Microk8s) Cluster installation Tip Consider reading the configuration section of the documentation before jumping into the installation instruction. This will help you understand all the different options you can modify during the installation process.","title":"Installation instructions"},{"location":"installation/monitoring/","text":"Monitoring with ELK \u00b6 The Assemblyline helm chart gives you the option of pointing logs to an existing ELK stack or having Assemblyline create its own internal ELK for logging and metrics. Elk Stack configuration \u00b6 In the values.yaml file of your deployment, you can edit the following parameters to configure Assemblyline to send metrics and logs to a specific ELK stack. Choose the type of ELK stack deployment that corresponds the best to your setup: Appliance Internal ELK stack Partial values.yaml config for an Appliance internal ELK stack ... # Have Assemblyline send logs to the configured ELK stack enableLogging : true # Have Assemblyline send metrics to the configured ELK stack enableMetrics : true # This would have Assemblyline send APM metrics to the # configured ELK stack as well but it is very costly in # terms of resources so only turn it on if you really # need insight on API response time and core components # operation timing. enableAPM : false # We are setting up an internal ELK stack so we can turn that on internalELKStack : true # Because this is an appliance, we will reuse the same elastic # database used for data to store logs as well seperateInternalELKStack : false # The internal ELK stack use elastic as its base username and # does not verify TLS loggingUsername : elastic loggingTLSVerify : none ... Cluster Internal ELK stack Partial values.yaml config for a cluster internal ELK stack ... # Have Assemblyline send logs to the configured ELK stack enableLogging : true # Have Assemblyline send metrics to the configured ELK stack enableMetrics : true # This would have Assemblyline send APM metrics to the # configured ELK stack as well but it is very costly in # terms of resources so only turn it on if you really # need insight on API response time and core components # operation timing. enableAPM : false # We are setting up an internal ELK stack so we can turn that on internalELKStack : true # Because this is a cluster, we will have Assemblyline spin up # a completely different elastic database so the logging does not # interfere with the performance of the data seperateInternalELKStack : true # The internal ELK stack use elastic as its base username and # does not verify TLS loggingUsername : elastic loggingTLSVerify : none ... External ELK stack Partial values.yaml config for external ELK stack ... # Have Assemblyline send logs to the configured ELK stack enableLogging : true # Have Assemblyline send metrics to the configured ELK stack enableMetrics : true # This would have Assemblyline send APM metrics to the # configured ELK stack as well but it is very costly in # terms of resources so only turn it on if you really # need insight on API response time and core components # operation timing. enableAPM : false # We are setting up an external ELK stack so we will disable # those settings internalELKStack : false seperateInternalELKStack : false # -- EXTERNAL ELK Stack config -- # Elastic host where the logs will be shipped to loggingHost : https://<ELK_HOST>:443/ # Kibana dashboard location kibanaHost : https://<ELK_HOST>:443/kibana # Username that will be used to login to the elastic on your # ELK stack loggingUsername : <YOUR_ELK_USERNAME> # Should you verify TLS on your ELK stack? loggingTLSVerify : \"full\" # Finally configure al_metrics to save metrics to your stack configuration : core : metrics : elasticsearch : hosts : [ \"https://${LOGGING_USERNAME}:${LOGGING_PASSWORD}@<ELK_HOST>:443\" ] # If you're using HTTPS and don't want certificate failures you can put # your CA here host_certificates : | -----BEGIN CERTIFICATE----- ... -----END CERTIFICATE----- ... Finally update your deployment using helm upgrade command : Appliance sudo microk8s helm upgrade assemblyline ~/git/assemblyline-helm-chart/assemblyline -f ~/git/deployment/values.yaml -n al Cluster helm upgrade assemblyline <assemblyline-helm-chart>/assemblyline -f <deployment_directory>/values.yaml -n al Logstash Pipelines \u00b6 You can write custom pipelines to help enrich your data when passed through Logstash. You can set your custom Logstash pipeline under customLogstashPipeline in your values.yaml file of your deployment. Partial values.yaml to add a simple Logstash pipeline ... # Turn on Logstash support useLogstash : true customLogstashPipeline : | input { beats { port => 5044 codec => \"json\" } } filter{ mutate { add_field => {\"sent_to_logstash\" => \"True\"} } } output { elasticsearch{ hosts => \"http://elasticsearch:9200\" index => \"assemblyline-logs\" codec => \"json\" } } ... Kibana Dashboards \u00b6 Within Kibana, there is the ability to use dashboards to visualize your data into one consolidated view to make it easier for monitoring, like a hub. You can get our latest exported dashboards directly from the assemblyline-base source and then use Kibana import features to use them in your ELK Stack. Creation \u00b6 Dashboards are made up of visualizations, and these can come in different forms: graphs, metrics, gauges, tables, maps, etc. Each visualization requires an index pattern to get the data from and setting a date range, this throws all relevant data within the specified timeframe into a bucket to be used by the visualization. Dashboards can also be imported/exported for use across different ELKs but require dependencies like index patterns for them to function out of the box, otherwise requires editing the dashboard file. Navigation \u00b6 All dashboards give you the ability to filter your data, like what you will find under the Discover tab of Kibana. This will allow you to filter a certain dashboard based on a query you give. If you want more info about using Kibana's filtering and navigation feature, check the explore your data documentation.","title":"Monitoring with ELK"},{"location":"installation/monitoring/#monitoring-with-elk","text":"The Assemblyline helm chart gives you the option of pointing logs to an existing ELK stack or having Assemblyline create its own internal ELK for logging and metrics.","title":"Monitoring with ELK"},{"location":"installation/monitoring/#elk-stack-configuration","text":"In the values.yaml file of your deployment, you can edit the following parameters to configure Assemblyline to send metrics and logs to a specific ELK stack. Choose the type of ELK stack deployment that corresponds the best to your setup: Appliance Internal ELK stack Partial values.yaml config for an Appliance internal ELK stack ... # Have Assemblyline send logs to the configured ELK stack enableLogging : true # Have Assemblyline send metrics to the configured ELK stack enableMetrics : true # This would have Assemblyline send APM metrics to the # configured ELK stack as well but it is very costly in # terms of resources so only turn it on if you really # need insight on API response time and core components # operation timing. enableAPM : false # We are setting up an internal ELK stack so we can turn that on internalELKStack : true # Because this is an appliance, we will reuse the same elastic # database used for data to store logs as well seperateInternalELKStack : false # The internal ELK stack use elastic as its base username and # does not verify TLS loggingUsername : elastic loggingTLSVerify : none ... Cluster Internal ELK stack Partial values.yaml config for a cluster internal ELK stack ... # Have Assemblyline send logs to the configured ELK stack enableLogging : true # Have Assemblyline send metrics to the configured ELK stack enableMetrics : true # This would have Assemblyline send APM metrics to the # configured ELK stack as well but it is very costly in # terms of resources so only turn it on if you really # need insight on API response time and core components # operation timing. enableAPM : false # We are setting up an internal ELK stack so we can turn that on internalELKStack : true # Because this is a cluster, we will have Assemblyline spin up # a completely different elastic database so the logging does not # interfere with the performance of the data seperateInternalELKStack : true # The internal ELK stack use elastic as its base username and # does not verify TLS loggingUsername : elastic loggingTLSVerify : none ... External ELK stack Partial values.yaml config for external ELK stack ... # Have Assemblyline send logs to the configured ELK stack enableLogging : true # Have Assemblyline send metrics to the configured ELK stack enableMetrics : true # This would have Assemblyline send APM metrics to the # configured ELK stack as well but it is very costly in # terms of resources so only turn it on if you really # need insight on API response time and core components # operation timing. enableAPM : false # We are setting up an external ELK stack so we will disable # those settings internalELKStack : false seperateInternalELKStack : false # -- EXTERNAL ELK Stack config -- # Elastic host where the logs will be shipped to loggingHost : https://<ELK_HOST>:443/ # Kibana dashboard location kibanaHost : https://<ELK_HOST>:443/kibana # Username that will be used to login to the elastic on your # ELK stack loggingUsername : <YOUR_ELK_USERNAME> # Should you verify TLS on your ELK stack? loggingTLSVerify : \"full\" # Finally configure al_metrics to save metrics to your stack configuration : core : metrics : elasticsearch : hosts : [ \"https://${LOGGING_USERNAME}:${LOGGING_PASSWORD}@<ELK_HOST>:443\" ] # If you're using HTTPS and don't want certificate failures you can put # your CA here host_certificates : | -----BEGIN CERTIFICATE----- ... -----END CERTIFICATE----- ... Finally update your deployment using helm upgrade command : Appliance sudo microk8s helm upgrade assemblyline ~/git/assemblyline-helm-chart/assemblyline -f ~/git/deployment/values.yaml -n al Cluster helm upgrade assemblyline <assemblyline-helm-chart>/assemblyline -f <deployment_directory>/values.yaml -n al","title":"Elk Stack configuration"},{"location":"installation/monitoring/#logstash-pipelines","text":"You can write custom pipelines to help enrich your data when passed through Logstash. You can set your custom Logstash pipeline under customLogstashPipeline in your values.yaml file of your deployment. Partial values.yaml to add a simple Logstash pipeline ... # Turn on Logstash support useLogstash : true customLogstashPipeline : | input { beats { port => 5044 codec => \"json\" } } filter{ mutate { add_field => {\"sent_to_logstash\" => \"True\"} } } output { elasticsearch{ hosts => \"http://elasticsearch:9200\" index => \"assemblyline-logs\" codec => \"json\" } } ...","title":"Logstash Pipelines"},{"location":"installation/monitoring/#kibana-dashboards","text":"Within Kibana, there is the ability to use dashboards to visualize your data into one consolidated view to make it easier for monitoring, like a hub. You can get our latest exported dashboards directly from the assemblyline-base source and then use Kibana import features to use them in your ELK Stack.","title":"Kibana Dashboards"},{"location":"installation/monitoring/#creation","text":"Dashboards are made up of visualizations, and these can come in different forms: graphs, metrics, gauges, tables, maps, etc. Each visualization requires an index pattern to get the data from and setting a date range, this throws all relevant data within the specified timeframe into a bucket to be used by the visualization. Dashboards can also be imported/exported for use across different ELKs but require dependencies like index patterns for them to function out of the box, otherwise requires editing the dashboard file.","title":"Creation"},{"location":"installation/monitoring/#navigation","text":"All dashboards give you the ability to filter your data, like what you will find under the Discover tab of Kibana. This will allow you to filter a certain dashboard based on a query you give. If you want more info about using Kibana's filtering and navigation feature, check the explore your data documentation.","title":"Navigation"},{"location":"installation/configuration/authentication/","text":"Authentication section \u00b6 Assemblyline comes with a built-in user management database, so no external identity sources are required. However, to facilitate user management in larger organizations you can integrate Assemblyline with external identity providers. The authentication section ( auth: ) of the configuration files contains all the different parameters that you can change to turn on/off the different authentication features that Assemblyline supports. Default values for the authentication section ... auth : allow_2fa : true allow_apikeys : true allow_extended_apikeys : true allow_security_tokens : true internal : enabled : true failure_ttl : 60 max_failures : 5 password_requirements : lower : false min_length : 12 number : false special : false upper : false signup : enabled : false notify : activated_template : null api_key : null authorization_template : null base_url : null password_reset_template : null registration_template : null smtp : from_adr : null host : null password : null port : 587 tls : true user : null valid_email_patterns : - .* - .*@localhost ldap : admin_dn : null auto_create : true auto_sync : true base : ou=people,dc=assemblyline,dc=local bind_pass : null bind_user : null classification_mappings : {} email_field : mail enabled : false group_lookup_query : (&(objectClass=Group)(member=%s)) image_field : jpegPhoto image_format : jpeg name_field : cn signature_importer_dn : null signature_manager_dn : null uid_field : uid uri : ldap://localhost:389 oauth : enabled : false gravatar_enabled : true providers : auth0 : access_token_url : https://{TENANT}.auth0.com/oauth/token api_base_url : https://{TENANT}.auth0.com/ authorize_url : https://{TENANT}.auth0.com/authorize client_id : null client_kwargs : scope : openid email profile client_secret : null jwks_uri : https://{TENANT}.auth0.com/.well-known/jwks.json user_get : userinfo azure_ad : access_token_url : https://login.microsoftonline.com/common/oauth2/token api_base_url : https://login.microsoft.com/common/ authorize_url : https://login.microsoftonline.com/common/oauth2/authorize client_id : null client_kwargs : scope : openid email profile client_secret : null jwks_uri : https://login.microsoftonline.com/common/discovery/v2.0/keys user_get : openid/userinfo google : access_token_url : https://oauth2.googleapis.com/token api_base_url : https://openidconnect.googleapis.com/ authorize_url : https://accounts.google.com/o/oauth2/v2/auth client_id : null client_kwargs : scope : openid email profile client_secret : null jwks_uri : https://www.googleapis.com/oauth2/v3/certs user_get : v1/userinfo ... Tip Refer to the changing the configuration file documentation for more detail on where and how to change the configuration of the system. Parameter definitions \u00b6 The auth configuration block has a few parameters at the top level that help you turn on or off a few security features supported in the system. Here is an example of a configuration block with those top-level parameters and an explanation of what they do: Top-level parameters auth : # Turns on/off two-factor authentication in the system allow_2fa : true # Turn on/off usage of API Keys in the system # NOTE: if you turn this off, this will severely limit API access allow_apikeys : true # Turn on/off usage of extended API via the API keys allow_extended_apikeys : true # Turn on/off usage of security token as two-factor authentication (ex: yubikeys) allow_security_tokens : true Internal authenticator \u00b6 The configuration block at auth.internal allows you to configure the Assemblyline internal authenticator. Here is an example of a configuration block with inline comments about the purpose of every single parameter: Internal auth configuration example auth : internal : # Enable or disable the internal authenticator enabled : true # Time in seconds the user will have to wait after # too many authentication failures failure_ttl : 60 # Number of authentication failures before temporarily # locking down the user max_failures : 5 # Password complexity requirements for the system password_requirements : # Are lowercase characters mandatory? lower : false # What is the minimal password length min_length : 12 # Are numbers mandatory? number : false # Are special characters mandatory? special : false # Are uppercase characters mandatory? upper : false signup : # Can a user automatically signup for the system enabled : false # Configuration block for GC Notify signup and password reset # see: https://notification.canada.ca/ notify : activated_template : null api_key : null authorization_template : null base_url : null password_reset_template : null registration_template : null # Configuration block for SMTP signup and password reset smtp : # Email address used for sender from_adr : null # Host of the SMTP server host : null # Password for the SMTP server password : null # Port of the SMTP server port : 587 # Should we communicate with SMTP server via TLS? tls : true # User to authenticate to the SMTP server user : null # Email patterns that will be allowed to # automatically signup for an account valid_email_patterns : - .* - .*@localhost LDAP Authentication \u00b6 The configuration block at auth.ldap allows you to easily add authentication via your LDAP server. The LDAP authentication module will be able to automatically assign roles, classification, avatar, name, and email address based on the properties of the LDAP user and the groups it is a member of. Here is an example configuration block to add to your configuration file that will allow you to connect to the docker-test-openldap server from: https://github.com/rroemhild/docker-test-openldap LDAP configuration example auth : internal : # Disable internal login, you could also leave it on if you want enabled : false ldap : # Should LDAP be enabled or not? enabled : true # DN of the group or the user who will get admin privileges admin_dn : cn=admin_staff,ou=people,dc=planetexpress,dc=com # Auto-create users if they are missing, this means # that if a user exists in LDAP, Assemblyline will create an # account for it upon the first login auto_create : true # Should we automatically sync roles, classification, avatar # email, name... with the LDAP server upon each login? auto_sync : true # Base DN for the users base : ou=people,dc=planetexpress,dc=com # Password used to query the LDAP server bind_pass : null # User use to query the LDAP server bind_user : null classification_mappings : {} # Name of the field containing the email address email_field : mail # How the group lookup is queried group_lookup_query : (&(objectClass=Group)(member=%s)) # Name of the field containing the user's avatar image_field : jpegPhoto # Type of image used to store the avatar image_format : jpeg # Name of the field containing the user's name name_field : cn # DN of the group or the user who will get signature_importer role signature_importer_dn : null # DN of the group or the user who will get signature_manager role signature_manager_dn : null # Field name for the UID uid_field : uid # URI to the LDAP server uri : ldaps://<ldap_ip_or_domain>:636 OAuth Authentication \u00b6 The configuration block at auth.oauth allows you to add OAuth authentication to your system. Assemblyline OAuth module is configurable enough to allow you to use almost any OAuth provider. It has been thoroughly tested with: Microsoft Accounts Google Accounts Auth0 Microsoft Azure Active Directory Accounts Here is an exhaustive configuration block that explains every single parameter from the OAuth configuration block: Exhaustive OAuth configuration example auth : internal : # Disable internal login, you could also leave it on if you want enabled : false oauth : # Should OAuth authentication be enabled or not enabled : true # Should we try to pull the user's avatar using gravatar gravatar_enabled : false # OAuth providers configuration block, you can have as many OAuth # providers as you want providers : # Name of the provider displayed in the UI local_provider : # Auto-create users if they are missing, this means # that if a user exists in the OAuth provider, Assemblyline # will create an account for it upon the first login # WARNING: If you set it to true for let's say Google's # OAuth provider, anyone with a google account # essentially has access to your system auto_create : true # Should we automatically sync roles, classification, avatar # email, name... with the OAuth provider upon each login? auto_sync : true # Automatic role and classification assignments auto_properties : # any user with a @localhost.local email will be given # TLP:Amber classification - field : email pattern : .*@localhost\\.local$ type : classification value : \"TLP:A\" # any user within the admins-sg will be made # administrator in the system - field : groups pattern : ^admins-sg$ type : role value : admin # URL used to get the access token access_token_url : https://oauth2.localhost/token # Base URL for downloading the user's and groups info api_base_url : https://openidconnect.localhost/ # URL used to authorize access to a resource authorize_url : https://localhost/oauth2/auth # ID of your application to authenticate to the OAuth # provider client_id : null # Password to your application to authenticate to the # OAuth provider client_secret : null # Keyword arguments passed to the different URLs # (to set the scope for example) client_kwargs : scope : openid email profile # URL used to verify if a returned JWKS token is valid jwks_uri : https://localhost/oauth2/certs # Name of the field that will contain the user ID uid_field : uid # Should we generate a random username for the # authenticated user? uid_randomize : false # How many digits should we add at the end of the username? uid_randomize_digits : 0 # What is the delimiter used by the random name generator? uid_randomize_delimiter : \"-\" # Reged used to parse and email address and capture parts # to create a user ID out of it uid_regex : ^(.*)@(\\w*).*$ # Format of the user ID based on the captured parts from the regex uid_format : '{}-{}' # Should we use the new callback method? use_new_callback_format : true # Path from the base_url to fetch the user info user_get : user/info # Path from the base to fetch the group info user_groups : group/info # Field return by the group info API call that contains the # list of groups user_groups_data_field : null # Name of the field in the list of groups that contains the # name of the group user_groups_name_field : null Here is an example configuration block that would let you use Auth0 if you would change your client_id and client_secret and that you would change the tenant_name to yours: Auth0 configuration example auth : internal : # Disable internal login, you could also leave it on if you want enabled : false oauth : # Enable oAuth enabled : true # Setup the auto0 provider providers : auth0 : # It is safe to auto-create users here # because it is your OAuth tenant auto_create : true auto_sync : true # Put your client ID and secret here client_id : <YOUR_CLIENT_ID> client_secret : <YOUR_CLIENT_SECRET> client_kwargs : scope : openid email profile # Set your tenant's name in the following URLs access_token_url : https://<TENANT_NAME>.auth0.com/oauth/token api_base_url : https://<TENANT_NAME>.auth0.com/ authorize_url : https://<TENANT_NAME>.auth0.com/authorize jwks_uri : https://<TENANT_NAME>.auth0.com/.well-known/jwks.json user_get : userinfo","title":"Authentication section"},{"location":"installation/configuration/authentication/#authentication-section","text":"Assemblyline comes with a built-in user management database, so no external identity sources are required. However, to facilitate user management in larger organizations you can integrate Assemblyline with external identity providers. The authentication section ( auth: ) of the configuration files contains all the different parameters that you can change to turn on/off the different authentication features that Assemblyline supports. Default values for the authentication section ... auth : allow_2fa : true allow_apikeys : true allow_extended_apikeys : true allow_security_tokens : true internal : enabled : true failure_ttl : 60 max_failures : 5 password_requirements : lower : false min_length : 12 number : false special : false upper : false signup : enabled : false notify : activated_template : null api_key : null authorization_template : null base_url : null password_reset_template : null registration_template : null smtp : from_adr : null host : null password : null port : 587 tls : true user : null valid_email_patterns : - .* - .*@localhost ldap : admin_dn : null auto_create : true auto_sync : true base : ou=people,dc=assemblyline,dc=local bind_pass : null bind_user : null classification_mappings : {} email_field : mail enabled : false group_lookup_query : (&(objectClass=Group)(member=%s)) image_field : jpegPhoto image_format : jpeg name_field : cn signature_importer_dn : null signature_manager_dn : null uid_field : uid uri : ldap://localhost:389 oauth : enabled : false gravatar_enabled : true providers : auth0 : access_token_url : https://{TENANT}.auth0.com/oauth/token api_base_url : https://{TENANT}.auth0.com/ authorize_url : https://{TENANT}.auth0.com/authorize client_id : null client_kwargs : scope : openid email profile client_secret : null jwks_uri : https://{TENANT}.auth0.com/.well-known/jwks.json user_get : userinfo azure_ad : access_token_url : https://login.microsoftonline.com/common/oauth2/token api_base_url : https://login.microsoft.com/common/ authorize_url : https://login.microsoftonline.com/common/oauth2/authorize client_id : null client_kwargs : scope : openid email profile client_secret : null jwks_uri : https://login.microsoftonline.com/common/discovery/v2.0/keys user_get : openid/userinfo google : access_token_url : https://oauth2.googleapis.com/token api_base_url : https://openidconnect.googleapis.com/ authorize_url : https://accounts.google.com/o/oauth2/v2/auth client_id : null client_kwargs : scope : openid email profile client_secret : null jwks_uri : https://www.googleapis.com/oauth2/v3/certs user_get : v1/userinfo ... Tip Refer to the changing the configuration file documentation for more detail on where and how to change the configuration of the system.","title":"Authentication section"},{"location":"installation/configuration/authentication/#parameter-definitions","text":"The auth configuration block has a few parameters at the top level that help you turn on or off a few security features supported in the system. Here is an example of a configuration block with those top-level parameters and an explanation of what they do: Top-level parameters auth : # Turns on/off two-factor authentication in the system allow_2fa : true # Turn on/off usage of API Keys in the system # NOTE: if you turn this off, this will severely limit API access allow_apikeys : true # Turn on/off usage of extended API via the API keys allow_extended_apikeys : true # Turn on/off usage of security token as two-factor authentication (ex: yubikeys) allow_security_tokens : true","title":"Parameter definitions"},{"location":"installation/configuration/authentication/#internal-authenticator","text":"The configuration block at auth.internal allows you to configure the Assemblyline internal authenticator. Here is an example of a configuration block with inline comments about the purpose of every single parameter: Internal auth configuration example auth : internal : # Enable or disable the internal authenticator enabled : true # Time in seconds the user will have to wait after # too many authentication failures failure_ttl : 60 # Number of authentication failures before temporarily # locking down the user max_failures : 5 # Password complexity requirements for the system password_requirements : # Are lowercase characters mandatory? lower : false # What is the minimal password length min_length : 12 # Are numbers mandatory? number : false # Are special characters mandatory? special : false # Are uppercase characters mandatory? upper : false signup : # Can a user automatically signup for the system enabled : false # Configuration block for GC Notify signup and password reset # see: https://notification.canada.ca/ notify : activated_template : null api_key : null authorization_template : null base_url : null password_reset_template : null registration_template : null # Configuration block for SMTP signup and password reset smtp : # Email address used for sender from_adr : null # Host of the SMTP server host : null # Password for the SMTP server password : null # Port of the SMTP server port : 587 # Should we communicate with SMTP server via TLS? tls : true # User to authenticate to the SMTP server user : null # Email patterns that will be allowed to # automatically signup for an account valid_email_patterns : - .* - .*@localhost","title":"Internal authenticator"},{"location":"installation/configuration/authentication/#ldap-authentication","text":"The configuration block at auth.ldap allows you to easily add authentication via your LDAP server. The LDAP authentication module will be able to automatically assign roles, classification, avatar, name, and email address based on the properties of the LDAP user and the groups it is a member of. Here is an example configuration block to add to your configuration file that will allow you to connect to the docker-test-openldap server from: https://github.com/rroemhild/docker-test-openldap LDAP configuration example auth : internal : # Disable internal login, you could also leave it on if you want enabled : false ldap : # Should LDAP be enabled or not? enabled : true # DN of the group or the user who will get admin privileges admin_dn : cn=admin_staff,ou=people,dc=planetexpress,dc=com # Auto-create users if they are missing, this means # that if a user exists in LDAP, Assemblyline will create an # account for it upon the first login auto_create : true # Should we automatically sync roles, classification, avatar # email, name... with the LDAP server upon each login? auto_sync : true # Base DN for the users base : ou=people,dc=planetexpress,dc=com # Password used to query the LDAP server bind_pass : null # User use to query the LDAP server bind_user : null classification_mappings : {} # Name of the field containing the email address email_field : mail # How the group lookup is queried group_lookup_query : (&(objectClass=Group)(member=%s)) # Name of the field containing the user's avatar image_field : jpegPhoto # Type of image used to store the avatar image_format : jpeg # Name of the field containing the user's name name_field : cn # DN of the group or the user who will get signature_importer role signature_importer_dn : null # DN of the group or the user who will get signature_manager role signature_manager_dn : null # Field name for the UID uid_field : uid # URI to the LDAP server uri : ldaps://<ldap_ip_or_domain>:636","title":"LDAP Authentication"},{"location":"installation/configuration/authentication/#oauth-authentication","text":"The configuration block at auth.oauth allows you to add OAuth authentication to your system. Assemblyline OAuth module is configurable enough to allow you to use almost any OAuth provider. It has been thoroughly tested with: Microsoft Accounts Google Accounts Auth0 Microsoft Azure Active Directory Accounts Here is an exhaustive configuration block that explains every single parameter from the OAuth configuration block: Exhaustive OAuth configuration example auth : internal : # Disable internal login, you could also leave it on if you want enabled : false oauth : # Should OAuth authentication be enabled or not enabled : true # Should we try to pull the user's avatar using gravatar gravatar_enabled : false # OAuth providers configuration block, you can have as many OAuth # providers as you want providers : # Name of the provider displayed in the UI local_provider : # Auto-create users if they are missing, this means # that if a user exists in the OAuth provider, Assemblyline # will create an account for it upon the first login # WARNING: If you set it to true for let's say Google's # OAuth provider, anyone with a google account # essentially has access to your system auto_create : true # Should we automatically sync roles, classification, avatar # email, name... with the OAuth provider upon each login? auto_sync : true # Automatic role and classification assignments auto_properties : # any user with a @localhost.local email will be given # TLP:Amber classification - field : email pattern : .*@localhost\\.local$ type : classification value : \"TLP:A\" # any user within the admins-sg will be made # administrator in the system - field : groups pattern : ^admins-sg$ type : role value : admin # URL used to get the access token access_token_url : https://oauth2.localhost/token # Base URL for downloading the user's and groups info api_base_url : https://openidconnect.localhost/ # URL used to authorize access to a resource authorize_url : https://localhost/oauth2/auth # ID of your application to authenticate to the OAuth # provider client_id : null # Password to your application to authenticate to the # OAuth provider client_secret : null # Keyword arguments passed to the different URLs # (to set the scope for example) client_kwargs : scope : openid email profile # URL used to verify if a returned JWKS token is valid jwks_uri : https://localhost/oauth2/certs # Name of the field that will contain the user ID uid_field : uid # Should we generate a random username for the # authenticated user? uid_randomize : false # How many digits should we add at the end of the username? uid_randomize_digits : 0 # What is the delimiter used by the random name generator? uid_randomize_delimiter : \"-\" # Reged used to parse and email address and capture parts # to create a user ID out of it uid_regex : ^(.*)@(\\w*).*$ # Format of the user ID based on the captured parts from the regex uid_format : '{}-{}' # Should we use the new callback method? use_new_callback_format : true # Path from the base_url to fetch the user info user_get : user/info # Path from the base to fetch the group info user_groups : group/info # Field return by the group info API call that contains the # list of groups user_groups_data_field : null # Name of the field in the list of groups that contains the # name of the group user_groups_name_field : null Here is an example configuration block that would let you use Auth0 if you would change your client_id and client_secret and that you would change the tenant_name to yours: Auth0 configuration example auth : internal : # Disable internal login, you could also leave it on if you want enabled : false oauth : # Enable oAuth enabled : true # Setup the auto0 provider providers : auth0 : # It is safe to auto-create users here # because it is your OAuth tenant auto_create : true auto_sync : true # Put your client ID and secret here client_id : <YOUR_CLIENT_ID> client_secret : <YOUR_CLIENT_SECRET> client_kwargs : scope : openid email profile # Set your tenant's name in the following URLs access_token_url : https://<TENANT_NAME>.auth0.com/oauth/token api_base_url : https://<TENANT_NAME>.auth0.com/ authorize_url : https://<TENANT_NAME>.auth0.com/authorize jwks_uri : https://<TENANT_NAME>.auth0.com/.well-known/jwks.json user_get : userinfo","title":"OAuth Authentication"},{"location":"installation/configuration/config_file/","text":"Configuration YAML file \u00b6 Assemblyline 4 configuration is done using a YAML file ( config.yml ) which is deployed to all containers when they are launched. Specification and defaults \u00b6 The full specification of the file is defined here . The Object Data Model (ODM) converts the python model to a YAML file which looks like the following by default: Default configuration file values auth : allow_2fa : true allow_apikeys : true allow_extended_apikeys : true allow_security_tokens : true internal : enabled : true failure_ttl : 60 max_failures : 5 password_requirements : lower : false min_length : 12 number : false special : false upper : false signup : enabled : false notify : activated_template : null api_key : null authorization_template : null base_url : null password_reset_template : null registration_template : null smtp : from_adr : null host : null password : null port : 587 tls : true user : null valid_email_patterns : - .* - .*@localhost ldap : admin_dn : null auto_create : true auto_sync : true base : ou=people,dc=assemblyline,dc=local bind_pass : null bind_user : null classification_mappings : {} email_field : mail enabled : false group_lookup_query : (&(objectClass=Group)(member=%s)) image_field : jpegPhoto image_format : jpeg name_field : cn signature_importer_dn : null signature_manager_dn : null uid_field : uid uri : ldap://localhost:389 oauth : enabled : false gravatar_enabled : true providers : auth0 : access_token_url : https://{TENANT}.auth0.com/oauth/token api_base_url : https://{TENANT}.auth0.com/ authorize_url : https://{TENANT}.auth0.com/authorize client_id : null client_kwargs : scope : openid email profile client_secret : null jwks_uri : https://{TENANT}.auth0.com/.well-known/jwks.json user_get : userinfo azure_ad : access_token_url : https://login.microsoftonline.com/common/oauth2/token api_base_url : https://login.microsoft.com/common/ authorize_url : https://login.microsoftonline.com/common/oauth2/authorize client_id : null client_kwargs : scope : openid email profile client_secret : null jwks_uri : https://login.microsoftonline.com/common/discovery/v2.0/keys user_get : openid/userinfo google : access_token_url : https://oauth2.googleapis.com/token api_base_url : https://openidconnect.googleapis.com/ authorize_url : https://accounts.google.com/o/oauth2/v2/auth client_id : null client_kwargs : scope : openid email profile client_secret : null jwks_uri : https://www.googleapis.com/oauth2/v3/certs user_get : v1/userinfo core : alerter : alert_ttl : 90 constant_alert_fields : - alert_id - file - ts default_group_field : file.sha256 delay : 300 filtering_group_fields : - file.name - status - priority non_filtering_group_fields : - file.md5 - file.sha1 - file.sha256 process_alert_message : assemblyline_core.alerter.processing.process_alert_message dispatcher : max_inflight : 1000 timeout : 900 expiry : batch_delete : false delay : 0 delete_storage : true sleep_time : 15 workers : 20 ingester : cache_dtl : 2 default_max_extracted : 100 default_max_supplementary : 100 default_resubmit_services : [] default_services : [] default_user : internal description_prefix : Bulk expire_after : 1296000 get_whitelist_verdict : assemblyline.common.signaturing.drop incomplete_expire_after_seconds : 3600 incomplete_stale_after_seconds : 1800 is_low_priority : assemblyline.common.null.always_false max_inflight : 500 sampling_at : critical : 500000 high : 1000000 low : 10000000 medium : 2000000 stale_after_seconds : 86400 whitelist : assemblyline.common.null.whitelist metrics : apm_server : server_url : null token : null elasticsearch : cold : 30 delete : 90 host_certificates : null hosts : null unit : d warm : 2 export_interval : 5 redis : &id001 host : 127.0.0.1 port : 6379 redis : nonpersistent : *id001 persistent : host : 127.0.0.1 port : 6380 scaler : service_defaults : backlog : 100 environment : - name : SERVICE_API_HOST value : http://service-server:5003 - name : AL_SERVICE_TASK_LIMIT value : inf growth : 60 min_instances : 0 shrink : 30 datasources : al : classpath : assemblyline.datasource.al.AL config : {} alert : classpath : assemblyline.datasource.alert.Alert config : {} datastore : hosts : - http://elastic:devpass@localhost ilm : days_until_archive : 15 enabled : false indexes : alert : &id002 cold : 15 delete : 30 unit : d warm : 5 error : *id002 file : *id002 result : *id002 submission : *id002 update_archive : false type : elasticsearch filestore : cache : - s3://al_storage_key:Ch@ngeTh!sPa33w0rd@localhost:9000?s3_bucket=al-cache&use_ssl=False storage : - s3://al_storage_key:Ch@ngeTh!sPa33w0rd@localhost:9000?s3_bucket=al-storage&use_ssl=False logging : export_interval : 5 heartbeat_file : /tmp/heartbeat log_as_json : true log_directory : /var/log/assemblyline/ log_level : INFO log_to_console : true log_to_file : false log_to_syslog : false syslog_host : localhost syslog_port : 514 services : allow_insecure_registry : false categories : - Antivirus - Dynamic Analysis - External - Extraction - Filtering - Networking - Static Analysis cpu_reservation : 0.25 default_timeout : 60 image_variables : {} min_service_workers : 0 preferred_update_channel : stable stages : - FILTER - EXTRACT - CORE - SECONDARY - POST submission : default_max_extracted : 500 default_max_supplementary : 500 dtl : 30 max_dtl : 0 max_extraction_depth : 6 max_file_size : 104857600 max_metadata_length : 4096 tag_types : attribution : - attribution.actor - attribution.campaign - attribution.exploit - attribution.implant - attribution.family - attribution.network - av.virus_name - file.config - technique.obfuscation behavior : - file.behavior ioc : - network.email.address - network.static.ip - network.static.domain - network.static.uri - network.dynamic.ip - network.dynamic.domain - network.dynamic.uri system : constants : assemblyline.common.constants organisation : ACME type : production ui : allow_malicious_hinting : false allow_raw_downloads : true allow_url_submissions : true audit : true banner : null banner_level : info debug : false download_encoding : cart email : null enforce_quota : true fqdn : localhost ingest_max_priority : 250 read_only : false read_only_offset : '' secret_key : This is the default flask secret key... you should change this! session_duration : 3600 statistics : alert : - al.attrib - al.av - al.behavior - al.domain - al.ip - al.yara - file.name - file.md5 - owner submission : - params.submitter tos : null tos_lockout : false tos_lockout_notify : null url_submission_headers : {} url_submission_proxies : {} validate_session_ip : true validate_session_useragent : true Layers of the configuration file \u00b6 The configuration file is built in layers: The ODM converts the python classes to the default values as shown above The default assemblyline helm chart values.yaml file changes certain of these values to adapt them to a Kubernetes deployment Your deployment's values.yaml file change the values to their final form Changing the configuration file \u00b6 If you want to change the config.yml file that will be deployed in the containers, it will have to be done through the configuration section found in the values.yml file of your deployment. Example Let's say that you would want to change the log level in the system to ERROR an up. First of you would edit the values.yaml file of your personal deployment to add the changes to the configuration section: ... configuration : logging : log_level : ERROR ... Then you would simply deploy that new values.yaml file using the helm upgrade command specific to your deployment: Cluster deployment update Appliance deployment update Exhaustive configuration file documentation \u00b6 All parameters of each configuration section will be thoroughly documented in their respective pages. Here are the links to the different section documentations: Authentication (auth:) Core components (core:) Data sources (datasources:) Database (datastore:) File storage (filestore:) Logging (logging:) Services (services:) Submission (submission:) System (system:) User Interface (ui:)","title":"Configuration YAML file"},{"location":"installation/configuration/config_file/#configuration-yaml-file","text":"Assemblyline 4 configuration is done using a YAML file ( config.yml ) which is deployed to all containers when they are launched.","title":"Configuration YAML file"},{"location":"installation/configuration/config_file/#specification-and-defaults","text":"The full specification of the file is defined here . The Object Data Model (ODM) converts the python model to a YAML file which looks like the following by default: Default configuration file values auth : allow_2fa : true allow_apikeys : true allow_extended_apikeys : true allow_security_tokens : true internal : enabled : true failure_ttl : 60 max_failures : 5 password_requirements : lower : false min_length : 12 number : false special : false upper : false signup : enabled : false notify : activated_template : null api_key : null authorization_template : null base_url : null password_reset_template : null registration_template : null smtp : from_adr : null host : null password : null port : 587 tls : true user : null valid_email_patterns : - .* - .*@localhost ldap : admin_dn : null auto_create : true auto_sync : true base : ou=people,dc=assemblyline,dc=local bind_pass : null bind_user : null classification_mappings : {} email_field : mail enabled : false group_lookup_query : (&(objectClass=Group)(member=%s)) image_field : jpegPhoto image_format : jpeg name_field : cn signature_importer_dn : null signature_manager_dn : null uid_field : uid uri : ldap://localhost:389 oauth : enabled : false gravatar_enabled : true providers : auth0 : access_token_url : https://{TENANT}.auth0.com/oauth/token api_base_url : https://{TENANT}.auth0.com/ authorize_url : https://{TENANT}.auth0.com/authorize client_id : null client_kwargs : scope : openid email profile client_secret : null jwks_uri : https://{TENANT}.auth0.com/.well-known/jwks.json user_get : userinfo azure_ad : access_token_url : https://login.microsoftonline.com/common/oauth2/token api_base_url : https://login.microsoft.com/common/ authorize_url : https://login.microsoftonline.com/common/oauth2/authorize client_id : null client_kwargs : scope : openid email profile client_secret : null jwks_uri : https://login.microsoftonline.com/common/discovery/v2.0/keys user_get : openid/userinfo google : access_token_url : https://oauth2.googleapis.com/token api_base_url : https://openidconnect.googleapis.com/ authorize_url : https://accounts.google.com/o/oauth2/v2/auth client_id : null client_kwargs : scope : openid email profile client_secret : null jwks_uri : https://www.googleapis.com/oauth2/v3/certs user_get : v1/userinfo core : alerter : alert_ttl : 90 constant_alert_fields : - alert_id - file - ts default_group_field : file.sha256 delay : 300 filtering_group_fields : - file.name - status - priority non_filtering_group_fields : - file.md5 - file.sha1 - file.sha256 process_alert_message : assemblyline_core.alerter.processing.process_alert_message dispatcher : max_inflight : 1000 timeout : 900 expiry : batch_delete : false delay : 0 delete_storage : true sleep_time : 15 workers : 20 ingester : cache_dtl : 2 default_max_extracted : 100 default_max_supplementary : 100 default_resubmit_services : [] default_services : [] default_user : internal description_prefix : Bulk expire_after : 1296000 get_whitelist_verdict : assemblyline.common.signaturing.drop incomplete_expire_after_seconds : 3600 incomplete_stale_after_seconds : 1800 is_low_priority : assemblyline.common.null.always_false max_inflight : 500 sampling_at : critical : 500000 high : 1000000 low : 10000000 medium : 2000000 stale_after_seconds : 86400 whitelist : assemblyline.common.null.whitelist metrics : apm_server : server_url : null token : null elasticsearch : cold : 30 delete : 90 host_certificates : null hosts : null unit : d warm : 2 export_interval : 5 redis : &id001 host : 127.0.0.1 port : 6379 redis : nonpersistent : *id001 persistent : host : 127.0.0.1 port : 6380 scaler : service_defaults : backlog : 100 environment : - name : SERVICE_API_HOST value : http://service-server:5003 - name : AL_SERVICE_TASK_LIMIT value : inf growth : 60 min_instances : 0 shrink : 30 datasources : al : classpath : assemblyline.datasource.al.AL config : {} alert : classpath : assemblyline.datasource.alert.Alert config : {} datastore : hosts : - http://elastic:devpass@localhost ilm : days_until_archive : 15 enabled : false indexes : alert : &id002 cold : 15 delete : 30 unit : d warm : 5 error : *id002 file : *id002 result : *id002 submission : *id002 update_archive : false type : elasticsearch filestore : cache : - s3://al_storage_key:Ch@ngeTh!sPa33w0rd@localhost:9000?s3_bucket=al-cache&use_ssl=False storage : - s3://al_storage_key:Ch@ngeTh!sPa33w0rd@localhost:9000?s3_bucket=al-storage&use_ssl=False logging : export_interval : 5 heartbeat_file : /tmp/heartbeat log_as_json : true log_directory : /var/log/assemblyline/ log_level : INFO log_to_console : true log_to_file : false log_to_syslog : false syslog_host : localhost syslog_port : 514 services : allow_insecure_registry : false categories : - Antivirus - Dynamic Analysis - External - Extraction - Filtering - Networking - Static Analysis cpu_reservation : 0.25 default_timeout : 60 image_variables : {} min_service_workers : 0 preferred_update_channel : stable stages : - FILTER - EXTRACT - CORE - SECONDARY - POST submission : default_max_extracted : 500 default_max_supplementary : 500 dtl : 30 max_dtl : 0 max_extraction_depth : 6 max_file_size : 104857600 max_metadata_length : 4096 tag_types : attribution : - attribution.actor - attribution.campaign - attribution.exploit - attribution.implant - attribution.family - attribution.network - av.virus_name - file.config - technique.obfuscation behavior : - file.behavior ioc : - network.email.address - network.static.ip - network.static.domain - network.static.uri - network.dynamic.ip - network.dynamic.domain - network.dynamic.uri system : constants : assemblyline.common.constants organisation : ACME type : production ui : allow_malicious_hinting : false allow_raw_downloads : true allow_url_submissions : true audit : true banner : null banner_level : info debug : false download_encoding : cart email : null enforce_quota : true fqdn : localhost ingest_max_priority : 250 read_only : false read_only_offset : '' secret_key : This is the default flask secret key... you should change this! session_duration : 3600 statistics : alert : - al.attrib - al.av - al.behavior - al.domain - al.ip - al.yara - file.name - file.md5 - owner submission : - params.submitter tos : null tos_lockout : false tos_lockout_notify : null url_submission_headers : {} url_submission_proxies : {} validate_session_ip : true validate_session_useragent : true","title":"Specification and defaults"},{"location":"installation/configuration/config_file/#layers-of-the-configuration-file","text":"The configuration file is built in layers: The ODM converts the python classes to the default values as shown above The default assemblyline helm chart values.yaml file changes certain of these values to adapt them to a Kubernetes deployment Your deployment's values.yaml file change the values to their final form","title":"Layers of the configuration file"},{"location":"installation/configuration/config_file/#changing-the-configuration-file","text":"If you want to change the config.yml file that will be deployed in the containers, it will have to be done through the configuration section found in the values.yml file of your deployment. Example Let's say that you would want to change the log level in the system to ERROR an up. First of you would edit the values.yaml file of your personal deployment to add the changes to the configuration section: ... configuration : logging : log_level : ERROR ... Then you would simply deploy that new values.yaml file using the helm upgrade command specific to your deployment: Cluster deployment update Appliance deployment update","title":"Changing the configuration file"},{"location":"installation/configuration/config_file/#exhaustive-configuration-file-documentation","text":"All parameters of each configuration section will be thoroughly documented in their respective pages. Here are the links to the different section documentations: Authentication (auth:) Core components (core:) Data sources (datasources:) Database (datastore:) File storage (filestore:) Logging (logging:) Services (services:) Submission (submission:) System (system:) User Interface (ui:)","title":"Exhaustive configuration file documentation"},{"location":"installation/configuration/core/","text":"Core component section \u00b6 The core components configuration section ( core: ) of the configuration file contains all the different parameters that you can change to modify the behavior of each core components. Default values for the core section ... core : alerter : alert_ttl : 90 constant_alert_fields : - alert_id - file - ts default_group_field : file.sha256 delay : 300 filtering_group_fields : - file.name - status - priority non_filtering_group_fields : - file.md5 - file.sha1 - file.sha256 process_alert_message : assemblyline_core.alerter.processing.process_alert_message dispatcher : max_inflight : 1000 timeout : 900 expiry : batch_delete : false delay : 0 delete_storage : true sleep_time : 15 workers : 20 ingester : cache_dtl : 2 default_max_extracted : 100 default_max_supplementary : 100 default_resubmit_services : [] default_services : [] default_user : internal description_prefix : Bulk expire_after : 1296000 get_whitelist_verdict : assemblyline.common.signaturing.drop incomplete_expire_after_seconds : 3600 incomplete_stale_after_seconds : 1800 is_low_priority : assemblyline.common.null.always_false max_inflight : 500 sampling_at : critical : 500000 high : 1000000 low : 10000000 medium : 2000000 stale_after_seconds : 86400 whitelist : assemblyline.common.null.whitelist metrics : apm_server : server_url : null token : null elasticsearch : cold : 30 delete : 90 host_certificates : null hosts : null unit : d warm : 2 export_interval : 5 redis : &id001 host : 127.0.0.1 port : 6379 redis : nonpersistent : *id001 persistent : host : 127.0.0.1 port : 6380 scaler : service_defaults : backlog : 100 environment : - name : SERVICE_API_HOST value : http://service-server:5003 - name : AL_SERVICE_TASK_LIMIT value : inf growth : 60 min_instances : 0 shrink : 30 ... Tip Refer to the changing the configuration file documentation for more detail on where and how to change the configuration of the system. Alerter \u00b6 The configuration block at core.alerter contains all the configuration parameters that the Assemblyline alerter component can take. Here is an example configuration block with inline comments about the purpose of every single parameters: Alerter configuration example core : alerter : # Time to live in days for alerts in the system alert_ttl : 90 # List of fields that should keep the same value in between normal and extended scan # NOTE: You should not have to change those ever, this might get removed in the future constant_alert_fields : - alert_id - file - ts # Default field to group alerts with in the UI default_group_field : file.sha256 # Delay applied to the alert UI to leave time to the extended scan to complete delay : 300 # List of fields allowed to be used for grouping that are present in every single alerts filtering_group_fields : - file.name - status - priority # List of fields allowed to be used for grouping that are present only in a subset of alerts non_filtering_group_fields : - file.md5 - file.sha1 - file.sha256 # Python class used to process alert messages # NOTE: This should not be changed unless you've built your own container with # extra packages with alert processing capability process_alert_message : assemblyline_core.alerter.processing.process_alert_message Dispatcher \u00b6 The configuration block at core.dispatcher contains all the configuration parameters that the Assemblyline dispatcher component can take. Here is an example configuration block with inline comments about the purpose of every single parameters: Dispatcher configuration example core : dispatcher : # Maximum amount of concurrent submission processing in the system max_inflight : 1000 # Use in earlier version of dispatcher (To be removed) timeout : 900 Expiry \u00b6 The configuration block at core.expiry contains all the configuration parameters that the Assemblyline expiry component can take. Here is an example configuration block with inline comments about the purpose of every single parameters: Expiry configuration example core : expiry : # Perform delete operation in batch when changing day instead of throughout the day # NOTE: Not deleting during the day will make the system more responsive during the day # but will significantly reduce performance when changing to a new day until # all delete operations are done. batch_delete : false # Delay in hours applied to the deletion schedule delay : 0 # Should data expiry operation get rid of files as well? delete_storage : true # Time to sleep (sec) in between runs when there is not data to delete sleep_time : 15 # Number of workers used to delete data workers : 20 Ingester \u00b6 The configuration block at core.ingester contains all the configuration parameters that the Assemblyline ingester component can take. Here is an example configuration block with inline comments about the purpose of every single parameters: Ingester configuration example core : ingester : # Number of days ingested files are valid in the ingester cache cache_dtl : 2 # Maximum number of extracted files for ingested submissions default_max_extracted : 100 # Maximum number of supplementary files for ingested submissions default_max_supplementary : 100 # UNUSED default_resubmit_services : [] default_services : [] default_user : internal description_prefix : Bulk # Seconds before a previously ingested file will be considered new again, # the file will then be processed as if never seen before. expire_after : 1296000 # Function import path for method to determine whitelisting. # Files selected by this function will be dropped. # See the default function for signature. get_whitelist_verdict : assemblyline.common.signaturing.drop # Special version of 'expire_after' applied when the previous run of a file had errors. incomplete_expire_after_seconds : 3600 # Special version of 'stale_after_seconds' applied when the previous run of a file had errors. incomplete_stale_after_seconds : 1800 # Function import path for method to determine low-priority filter. # Files selected by this function will be forced to low-priority. # See the default function for signature. is_low_priority : assemblyline.common.null.always_false # How many submissions should ingester try to submit concurrently. max_inflight : 500 # How long should X queue be before the sampling (randomly dropping files) starts. # At the given value sampling will start, and grow gradually more agressive until 3 times # the value given. At 3 times the value given the system will proccess as many files # as possible, and all others will be discarded. sampling_at : critical : 500000 high : 1000000 low : 10000000 medium : 2000000 # Seconds before a previously ingested file will be considered stale, # the file will be reprocessed but the priority will be modified depending # on the score from the previous run. stale_after_seconds : 86400 # File whitelist import path. Imported object will be passed to the 'get_whitelist_verdict' # function. The default verdict function will use this as a file whitelist. # See default value for sample. whitelist : assemblyline.common.null.whitelist Metrics \u00b6 The configuration block at core.metrics contains all the configuration parameters that the Assemblyline metrics gathering components can take. Here is an example configuration block with inline comments about the purpose of every single parameters: Metrics configuration example core : metrics : # APM specific settings apm_server : # URL to the APM server server_url : null # Token use to connect to the APM server token : null # Elasticsearch specific configuration elasticsearch : # Number of `unit` document spend time in ILM cold storage cold : 30 # Number of `unit` after which documents are deleted using ILM delete : 90 # Cert used to connect to Elastic host_certificates : null # List of Elatic hosts hosts : null # Time unit for ILM cold, warm and delete unit : d # Number of `unit` document spend time in ILM warm storage warm : 2 # Interval at which the metrics components export their data export_interval : 5 # Redis specific configuration redis : # Host of the redis server host : 127.0.0.1 # Port of the redis server port : 6379 Redis \u00b6 The configuration block at core.redis contains all the configuration parameters used by Assemblyline components to connect to redis. Here is an example configuration block with inline comments about the purpose of every single parameters: Redis configuration example core : redis : # Configuration of the non-persistent redis (use mainly for messaging) nonpersistent : # Host of the redis server host : 127.0.0.1 # Port of the redis server port : 6379 # Configuration of the persistent redis (use mainly for task queuing) persistent : # Host of the redis server host : 127.0.0.1 # Port of the redis server port : 6380 Scaler \u00b6 The configuration block at core.scaler contains all the configuration parameters that the Assemblyline scaler component can take. Here is an example configuration block with inline comments about the purpose of every single parameters: Scaler configuration example core : scaler : service_defaults : # How many files in a service queue are considered a backlog. # This weights how important scaling up a service is relative # to its queue. You probably don't want to change this. backlog : 100 # List of environment variables set for all services. # Usually used for deployment related environment variables. # Service specific environent variables can be set in the # service manifest or in the UI for a particular system. environment : - name : SERVICE_API_HOST value : http://service-server:5003 - name : AL_SERVICE_TASK_LIMIT value : inf # Roughly how many seconds to wait before a service # scales up to meet increased demand. growth : 60 # The minimum number of instances of every service that # should be kept running at all times. min_instances : 0 # Roughly how many seconds to wait before a service scales down when # instances are consistently idle. shrink : 30","title":"Core component section"},{"location":"installation/configuration/core/#core-component-section","text":"The core components configuration section ( core: ) of the configuration file contains all the different parameters that you can change to modify the behavior of each core components. Default values for the core section ... core : alerter : alert_ttl : 90 constant_alert_fields : - alert_id - file - ts default_group_field : file.sha256 delay : 300 filtering_group_fields : - file.name - status - priority non_filtering_group_fields : - file.md5 - file.sha1 - file.sha256 process_alert_message : assemblyline_core.alerter.processing.process_alert_message dispatcher : max_inflight : 1000 timeout : 900 expiry : batch_delete : false delay : 0 delete_storage : true sleep_time : 15 workers : 20 ingester : cache_dtl : 2 default_max_extracted : 100 default_max_supplementary : 100 default_resubmit_services : [] default_services : [] default_user : internal description_prefix : Bulk expire_after : 1296000 get_whitelist_verdict : assemblyline.common.signaturing.drop incomplete_expire_after_seconds : 3600 incomplete_stale_after_seconds : 1800 is_low_priority : assemblyline.common.null.always_false max_inflight : 500 sampling_at : critical : 500000 high : 1000000 low : 10000000 medium : 2000000 stale_after_seconds : 86400 whitelist : assemblyline.common.null.whitelist metrics : apm_server : server_url : null token : null elasticsearch : cold : 30 delete : 90 host_certificates : null hosts : null unit : d warm : 2 export_interval : 5 redis : &id001 host : 127.0.0.1 port : 6379 redis : nonpersistent : *id001 persistent : host : 127.0.0.1 port : 6380 scaler : service_defaults : backlog : 100 environment : - name : SERVICE_API_HOST value : http://service-server:5003 - name : AL_SERVICE_TASK_LIMIT value : inf growth : 60 min_instances : 0 shrink : 30 ... Tip Refer to the changing the configuration file documentation for more detail on where and how to change the configuration of the system.","title":"Core component section"},{"location":"installation/configuration/core/#alerter","text":"The configuration block at core.alerter contains all the configuration parameters that the Assemblyline alerter component can take. Here is an example configuration block with inline comments about the purpose of every single parameters: Alerter configuration example core : alerter : # Time to live in days for alerts in the system alert_ttl : 90 # List of fields that should keep the same value in between normal and extended scan # NOTE: You should not have to change those ever, this might get removed in the future constant_alert_fields : - alert_id - file - ts # Default field to group alerts with in the UI default_group_field : file.sha256 # Delay applied to the alert UI to leave time to the extended scan to complete delay : 300 # List of fields allowed to be used for grouping that are present in every single alerts filtering_group_fields : - file.name - status - priority # List of fields allowed to be used for grouping that are present only in a subset of alerts non_filtering_group_fields : - file.md5 - file.sha1 - file.sha256 # Python class used to process alert messages # NOTE: This should not be changed unless you've built your own container with # extra packages with alert processing capability process_alert_message : assemblyline_core.alerter.processing.process_alert_message","title":"Alerter"},{"location":"installation/configuration/core/#dispatcher","text":"The configuration block at core.dispatcher contains all the configuration parameters that the Assemblyline dispatcher component can take. Here is an example configuration block with inline comments about the purpose of every single parameters: Dispatcher configuration example core : dispatcher : # Maximum amount of concurrent submission processing in the system max_inflight : 1000 # Use in earlier version of dispatcher (To be removed) timeout : 900","title":"Dispatcher"},{"location":"installation/configuration/core/#expiry","text":"The configuration block at core.expiry contains all the configuration parameters that the Assemblyline expiry component can take. Here is an example configuration block with inline comments about the purpose of every single parameters: Expiry configuration example core : expiry : # Perform delete operation in batch when changing day instead of throughout the day # NOTE: Not deleting during the day will make the system more responsive during the day # but will significantly reduce performance when changing to a new day until # all delete operations are done. batch_delete : false # Delay in hours applied to the deletion schedule delay : 0 # Should data expiry operation get rid of files as well? delete_storage : true # Time to sleep (sec) in between runs when there is not data to delete sleep_time : 15 # Number of workers used to delete data workers : 20","title":"Expiry"},{"location":"installation/configuration/core/#ingester","text":"The configuration block at core.ingester contains all the configuration parameters that the Assemblyline ingester component can take. Here is an example configuration block with inline comments about the purpose of every single parameters: Ingester configuration example core : ingester : # Number of days ingested files are valid in the ingester cache cache_dtl : 2 # Maximum number of extracted files for ingested submissions default_max_extracted : 100 # Maximum number of supplementary files for ingested submissions default_max_supplementary : 100 # UNUSED default_resubmit_services : [] default_services : [] default_user : internal description_prefix : Bulk # Seconds before a previously ingested file will be considered new again, # the file will then be processed as if never seen before. expire_after : 1296000 # Function import path for method to determine whitelisting. # Files selected by this function will be dropped. # See the default function for signature. get_whitelist_verdict : assemblyline.common.signaturing.drop # Special version of 'expire_after' applied when the previous run of a file had errors. incomplete_expire_after_seconds : 3600 # Special version of 'stale_after_seconds' applied when the previous run of a file had errors. incomplete_stale_after_seconds : 1800 # Function import path for method to determine low-priority filter. # Files selected by this function will be forced to low-priority. # See the default function for signature. is_low_priority : assemblyline.common.null.always_false # How many submissions should ingester try to submit concurrently. max_inflight : 500 # How long should X queue be before the sampling (randomly dropping files) starts. # At the given value sampling will start, and grow gradually more agressive until 3 times # the value given. At 3 times the value given the system will proccess as many files # as possible, and all others will be discarded. sampling_at : critical : 500000 high : 1000000 low : 10000000 medium : 2000000 # Seconds before a previously ingested file will be considered stale, # the file will be reprocessed but the priority will be modified depending # on the score from the previous run. stale_after_seconds : 86400 # File whitelist import path. Imported object will be passed to the 'get_whitelist_verdict' # function. The default verdict function will use this as a file whitelist. # See default value for sample. whitelist : assemblyline.common.null.whitelist","title":"Ingester"},{"location":"installation/configuration/core/#metrics","text":"The configuration block at core.metrics contains all the configuration parameters that the Assemblyline metrics gathering components can take. Here is an example configuration block with inline comments about the purpose of every single parameters: Metrics configuration example core : metrics : # APM specific settings apm_server : # URL to the APM server server_url : null # Token use to connect to the APM server token : null # Elasticsearch specific configuration elasticsearch : # Number of `unit` document spend time in ILM cold storage cold : 30 # Number of `unit` after which documents are deleted using ILM delete : 90 # Cert used to connect to Elastic host_certificates : null # List of Elatic hosts hosts : null # Time unit for ILM cold, warm and delete unit : d # Number of `unit` document spend time in ILM warm storage warm : 2 # Interval at which the metrics components export their data export_interval : 5 # Redis specific configuration redis : # Host of the redis server host : 127.0.0.1 # Port of the redis server port : 6379","title":"Metrics"},{"location":"installation/configuration/core/#redis","text":"The configuration block at core.redis contains all the configuration parameters used by Assemblyline components to connect to redis. Here is an example configuration block with inline comments about the purpose of every single parameters: Redis configuration example core : redis : # Configuration of the non-persistent redis (use mainly for messaging) nonpersistent : # Host of the redis server host : 127.0.0.1 # Port of the redis server port : 6379 # Configuration of the persistent redis (use mainly for task queuing) persistent : # Host of the redis server host : 127.0.0.1 # Port of the redis server port : 6380","title":"Redis"},{"location":"installation/configuration/core/#scaler","text":"The configuration block at core.scaler contains all the configuration parameters that the Assemblyline scaler component can take. Here is an example configuration block with inline comments about the purpose of every single parameters: Scaler configuration example core : scaler : service_defaults : # How many files in a service queue are considered a backlog. # This weights how important scaling up a service is relative # to its queue. You probably don't want to change this. backlog : 100 # List of environment variables set for all services. # Usually used for deployment related environment variables. # Service specific environent variables can be set in the # service manifest or in the UI for a particular system. environment : - name : SERVICE_API_HOST value : http://service-server:5003 - name : AL_SERVICE_TASK_LIMIT value : inf # Roughly how many seconds to wait before a service # scales up to meet increased demand. growth : 60 # The minimum number of instances of every service that # should be kept running at all times. min_instances : 0 # Roughly how many seconds to wait before a service scales down when # instances are consistently idle. shrink : 30","title":"Scaler"},{"location":"installation/configuration/datasources/","text":"Data sources section \u00b6 The Data Sources configuration section ( datasources: ) of the configuration file contains all the different parameters that you can change to add/modify data sources used by the hash search API. Since this section is quite simple, we will list the default configuration at the same time as we describe the different values. Datasources section configuration example ... # The data source section is essentially a key/value pair of source and their configuration datasources : # Source name (al) al : # Path to the module that will process the query received by the hash search API classpath : assemblyline.datasource.al.AL # Dictionary holding the configuration for the module config : {} # Source name (alert) alert : # Path to the module that will process the query received by the hash search API classpath : assemblyline.datasource.alert.Alert # Dictionary holding the configuration for the module config : {} ... Tip Refer to the changing the configuration file documentation for more detail on where and how to change the configuration of the system.","title":"Data sources section"},{"location":"installation/configuration/datasources/#data-sources-section","text":"The Data Sources configuration section ( datasources: ) of the configuration file contains all the different parameters that you can change to add/modify data sources used by the hash search API. Since this section is quite simple, we will list the default configuration at the same time as we describe the different values. Datasources section configuration example ... # The data source section is essentially a key/value pair of source and their configuration datasources : # Source name (al) al : # Path to the module that will process the query received by the hash search API classpath : assemblyline.datasource.al.AL # Dictionary holding the configuration for the module config : {} # Source name (alert) alert : # Path to the module that will process the query received by the hash search API classpath : assemblyline.datasource.alert.Alert # Dictionary holding the configuration for the module config : {} ... Tip Refer to the changing the configuration file documentation for more detail on where and how to change the configuration of the system.","title":"Data sources section"},{"location":"installation/configuration/datastore/","text":"Database section \u00b6 The Database configuration section ( datastore: ) of the configuration file contains all the different parameters that you can change modify how to connect to the database and to modify the Index Lifecycle Management (ILM) parameters. Since this section is quite simple, we will list the default configuration at the same time as we describe the different values. Datastore section configuration example ... datastore : # List of elastic hosts to connect to hosts : - http://elastic:devpass@localhost # Index Lifecycle management configuration block ilm : # After how many days do documents go in the ILM managed indexes days_until_archive : 15 # Is ILM enabled or not? enabled : false # Index specific ILM configuration indexes : alert : # After how many `unit` documents goes in cold storage cold : 15 # After how many `unit` documents are deleted delete : 30 # Time unit definition for the current index unit : d # After how many `unit` documents goes in warm storage warm : 5 error : cold : 15 delete : 30 unit : d warm : 5 file : cold : 15 delete : 30 unit : d warm : 5 result : cold : 15 delete : 30 unit : d warm : 5 submission : cold : 15 delete : 30 unit : d warm : 5 # Show saving new document update it's archive counterpart # NOTE: Setting this to false makes it faster but it will be possible to have # duplicate documents update_archive : false # Type of datastore (only elasticsearch is supported so far, do not change) type : elasticsearch ... Tip Refer to the changing the configuration file documentation for more detail on where and how to change the configuration of the system.","title":"Database section"},{"location":"installation/configuration/datastore/#database-section","text":"The Database configuration section ( datastore: ) of the configuration file contains all the different parameters that you can change modify how to connect to the database and to modify the Index Lifecycle Management (ILM) parameters. Since this section is quite simple, we will list the default configuration at the same time as we describe the different values. Datastore section configuration example ... datastore : # List of elastic hosts to connect to hosts : - http://elastic:devpass@localhost # Index Lifecycle management configuration block ilm : # After how many days do documents go in the ILM managed indexes days_until_archive : 15 # Is ILM enabled or not? enabled : false # Index specific ILM configuration indexes : alert : # After how many `unit` documents goes in cold storage cold : 15 # After how many `unit` documents are deleted delete : 30 # Time unit definition for the current index unit : d # After how many `unit` documents goes in warm storage warm : 5 error : cold : 15 delete : 30 unit : d warm : 5 file : cold : 15 delete : 30 unit : d warm : 5 result : cold : 15 delete : 30 unit : d warm : 5 submission : cold : 15 delete : 30 unit : d warm : 5 # Show saving new document update it's archive counterpart # NOTE: Setting this to false makes it faster but it will be possible to have # duplicate documents update_archive : false # Type of datastore (only elasticsearch is supported so far, do not change) type : elasticsearch ... Tip Refer to the changing the configuration file documentation for more detail on where and how to change the configuration of the system.","title":"Database section"},{"location":"installation/configuration/filestore/","text":"File storage section \u00b6 The file storage configuration section ( filestore: ) of the configuration file contains URLs to the different filestores and cachestore used by Assemblyline. Since this section is quite simple, we will list the default configuration at the same time as we describe the different values. Filestore section configuration example ... # Assemblyline uses a multistage file storing system. When multiple filestores are defined for # a single type, Assemblyline will save to all levels at once when adding files but when # retrieving file will try one level at the time in order until it finds the file. # # This allows you to have different retention schedule on the different levels and have faster # filestore store only files that are currently scanning in the system but slower ones to keep # more files but to look them up less often. filestore : # List of URLs to connect to the cache filestore cache : - s3://al_storage_key:Ch@ngeTh!sPa33w0rd@localhost:9000?s3_bucket=al-cache&use_ssl=False # List of URLs to connect to the data filestore storage : - s3://al_storage_key:Ch@ngeTh!sPa33w0rd@localhost:9000?s3_bucket=al-storage&use_ssl=False ... Tip Refer to the changing the configuration file documentation for more detail on where and how to change the configuration of the system.","title":"File storage section"},{"location":"installation/configuration/filestore/#file-storage-section","text":"The file storage configuration section ( filestore: ) of the configuration file contains URLs to the different filestores and cachestore used by Assemblyline. Since this section is quite simple, we will list the default configuration at the same time as we describe the different values. Filestore section configuration example ... # Assemblyline uses a multistage file storing system. When multiple filestores are defined for # a single type, Assemblyline will save to all levels at once when adding files but when # retrieving file will try one level at the time in order until it finds the file. # # This allows you to have different retention schedule on the different levels and have faster # filestore store only files that are currently scanning in the system but slower ones to keep # more files but to look them up less often. filestore : # List of URLs to connect to the cache filestore cache : - s3://al_storage_key:Ch@ngeTh!sPa33w0rd@localhost:9000?s3_bucket=al-cache&use_ssl=False # List of URLs to connect to the data filestore storage : - s3://al_storage_key:Ch@ngeTh!sPa33w0rd@localhost:9000?s3_bucket=al-storage&use_ssl=False ... Tip Refer to the changing the configuration file documentation for more detail on where and how to change the configuration of the system.","title":"File storage section"},{"location":"installation/configuration/logging/","text":"Logging section \u00b6 The logging configuration section ( logging: ) of the configuration file allows you to modify the log level and where the logs will be shipped in the system . Since this section is quite simple, we will list the default configuration at the same time as we describe the different values. Logging section configuration example ... logging : # Interval at which the container heartbeat is written export_interval : 5 # Location of the container heartbeat heartbeat_file : /tmp/heartbeat # Should logs use a JSON format # (mainly used to parse logs into kibana, otherwise set to false to make them readable) log_as_json : true # Location on disk where the logs are stored if log_to_file enabled log_directory : /var/log/assemblyline/ # Minimum log level # (DEBUG, INFO, WARNING, ERROR) log_level : INFO # Should logs be shown in the console? # You should have that to true if running inside containers log_to_console : true # Should you write logs to files? # Set this to false when running inside a container log_to_file : false # Should you send logs to a syslog server? log_to_syslog : false # Host of the syslog server syslog_host : localhost # Port of the syslog server syslog_port : 514 ... Tip Refer to the changing the configuration file documentation for more detail on where and how to change the configuration of the system.","title":"Logging section"},{"location":"installation/configuration/logging/#logging-section","text":"The logging configuration section ( logging: ) of the configuration file allows you to modify the log level and where the logs will be shipped in the system . Since this section is quite simple, we will list the default configuration at the same time as we describe the different values. Logging section configuration example ... logging : # Interval at which the container heartbeat is written export_interval : 5 # Location of the container heartbeat heartbeat_file : /tmp/heartbeat # Should logs use a JSON format # (mainly used to parse logs into kibana, otherwise set to false to make them readable) log_as_json : true # Location on disk where the logs are stored if log_to_file enabled log_directory : /var/log/assemblyline/ # Minimum log level # (DEBUG, INFO, WARNING, ERROR) log_level : INFO # Should logs be shown in the console? # You should have that to true if running inside containers log_to_console : true # Should you write logs to files? # Set this to false when running inside a container log_to_file : false # Should you send logs to a syslog server? log_to_syslog : false # Host of the syslog server syslog_host : localhost # Port of the syslog server syslog_port : 514 ... Tip Refer to the changing the configuration file documentation for more detail on where and how to change the configuration of the system.","title":"Logging section"},{"location":"installation/configuration/services/","text":"Service section \u00b6 The service configuration section ( services: ) of the configuration file allows you to modify the parameters on how services are executed in the system. Since this section is quite simple, we will list the default configuration at the same time as we describe the different values. Service section configuration example ... services : # Are we allowed to pull service containers from insecure registries allow_insecure_registry : false # List of valid categories for services # You should not change this except to add a category maybe? categories : - Antivirus - Dynamic Analysis - External - Extraction - Filtering - Networking - Static Analysis # Percentage of CPU resevation scaler will do for each service (1 = 100%) cpu_reservation : 0.25 # Default service execution timeout default_timeout : 60 # Set of environment varables applied to the service containers # while loaded from updater or scaler image_variables : {} # Minimum amount of service that will be loaded for each service min_service_workers : 0 # Type of services the updater will be looking for when looking for service update # (dev or stable) preferred_update_channel : stable # List of available stages : - FILTER - EXTRACT - CORE - SECONDARY - POST ... Tip Refer to the changing the configuration file documentation for more detail on where and how to change the configuration of the system.","title":"Service section"},{"location":"installation/configuration/services/#service-section","text":"The service configuration section ( services: ) of the configuration file allows you to modify the parameters on how services are executed in the system. Since this section is quite simple, we will list the default configuration at the same time as we describe the different values. Service section configuration example ... services : # Are we allowed to pull service containers from insecure registries allow_insecure_registry : false # List of valid categories for services # You should not change this except to add a category maybe? categories : - Antivirus - Dynamic Analysis - External - Extraction - Filtering - Networking - Static Analysis # Percentage of CPU resevation scaler will do for each service (1 = 100%) cpu_reservation : 0.25 # Default service execution timeout default_timeout : 60 # Set of environment varables applied to the service containers # while loaded from updater or scaler image_variables : {} # Minimum amount of service that will be loaded for each service min_service_workers : 0 # Type of services the updater will be looking for when looking for service update # (dev or stable) preferred_update_channel : stable # List of available stages : - FILTER - EXTRACT - CORE - SECONDARY - POST ... Tip Refer to the changing the configuration file documentation for more detail on where and how to change the configuration of the system.","title":"Service section"},{"location":"installation/configuration/submission/","text":"Submission section \u00b6 The submission configuration section ( submission: ) of the configuration file allows you to modify the parameters off how submissions are handled in the system. Since this section is quite simple, we will list the default configuration at the same time as we describe the different values. Submission section configuration example ... submission : # Maximum amount of extracted files for a submission default_max_extracted : 500 # Maximum amount of supplementary files for a submission default_max_supplementary : 500 # Default amount of days submissions live in the system dtl : 30 # Maximum amount of days submissions live in the system max_dtl : 0 # Maximum extraction depth service can go max_extraction_depth : 6 # Maximum file size allowed in the system max_file_size : 104857600 # Maximum size of each metadata entry max_metadata_length : 4096 # Types of tags to be included in the submission summary in # the attribution, behavior and ioc sectiona. tag_types : attribution : - attribution.actor - attribution.campaign - attribution.exploit - attribution.implant - attribution.family - attribution.network - av.virus_name - file.config - technique.obfuscation behavior : - file.behavior ioc : - network.email.address - network.static.ip - network.static.domain - network.static.uri - network.dynamic.ip - network.dynamic.domain - network.dynamic.uri ... Tip Refer to the changing the configuration file documentation for more detail on where and how to change the configuration of the system.","title":"Submission section"},{"location":"installation/configuration/submission/#submission-section","text":"The submission configuration section ( submission: ) of the configuration file allows you to modify the parameters off how submissions are handled in the system. Since this section is quite simple, we will list the default configuration at the same time as we describe the different values. Submission section configuration example ... submission : # Maximum amount of extracted files for a submission default_max_extracted : 500 # Maximum amount of supplementary files for a submission default_max_supplementary : 500 # Default amount of days submissions live in the system dtl : 30 # Maximum amount of days submissions live in the system max_dtl : 0 # Maximum extraction depth service can go max_extraction_depth : 6 # Maximum file size allowed in the system max_file_size : 104857600 # Maximum size of each metadata entry max_metadata_length : 4096 # Types of tags to be included in the submission summary in # the attribution, behavior and ioc sectiona. tag_types : attribution : - attribution.actor - attribution.campaign - attribution.exploit - attribution.implant - attribution.family - attribution.network - av.virus_name - file.config - technique.obfuscation behavior : - file.behavior ioc : - network.email.address - network.static.ip - network.static.domain - network.static.uri - network.dynamic.ip - network.dynamic.domain - network.dynamic.uri ... Tip Refer to the changing the configuration file documentation for more detail on where and how to change the configuration of the system.","title":"Submission section"},{"location":"installation/configuration/system/","text":"System section \u00b6 The system configuration section ( system: ) of the configuration file allows you to modify the parameters of your system. Since this section is quite simple, we will list the default configuration at the same time as we describe the different values. System section configuration example ... system : # Path path to the constants module constants : assemblyline.common.constants # Organisation Name organisation : ACME # Type of system. If different then production, watermark will be shown in the UI type : production ... Tip Refer to the changing the configuration file documentation for more detail on where and how to change the configuration of the system.","title":"System section"},{"location":"installation/configuration/system/#system-section","text":"The system configuration section ( system: ) of the configuration file allows you to modify the parameters of your system. Since this section is quite simple, we will list the default configuration at the same time as we describe the different values. System section configuration example ... system : # Path path to the constants module constants : assemblyline.common.constants # Organisation Name organisation : ACME # Type of system. If different then production, watermark will be shown in the UI type : production ... Tip Refer to the changing the configuration file documentation for more detail on where and how to change the configuration of the system.","title":"System section"},{"location":"installation/configuration/ui/","text":"User Interface section \u00b6 The user interface configuration section ( ui: ) of the configuration file allows you to modify the parameters of the user interface and API Server. Since this section is quite simple, we will list the default configuration at the same time as we describe the different values. User interface section configuration example ... ui : # Show the malicious hinting checkbox when submitting files allow_malicious_hinting : false # Allow malicious files to be download in raw format allow_raw_downloads : true # Allow URL submissions to be processed in the system allow_url_submissions : true # Audit API queries audit : true # String to be displayed in the banner banner : null # Color of the banner (info, success, error, warning) banner_level : info # Turn on/off debug mode debug : false # Default encoding for downloaded files download_encoding : cart # Email address users can reach the admins at email : null # Enforce API and submissions quotas or not enforce_quota : true # Domain for your deployment (Especially important for kubernetes deployments) fqdn : localhost # Maximum submission priority for ingestion tasks ingest_max_priority : 250 # Make the UI read only # (Not supported in the new UI yet) read_only : false # Time offset for queries done in raed only mode # (Not supported in the new UI yet) read_only_offset : '' # Secret key for your flask app (API) # You should definitely change this! secret_key : This is the default flask secret key... you should change this! # Timeout after which a stale session is no longer valid session_duration : 3600 # Fields to generate statistics on statistics : # Statistics in the alert view alert : - al.attrib - al.av - al.behavior - al.domain - al.ip - al.yara - file.name - file.md5 - owner # Statistics in the submission view submission : - params.submitter # Terms of service for the deployment (in markdown format) tos : null # Lockout the user after they agree to the terms of service # (requires an admin to enable their account) tos_lockout : false # List of email addresses to notify when a user agreed to the TOS # and its account is locked out tos_lockout_notify : null # Headers added to fetch the files during URL submissions url_submission_headers : {} # Proxy configuration to use while fetching the file during URL submissions url_submission_proxies : {} # Should we validate that the session comes from the same IP? validate_session_ip : true # Should we validate that the session uses the same user agent validate_session_useragent : true ... Tip Refer to the changing the configuration file documentation for more detail on where and how to change the configuration of the system.","title":"User Interface section"},{"location":"installation/configuration/ui/#user-interface-section","text":"The user interface configuration section ( ui: ) of the configuration file allows you to modify the parameters of the user interface and API Server. Since this section is quite simple, we will list the default configuration at the same time as we describe the different values. User interface section configuration example ... ui : # Show the malicious hinting checkbox when submitting files allow_malicious_hinting : false # Allow malicious files to be download in raw format allow_raw_downloads : true # Allow URL submissions to be processed in the system allow_url_submissions : true # Audit API queries audit : true # String to be displayed in the banner banner : null # Color of the banner (info, success, error, warning) banner_level : info # Turn on/off debug mode debug : false # Default encoding for downloaded files download_encoding : cart # Email address users can reach the admins at email : null # Enforce API and submissions quotas or not enforce_quota : true # Domain for your deployment (Especially important for kubernetes deployments) fqdn : localhost # Maximum submission priority for ingestion tasks ingest_max_priority : 250 # Make the UI read only # (Not supported in the new UI yet) read_only : false # Time offset for queries done in raed only mode # (Not supported in the new UI yet) read_only_offset : '' # Secret key for your flask app (API) # You should definitely change this! secret_key : This is the default flask secret key... you should change this! # Timeout after which a stale session is no longer valid session_duration : 3600 # Fields to generate statistics on statistics : # Statistics in the alert view alert : - al.attrib - al.av - al.behavior - al.domain - al.ip - al.yara - file.name - file.md5 - owner # Statistics in the submission view submission : - params.submitter # Terms of service for the deployment (in markdown format) tos : null # Lockout the user after they agree to the terms of service # (requires an admin to enable their account) tos_lockout : false # List of email addresses to notify when a user agreed to the TOS # and its account is locked out tos_lockout_notify : null # Headers added to fetch the files during URL submissions url_submission_headers : {} # Proxy configuration to use while fetching the file during URL submissions url_submission_proxies : {} # Should we validate that the session comes from the same IP? validate_session_ip : true # Should we validate that the session uses the same user agent validate_session_useragent : true ... Tip Refer to the changing the configuration file documentation for more detail on where and how to change the configuration of the system.","title":"User Interface section"},{"location":"integration/ingestion_method/","text":"Choosing your ingestion method \u00b6 While integrating Assemblyline with other systems, the first thing you will need to do is to pick an ingestion method. Assemblyline gives you two options: Asynchronous (Using the Ingest API: /api/v4/ingest/ ) Synchronous (Using the Submit API: /api/v4/submit/ ) We will give you here a rundown of the different particularities of each method so you can pick the one that fits your needs the best. Asynchronous ingestion \u00b6 This is the preferred ingestion method for use with Assemblyline. In this mode, Assemblyline will queue your submission based on priority and will process them when the services have empty processing cycles. For each submission in this mode, you will get assigned an ingestion ID and you can be notified via a completion queue when your file has completed scanning. Alternatively, you can use the alerting page in the Assemblyline UI if you want to only view asynchronous submissions that Assemblyline deems highly suspicious. The asynchronous model was built to sustain a very large sample set of files and to help analysts focus on what is important. Benefits and Drawbacks \u00b6 Benefits Support very large volume of files Not subjected to any quota Will resort to data sampling if it gets overwhelmed with too many files Allows for alerting perspective to be used Does submission level caching if the same file is submitted twice with the same parameters Drawbacks Submissions may sit in the queue a long time if the system is very busy Submissions may be skipped if the system is overwhelmed Metadata is not searchable for all submissions since the system does not create a submission entry for cache submissions Typical use cases \u00b6 Here are the typical use cases that users encounter while using the asynchronous submission mode in the system. Using the Ingest API while reading a message from the notification queue The user submits all its files and receives ingestion IDs for its files API: /api/v4/ingest/ The user asks the notification for messages until it receives a confirmation message for all its files API: /api/v4/ingest/get_message_list/ This is how this works in the backend: Using the Ingest API ignoring the notification queue but using the alert perspective The user submits all its files and ignores the returned ingestion IDs API: /api/v4/ingest/ The user then monitors the UI alerting perspective for newly created alerts UI: /alerts This is how this works in the backend: Synchronous ingestion \u00b6 In this mode, Assemblyline will start the scanning of your file right away and will return you the ID of your submission. You will be able to use this ID to ask the system if the submission is complete and to pull the results when all the services are done reporting results for that submission. This is more suited for a very small volume of files and manual analysis. Files submitted via the User interface are using the synchronous mode. Benefits and Drawbacks \u00b6 Benefits Instant scanning Higher priority than asynchronous Submission guaranteed to be processed Metadata searchable for all submissions Drawbacks Subjected to quota (Default: 5 concurrent submissions) Not suited for high load No submission level caching Alerting not available Typical use cases \u00b6 Here are the typical use cases that user's encounter while using the synchronous submission mode in the system. Using the Submit API waiting for the submission to be done The user sends its file for processing and receives an ID for its submission API: /api/v4/submit/ The user queries the is completed API until the system says the submission is completed API: /api/v4/submission/is_completed/ / The user pulls the results for the submission API: /api/v4/submission/full/ / This is how this works in the backend:","title":"Choosing your ingestion method"},{"location":"integration/ingestion_method/#choosing-your-ingestion-method","text":"While integrating Assemblyline with other systems, the first thing you will need to do is to pick an ingestion method. Assemblyline gives you two options: Asynchronous (Using the Ingest API: /api/v4/ingest/ ) Synchronous (Using the Submit API: /api/v4/submit/ ) We will give you here a rundown of the different particularities of each method so you can pick the one that fits your needs the best.","title":"Choosing your ingestion method"},{"location":"integration/ingestion_method/#asynchronous-ingestion","text":"This is the preferred ingestion method for use with Assemblyline. In this mode, Assemblyline will queue your submission based on priority and will process them when the services have empty processing cycles. For each submission in this mode, you will get assigned an ingestion ID and you can be notified via a completion queue when your file has completed scanning. Alternatively, you can use the alerting page in the Assemblyline UI if you want to only view asynchronous submissions that Assemblyline deems highly suspicious. The asynchronous model was built to sustain a very large sample set of files and to help analysts focus on what is important.","title":"Asynchronous ingestion"},{"location":"integration/ingestion_method/#benefits-and-drawbacks","text":"Benefits Support very large volume of files Not subjected to any quota Will resort to data sampling if it gets overwhelmed with too many files Allows for alerting perspective to be used Does submission level caching if the same file is submitted twice with the same parameters Drawbacks Submissions may sit in the queue a long time if the system is very busy Submissions may be skipped if the system is overwhelmed Metadata is not searchable for all submissions since the system does not create a submission entry for cache submissions","title":"Benefits and Drawbacks"},{"location":"integration/ingestion_method/#typical-use-cases","text":"Here are the typical use cases that users encounter while using the asynchronous submission mode in the system. Using the Ingest API while reading a message from the notification queue The user submits all its files and receives ingestion IDs for its files API: /api/v4/ingest/ The user asks the notification for messages until it receives a confirmation message for all its files API: /api/v4/ingest/get_message_list/ This is how this works in the backend: Using the Ingest API ignoring the notification queue but using the alert perspective The user submits all its files and ignores the returned ingestion IDs API: /api/v4/ingest/ The user then monitors the UI alerting perspective for newly created alerts UI: /alerts This is how this works in the backend:","title":"Typical use cases"},{"location":"integration/ingestion_method/#synchronous-ingestion","text":"In this mode, Assemblyline will start the scanning of your file right away and will return you the ID of your submission. You will be able to use this ID to ask the system if the submission is complete and to pull the results when all the services are done reporting results for that submission. This is more suited for a very small volume of files and manual analysis. Files submitted via the User interface are using the synchronous mode.","title":"Synchronous ingestion"},{"location":"integration/ingestion_method/#benefits-and-drawbacks_1","text":"Benefits Instant scanning Higher priority than asynchronous Submission guaranteed to be processed Metadata searchable for all submissions Drawbacks Subjected to quota (Default: 5 concurrent submissions) Not suited for high load No submission level caching Alerting not available","title":"Benefits and Drawbacks"},{"location":"integration/ingestion_method/#typical-use-cases_1","text":"Here are the typical use cases that user's encounter while using the synchronous submission mode in the system. Using the Submit API waiting for the submission to be done The user sends its file for processing and receives an ID for its submission API: /api/v4/submit/ The user queries the is completed API until the system says the submission is completed API: /api/v4/submission/is_completed/ / The user pulls the results for the submission API: /api/v4/submission/full/ / This is how this works in the backend:","title":"Typical use cases"},{"location":"integration/java/","text":"Java Client \u00b6 The assemblyline java client library provides methods to submit requests to assemblyline. Get the Java client Using the client \u00b6 To instantiate the client bean set the application properties associated with the desired authentication method. The client can be accessed by auto-wiring the bean into the class using it. There are two authentication methods: username/apikey or username/password. API Key Authentication \u00b6 To instantiate an API key authenticated assemblyline client, define the following properties: assemblyline-java-client: url: <assemblyline-instance-url> api-auth: apikey: <api-key> username: <username> Password Authentication \u00b6 To instantiate a password authenticated assemblyline client, define the following properties: assemblyline-java-client: url: <assemblyline-instance-url> password-auth: password: <password> username: <username> Proxy \u00b6 To go through a proxy, add the following properties: assemblyline-java-client: proxy: host: <host> port: <port>","title":"Java Client"},{"location":"integration/java/#java-client","text":"The assemblyline java client library provides methods to submit requests to assemblyline. Get the Java client","title":"Java Client"},{"location":"integration/java/#using-the-client","text":"To instantiate the client bean set the application properties associated with the desired authentication method. The client can be accessed by auto-wiring the bean into the class using it. There are two authentication methods: username/apikey or username/password.","title":"Using the client"},{"location":"integration/java/#api-key-authentication","text":"To instantiate an API key authenticated assemblyline client, define the following properties: assemblyline-java-client: url: <assemblyline-instance-url> api-auth: apikey: <api-key> username: <username>","title":"API Key Authentication"},{"location":"integration/java/#password-authentication","text":"To instantiate a password authenticated assemblyline client, define the following properties: assemblyline-java-client: url: <assemblyline-instance-url> password-auth: password: <password> username: <username>","title":"Password Authentication"},{"location":"integration/java/#proxy","text":"To go through a proxy, add the following properties: assemblyline-java-client: proxy: host: <host> port: <port>","title":"Proxy"},{"location":"integration/key_generation/","text":"Generating an API key \u00b6 While integrating Assemblyline to another system, you should not save your username and password into another app. Instead, you should create an API Key with only the appropriate requirements for that specific integration. Here is how to do this: Login to Assemblyline's user interface with the user that will perform API requests Click on your avatar in the top-right corner of the Assemblyline UI and select \"Manage Account\" Scroll down to the bottom to the \"Security\" section and select \"Manage API Keys\" Add the API Key name, select access privileges then click the \"Add\" button. The API KEY will only be displayed once and can't be recovered. Copy it somewhere safe so that you can use it later. Click the \"Done\" button.","title":"Generating an API key"},{"location":"integration/key_generation/#generating-an-api-key","text":"While integrating Assemblyline to another system, you should not save your username and password into another app. Instead, you should create an API Key with only the appropriate requirements for that specific integration. Here is how to do this: Login to Assemblyline's user interface with the user that will perform API requests Click on your avatar in the top-right corner of the Assemblyline UI and select \"Manage Account\" Scroll down to the bottom to the \"Security\" section and select \"Manage API Keys\" Add the API Key name, select access privileges then click the \"Add\" button. The API KEY will only be displayed once and can't be recovered. Copy it somewhere safe so that you can use it later. Click the \"Done\" button.","title":"Generating an API key"},{"location":"integration/python/","text":"Python Client \u00b6 The Assemblyline python client facilitates issuing requests to Assemblyline. Installing the client \u00b6 pip install assemblyline_client Connecting to Assemblyline \u00b6 You can instantiate the client by using the following snippet of Python code: When connecting to Assemblyline, you can also provide a certificate for SSL or ignore the certificate error al_client = get_client ( ... , verify = '/path/to/server.crt' ) al_client = get_client ( ... , verify = False ) API Key You will need an API key . from assemblyline_client import get_client al_client = get_client ( \"https://yourdomain:443\" , apikey = ( 'user' , 'key' )) User/Password from assemblyline_client import get_client al_client = get_client ( \"https://yourdomain:443\" , auth = ( 'user' , 'password' )) Certificate from assemblyline_client import get_client # and if your Assemblyline server is using a self-signed certificate al_client = get_client ( \"https://yourdomain:443\" , cert = '/path/to/cert/file.pem' ) The client is fully documented in the docstrings so that you can use the 'help' feature of IPython or Jupyter Notebook al_client . search . alert ? Signature : al_client . search . alert ( query , filters = None , fl = None , offset = 0 , rows = 25 , sort = None , timeout = None , ) Docstring : Search alerts with a Lucene query . Required : query : Lucene query . ( string ) Optional : filters : Additional Lucene queries used to filter the data ( list of strings ) fl : List of fields to return ( comma separated string of fields ) offset : Offset at which the query items should start ( integer ) rows : Number of records to return ( integer ) sort : Field used for sorting with direction ( string : ex . 'id desc' ) timeout : Max number of milliseconds the query will run ( integer ) Returns all results . File : / usr / local / lib / python3 .7 / site - packages / assemblyline_client / v4_client / module / search / __init__ . py Type : method Examples \u00b6 Submit a file or URL for analysis \u00b6 There are two methods for sending a file/URL to Assemblyline for analysis: Ingest and Submit . In most cases, you want to use the Ingest API via the CLI Ingest Provides a fast, non-blocking method of submitting many files Ingest results will typically be analyzed by using a callback (if you need to look at all results) or by monitoring the alerts Supports alert generation Submit High priority, low volume (5 concurrent submissions by default, this can be increased slightly in the user settings) You will need to wait for analysis to complete before submitting more Useful to support manual analysis (Optional) Customizing your submission Note: Service names are case-sensitive # Submission parameters (works for both Ingest and Submit) settings = { 'classification' : 'TLP:A' , # classification 'description' : 'Hello world' , # file description 'name' : 'filename' , # file name 'deep_scan' : False , # activate deep scan mode 'priority' : 1000 , # queue priority (the higher the number, the higher the priority) 'ignore_cache' : False , # ignore system cache 'services' : { 'selected' : [ # selected service list (override user profile) 'Cuckoo' , 'Extract' ], 'resubmit' : [], # resubmit to these services if file initially scores > 500 'excluded' : [], # exclude these services }, 'service_spec' : { # provide a service parameter 'Extract' : { 'password' : 'password' } } } # Adding metadata (such as the source of the files or anything you want!) my_meta = { 'my_metadata' : 'value' , # any metadata of your liking 'my_metadata2' : 'value2' # any metadata of your liking } You can find all parameters and their default values in the SubmissionParams class . For submitting a URL instead of a file, use the url argument instead of path Ingest The Ingest API supports three additional functionalities over the Submit API: The ingest API is for high throughput submission (feeding the system) By passing the argument alert=True , the system will generate an alert if the score is over 500 By passing the argument nq='notification_queue_name' , you can use the client to poll a notification queue for a message indicating if the analysis has completed If you don't need to know about when the analysis completes, then you can omit the nq argument and ignore the subsequent code that interacts with the notification queue ingest_id = al_client . ingest ( path = '/pathto/file.txt' , nq = 'my_queue_name' , params = settings , metadata = my_meta ) # If you use a notification queue you can get your asynchronous results with: from time import sleep message = None while True : message = al_client . ingest . get_message ( \"my_queue_name\" ) if message is None : sleep ( 1 ) # Poll every second else : do something ... Submit submit_results = al_client . submit ( path = '/pathto/file.txt' , fname = 'fname' , params = settings , metadata = my_meta ) Submission details \u00b6 To get the details about a submission, you simply need to pass the client a submission ID (sid) submission_details = al_client . submission ( \"4nxrpBePQDLH427aA8m3TZ\" ) Using search \u00b6 More details about Search You can use the search engine in the client by simply passing a Lucene query. In the following example, we want to retrieve the first page of submissions made by user : search_result = al_client . search . submission ( \"params.submitter:user\" ) Using search iterator \u00b6 Instead of using search and getting a page of results, you can use the search iterator stream to go through all the results. Streamed results only return indexed fields. If you want the full result, you have to go get it via the client for submission in al_client . search . stream . submission ( \"params.submitter:user\" ): submission_id = submission [ \"sid\" ] full_submission = al_client . submission ( submission_id ) Using search parameters \u00b6 In the following example, we want to retrieve the first page of submissions that were submitted in the last week, and we only want the submission IDs: submission_results = al_client . search . submission ( 'times.submitted:[now-7d TO now]' , fl = 'sid' ) Using facet searching \u00b6 In the following example, we want to retrieve the users who have made submissions in the last week, and the number of submissions that they have made: submission_results = al_client . search . facet . submission ( 'params.submitter' , query = 'times.submitted:[now-7d TO now]' ) Using the Command-line Tool \u00b6 By installing the assemblyline_client PIP package, a command-line tool al-submit is installed. In case you don't want to use Python code to interface with the Assemblyline client, you can use this tool instead. You can view the user options via al-submit --help . (Optional) Configuration file example Rather than passing authentication and server details as parameters in a command-line, you can use a configuration file. This configuration file should be placed at ~/.al/submit.cfg . A template for this configuration file can be found below. NOTE: You can use = or : as the delimiter between key and value. [ auth ] # Username for the Assemblyline account. user = # There are three methods to authenticate a user account. Choose one: # - Password Provided via User Prompt # Leave the `password' configuration value below empty. # - Password Provided in Configuration File # Enter the password for the Assemblyline account in plaintext. password = # - API Key in Configuration File # Enter the API key to use in plaintext for the user to login. # NOTE: The API key must have WRITE access for INGEST and WRITE+READ for SUBMIT. apikey = # Skip server cert validation. # Value can be one of: true, false, yes, no # If not supplied, the default value is: false insecure = [ server ] # Method of network transport. # If not supplied, the default value is: https transport = # Domain of Assemblyline instance. # If not supplied, the default value is: localhost host = # Port to which traffic will be sent. # If not supplied, the default value is: 443 port = # Server cert used to connect to server. cert = Mass Submission Toolkit \u00b6 The Assemblyline Incident Manager can assist you with this process. One key consideration for a very large volume of files in a burst is the default sampling values . You must keep your ingestion flow at a rate such that the size of the priority ingestion queue remains lower than the corresponding priority queue sampling_at values, otherwise, Assemblyline will skip files.","title":"Python Client"},{"location":"integration/python/#python-client","text":"The Assemblyline python client facilitates issuing requests to Assemblyline.","title":"Python Client"},{"location":"integration/python/#installing-the-client","text":"pip install assemblyline_client","title":"Installing the client"},{"location":"integration/python/#connecting-to-assemblyline","text":"You can instantiate the client by using the following snippet of Python code: When connecting to Assemblyline, you can also provide a certificate for SSL or ignore the certificate error al_client = get_client ( ... , verify = '/path/to/server.crt' ) al_client = get_client ( ... , verify = False ) API Key You will need an API key . from assemblyline_client import get_client al_client = get_client ( \"https://yourdomain:443\" , apikey = ( 'user' , 'key' )) User/Password from assemblyline_client import get_client al_client = get_client ( \"https://yourdomain:443\" , auth = ( 'user' , 'password' )) Certificate from assemblyline_client import get_client # and if your Assemblyline server is using a self-signed certificate al_client = get_client ( \"https://yourdomain:443\" , cert = '/path/to/cert/file.pem' ) The client is fully documented in the docstrings so that you can use the 'help' feature of IPython or Jupyter Notebook al_client . search . alert ? Signature : al_client . search . alert ( query , filters = None , fl = None , offset = 0 , rows = 25 , sort = None , timeout = None , ) Docstring : Search alerts with a Lucene query . Required : query : Lucene query . ( string ) Optional : filters : Additional Lucene queries used to filter the data ( list of strings ) fl : List of fields to return ( comma separated string of fields ) offset : Offset at which the query items should start ( integer ) rows : Number of records to return ( integer ) sort : Field used for sorting with direction ( string : ex . 'id desc' ) timeout : Max number of milliseconds the query will run ( integer ) Returns all results . File : / usr / local / lib / python3 .7 / site - packages / assemblyline_client / v4_client / module / search / __init__ . py Type : method","title":"Connecting to Assemblyline"},{"location":"integration/python/#examples","text":"","title":"Examples"},{"location":"integration/python/#submit-a-file-or-url-for-analysis","text":"There are two methods for sending a file/URL to Assemblyline for analysis: Ingest and Submit . In most cases, you want to use the Ingest API via the CLI Ingest Provides a fast, non-blocking method of submitting many files Ingest results will typically be analyzed by using a callback (if you need to look at all results) or by monitoring the alerts Supports alert generation Submit High priority, low volume (5 concurrent submissions by default, this can be increased slightly in the user settings) You will need to wait for analysis to complete before submitting more Useful to support manual analysis (Optional) Customizing your submission Note: Service names are case-sensitive # Submission parameters (works for both Ingest and Submit) settings = { 'classification' : 'TLP:A' , # classification 'description' : 'Hello world' , # file description 'name' : 'filename' , # file name 'deep_scan' : False , # activate deep scan mode 'priority' : 1000 , # queue priority (the higher the number, the higher the priority) 'ignore_cache' : False , # ignore system cache 'services' : { 'selected' : [ # selected service list (override user profile) 'Cuckoo' , 'Extract' ], 'resubmit' : [], # resubmit to these services if file initially scores > 500 'excluded' : [], # exclude these services }, 'service_spec' : { # provide a service parameter 'Extract' : { 'password' : 'password' } } } # Adding metadata (such as the source of the files or anything you want!) my_meta = { 'my_metadata' : 'value' , # any metadata of your liking 'my_metadata2' : 'value2' # any metadata of your liking } You can find all parameters and their default values in the SubmissionParams class . For submitting a URL instead of a file, use the url argument instead of path Ingest The Ingest API supports three additional functionalities over the Submit API: The ingest API is for high throughput submission (feeding the system) By passing the argument alert=True , the system will generate an alert if the score is over 500 By passing the argument nq='notification_queue_name' , you can use the client to poll a notification queue for a message indicating if the analysis has completed If you don't need to know about when the analysis completes, then you can omit the nq argument and ignore the subsequent code that interacts with the notification queue ingest_id = al_client . ingest ( path = '/pathto/file.txt' , nq = 'my_queue_name' , params = settings , metadata = my_meta ) # If you use a notification queue you can get your asynchronous results with: from time import sleep message = None while True : message = al_client . ingest . get_message ( \"my_queue_name\" ) if message is None : sleep ( 1 ) # Poll every second else : do something ... Submit submit_results = al_client . submit ( path = '/pathto/file.txt' , fname = 'fname' , params = settings , metadata = my_meta )","title":"Submit a file or URL for analysis"},{"location":"integration/python/#submission-details","text":"To get the details about a submission, you simply need to pass the client a submission ID (sid) submission_details = al_client . submission ( \"4nxrpBePQDLH427aA8m3TZ\" )","title":"Submission details"},{"location":"integration/python/#using-search","text":"More details about Search You can use the search engine in the client by simply passing a Lucene query. In the following example, we want to retrieve the first page of submissions made by user : search_result = al_client . search . submission ( \"params.submitter:user\" )","title":"Using search"},{"location":"integration/python/#using-search-iterator","text":"Instead of using search and getting a page of results, you can use the search iterator stream to go through all the results. Streamed results only return indexed fields. If you want the full result, you have to go get it via the client for submission in al_client . search . stream . submission ( \"params.submitter:user\" ): submission_id = submission [ \"sid\" ] full_submission = al_client . submission ( submission_id )","title":"Using search iterator"},{"location":"integration/python/#using-search-parameters","text":"In the following example, we want to retrieve the first page of submissions that were submitted in the last week, and we only want the submission IDs: submission_results = al_client . search . submission ( 'times.submitted:[now-7d TO now]' , fl = 'sid' )","title":"Using search parameters"},{"location":"integration/python/#using-facet-searching","text":"In the following example, we want to retrieve the users who have made submissions in the last week, and the number of submissions that they have made: submission_results = al_client . search . facet . submission ( 'params.submitter' , query = 'times.submitted:[now-7d TO now]' )","title":"Using facet searching"},{"location":"integration/python/#using-the-command-line-tool","text":"By installing the assemblyline_client PIP package, a command-line tool al-submit is installed. In case you don't want to use Python code to interface with the Assemblyline client, you can use this tool instead. You can view the user options via al-submit --help . (Optional) Configuration file example Rather than passing authentication and server details as parameters in a command-line, you can use a configuration file. This configuration file should be placed at ~/.al/submit.cfg . A template for this configuration file can be found below. NOTE: You can use = or : as the delimiter between key and value. [ auth ] # Username for the Assemblyline account. user = # There are three methods to authenticate a user account. Choose one: # - Password Provided via User Prompt # Leave the `password' configuration value below empty. # - Password Provided in Configuration File # Enter the password for the Assemblyline account in plaintext. password = # - API Key in Configuration File # Enter the API key to use in plaintext for the user to login. # NOTE: The API key must have WRITE access for INGEST and WRITE+READ for SUBMIT. apikey = # Skip server cert validation. # Value can be one of: true, false, yes, no # If not supplied, the default value is: false insecure = [ server ] # Method of network transport. # If not supplied, the default value is: https transport = # Domain of Assemblyline instance. # If not supplied, the default value is: localhost host = # Port to which traffic will be sent. # If not supplied, the default value is: 443 port = # Server cert used to connect to server. cert =","title":"Using the Command-line Tool"},{"location":"integration/python/#mass-submission-toolkit","text":"The Assemblyline Incident Manager can assist you with this process. One key consideration for a very large volume of files in a burst is the default sampling values . You must keep your ingestion flow at a rate such that the size of the priority ingestion queue remains lower than the corresponding priority queue sampling_at values, otherwise, Assemblyline will skip files.","title":"Mass Submission Toolkit"},{"location":"integration/rest/","text":"RESTful API \u00b6 When it is impossible to integrate your application using the dedicated python client, you can use Assemblyline's RESTful API to perform any task that you can think of. API documentation \u00b6 Each instance of Assemblyline comes with its internal API documentation which can be viewed by browsing to https://yourdomain/help/api Connecting to the API \u00b6 For easy integration, it is recommended that you generate an API key for the user who will perform RESTful queries. Otherwise, you will have to build yourself a library that will handle session cookies and XSRF tokens and you probably want something simpler. Using the API key \u00b6 To use your newly created API key you can simply add the X-USER and X-APIKEY headers to your request and the system will identify you with that key at each request instead of relying on a session cookie. Example Let's use a hypothetical API key to ask the system who we are. (Using the /api/v4/user/whoami/ API) CURL curl -X GET \"https://yourdomain/api/v4/user/whoami/\" \\ -H 'x-user: <your_user_id>' \\ -H 'x-apikey: <key_name:randomly_generated_password>' \\ -H 'accept: application/json' Javascript (fetch) fetch ( \"https://yourdomain/api/v4/user/whoami/\" , { \"headers\" : { \"accept\" : \"application/json\" , \"x-apikey\" : \"<key_name:randomly_generated_password>\" , \"x-user\" : \"<your_user_id>\" }, \"method\" : \"GET\" } ); Python (requests) import requests requests . get ( \"https://yourdomain/api/v4/user/whoami/\" , headers = { \"x-user\" : \"<your_user_id>\" , \"x-apikey\" : \"<key_name:randomly_generated_password>\" , \"accept\" : \"application/json\" } ) API Gotcha! \u00b6 Here is a list of the most common issues users are facing while using the API Wrong content type \u00b6 All Assemblyline APIs are built around receiving and returning JSON data. Do not forget to set your Content-Type and Accept headers to \"application/json\" or you might encounter some issues. Trailing forward slash \u00b6 All Assemblyline APIs end with a trailing forward slash \"/\" . Make sure that the API URL has it at the end of the URL otherwise you may get a \"Method not allowed\" error and you'll have issues figuring out why.","title":"RESTful API"},{"location":"integration/rest/#restful-api","text":"When it is impossible to integrate your application using the dedicated python client, you can use Assemblyline's RESTful API to perform any task that you can think of.","title":"RESTful API"},{"location":"integration/rest/#api-documentation","text":"Each instance of Assemblyline comes with its internal API documentation which can be viewed by browsing to https://yourdomain/help/api","title":"API documentation"},{"location":"integration/rest/#connecting-to-the-api","text":"For easy integration, it is recommended that you generate an API key for the user who will perform RESTful queries. Otherwise, you will have to build yourself a library that will handle session cookies and XSRF tokens and you probably want something simpler.","title":"Connecting to the API"},{"location":"integration/rest/#using-the-api-key","text":"To use your newly created API key you can simply add the X-USER and X-APIKEY headers to your request and the system will identify you with that key at each request instead of relying on a session cookie. Example Let's use a hypothetical API key to ask the system who we are. (Using the /api/v4/user/whoami/ API) CURL curl -X GET \"https://yourdomain/api/v4/user/whoami/\" \\ -H 'x-user: <your_user_id>' \\ -H 'x-apikey: <key_name:randomly_generated_password>' \\ -H 'accept: application/json' Javascript (fetch) fetch ( \"https://yourdomain/api/v4/user/whoami/\" , { \"headers\" : { \"accept\" : \"application/json\" , \"x-apikey\" : \"<key_name:randomly_generated_password>\" , \"x-user\" : \"<your_user_id>\" }, \"method\" : \"GET\" } ); Python (requests) import requests requests . get ( \"https://yourdomain/api/v4/user/whoami/\" , headers = { \"x-user\" : \"<your_user_id>\" , \"x-apikey\" : \"<key_name:randomly_generated_password>\" , \"accept\" : \"application/json\" } )","title":"Using the API key"},{"location":"integration/rest/#api-gotcha","text":"Here is a list of the most common issues users are facing while using the API","title":"API Gotcha!"},{"location":"integration/rest/#wrong-content-type","text":"All Assemblyline APIs are built around receiving and returning JSON data. Do not forget to set your Content-Type and Accept headers to \"application/json\" or you might encounter some issues.","title":"Wrong content type"},{"location":"integration/rest/#trailing-forward-slash","text":"All Assemblyline APIs end with a trailing forward slash \"/\" . Make sure that the API URL has it at the end of the URL otherwise you may get a \"Method not allowed\" error and you'll have issues figuring out why.","title":"Trailing forward slash"},{"location":"overview/community_services/","text":"Community services \u00b6 The Assemblyline community has been hard at work to improve Assemblyline's ability to detect malicious files and extract information about them. This page lists all the services that our members have created and shared with the public. Warning These services are not managed by the Assemblyline team so make sure that you check their source thoroughly and that you are comfortable with what they do before you install them on your system. Service list \u00b6 Service Name Description Author Source AutoItRipper AutoIt unpacker service NVISO link Cape Assemblyline service build for CAPE's API NVISO link ClamAV Assemblyline service which submits a file to ClamAV and displays the result NVISO link DocumentPreview Document preview service NVISO link IntezerStatic Assemblyline service which fetchs the result of a specific sha256 intezer scan NVISO link MalwareBazaar Assemblyline service fetching Malware Bazaar report NVISO link MsgParser Simple MSG extractor AssemblyLine service NVISO link PythonExeUnpack Python exe unpacker service NVISO link StegFinder AssemblyLine service which scans for embedded data in image using StegExpose NVISO link Unfurl Assemblyline service parsing a submitted URL to unshorten it. NVISO link UrlScanIo URLScan.io AL service NVISO link WindowsDefender Windows defender service being adapted from an Assemblyline community conversation Adam McHugh link Building a Community Service \u00b6 Obtain the service source code Edit the service manifest and ensure the following is set version : $SERVICE_TAG ... docker_config : image : ${REGISTRY}<service_container_image>:$SERVICE_TAG Build image and push to your local registry: Warning It's strongly recommended to tag service images following the Assemblyline format. Otherwise, the system will disable your service because it will deem it incompatible with the rest of the components (4.1+). Service versions should follow the format A.B.C.(dev|stable).D , where: A, B represents the framework and system version, respectively. C, D can be used to indicate the major and minor of a service, respectively. The dev or stable portion of the tag should indicate the state of the service build. This is also relevant for providing service updates under a certain channel. The following is an example of a service build targetted for an Assemblyline deployment running release 4.1.x.x: docker build . -t <private_registry>/<service_container_image>:4.1.0.stable0 --build-arg version = 4 .1.0.stable0 docker push <private_registry>/<service_container_image>:4.1.0.stable0 4. Add contents of service manifest to UI or using the REST API to add the service to Assemblyline Add your service \u00b6 Contact us on google groups to get your service featured on this page.","title":"Community services"},{"location":"overview/community_services/#community-services","text":"The Assemblyline community has been hard at work to improve Assemblyline's ability to detect malicious files and extract information about them. This page lists all the services that our members have created and shared with the public. Warning These services are not managed by the Assemblyline team so make sure that you check their source thoroughly and that you are comfortable with what they do before you install them on your system.","title":"Community services"},{"location":"overview/community_services/#service-list","text":"Service Name Description Author Source AutoItRipper AutoIt unpacker service NVISO link Cape Assemblyline service build for CAPE's API NVISO link ClamAV Assemblyline service which submits a file to ClamAV and displays the result NVISO link DocumentPreview Document preview service NVISO link IntezerStatic Assemblyline service which fetchs the result of a specific sha256 intezer scan NVISO link MalwareBazaar Assemblyline service fetching Malware Bazaar report NVISO link MsgParser Simple MSG extractor AssemblyLine service NVISO link PythonExeUnpack Python exe unpacker service NVISO link StegFinder AssemblyLine service which scans for embedded data in image using StegExpose NVISO link Unfurl Assemblyline service parsing a submitted URL to unshorten it. NVISO link UrlScanIo URLScan.io AL service NVISO link WindowsDefender Windows defender service being adapted from an Assemblyline community conversation Adam McHugh link","title":"Service list"},{"location":"overview/community_services/#building-a-community-service","text":"Obtain the service source code Edit the service manifest and ensure the following is set version : $SERVICE_TAG ... docker_config : image : ${REGISTRY}<service_container_image>:$SERVICE_TAG Build image and push to your local registry: Warning It's strongly recommended to tag service images following the Assemblyline format. Otherwise, the system will disable your service because it will deem it incompatible with the rest of the components (4.1+). Service versions should follow the format A.B.C.(dev|stable).D , where: A, B represents the framework and system version, respectively. C, D can be used to indicate the major and minor of a service, respectively. The dev or stable portion of the tag should indicate the state of the service build. This is also relevant for providing service updates under a certain channel. The following is an example of a service build targetted for an Assemblyline deployment running release 4.1.x.x: docker build . -t <private_registry>/<service_container_image>:4.1.0.stable0 --build-arg version = 4 .1.0.stable0 docker push <private_registry>/<service_container_image>:4.1.0.stable0 4. Add contents of service manifest to UI or using the REST API to add the service to Assemblyline","title":"Building a Community Service"},{"location":"overview/community_services/#add-your-service","text":"Contact us on google groups to get your service featured on this page.","title":"Add your service"},{"location":"overview/how_it_works/","text":"How it works \u00b6 Assemblyline minimizes the number of harmless files that IT practitioners are required to inspect every day, allowing them to collaborate with other users to customize and improve the platform in the process. A) Assemblyline works very much like a conveyor belt: files arrive in the system and are triaged in a certain sequence. B) Assemblyline generates information about each file and assigns a unique identifier that travels with the file as it flows through the system. C) Users can add their own analytics, which we refer to as services, to Assemblyline. D) The services selected by the user in Assemblyline then analyze the files, looking for indications of maliciousness and/or extracting features for further analysis. The system generates alerts about a malicious file at any point during the analysis and assigns the file a score. The system can also trigger automated defensive systems. Malicious indicators generated by the system can be distributed to other defense systems.","title":"How it works"},{"location":"overview/how_it_works/#how-it-works","text":"Assemblyline minimizes the number of harmless files that IT practitioners are required to inspect every day, allowing them to collaborate with other users to customize and improve the platform in the process. A) Assemblyline works very much like a conveyor belt: files arrive in the system and are triaged in a certain sequence. B) Assemblyline generates information about each file and assigns a unique identifier that travels with the file as it flows through the system. C) Users can add their own analytics, which we refer to as services, to Assemblyline. D) The services selected by the user in Assemblyline then analyze the files, looking for indications of maliciousness and/or extracting features for further analysis. The system generates alerts about a malicious file at any point during the analysis and assigns the file a score. The system can also trigger automated defensive systems. Malicious indicators generated by the system can be distributed to other defense systems.","title":"How it works"},{"location":"overview/services/","text":"Assemblyline services \u00b6 Services currently installed on a system can be found under Help > Service Listing . This is the list of all the services that are bundled with Assemblyline and that are maintained by the Assemblyline team: Service Name Speciality Description Source APKaye Android APK APKs are decompiled and inspected. Network indicators and information found in the APK manifest file are displayed link Anti-virus Anti-virus Generic ICAP client to integrate with most Anti-virus enterprise scanners link Characterize Entropy analysis Partitions the file and calculates visual entropy for each partition, extract Exif metadata link ConfigExtractor IoC extraction Extract malware configuration file, allowing to get list of C2, encryption material etc. link Cuckoo Sandbox Provides dynamic malware analysis through sandboxing. link DeobfuScripter Deobfuscation Static script de-obfuscator. The purpose is not to get surgical de-obfuscation, but rather to extract obfuscated IOCs. link EmlParser Email Parse emails using GOVCERT-LU eml_parser library while extracting header information, attachments, URIs link Espresso Java All classes are extracted, decompiled, and analyzed for malicious behavior link Extract Compressed file This service extracts embedded files from file containers (like ZIP, RAR, 7z, ...) link Floss IoC extraction Automatically extract obfuscated strings from malware using FireEye Labs Obfuscated String Solver link FrankenStrings IoC extraction This service performs file and IOC extractions using pattern matching, simple encoding decoder and script de-obfuscators link IPArse Apple IOS Analyze Apple apps link JSJaws Javascript Analyze malicious Javascript link MetaDefender Anti-virus Service for OPSWAT MetaDefender anti-virus (multi-engine) link MetaPeek Meta data analysis Checks submission metadata for indicators of potential malicious behavior (double file extensions, ...) link Oletools Office documents This service extracts metadata, network information and reports anomalies in Microsoft OLE and XML documents using the Python library py-oletools by Philippe Lagadec - http://www.decalage.info link Overpower PowerShell De-obfuscate PowerShell scripts link PDFId PDF This service extracts metadata from PDFs using Didier Stevens PDFId & PDFParse link PEFile Windows binaries This service extracts attributes (imports, exports, section names, ...) from windows PE files using the Python library pefile link PeePDF PDF This service uses the Python PeePDF library information from PDFs including JavaScript blocks which it will attempt to de-obfuscate, if necessary, for further analysis link PixAxe Images Extract text from images link Safelist Safelisting Allow for hash, IoC and signature safelisting, including support for downloading NSRL link Sigma Eventlog signatures Scan event logs (e.g. from sandbox or a compromised host) using Sigma link Suricata Network signatures Scan network capture (.pcap) submitted and extracted from analysis via Suricata link Swiffer Adobe Shockwave This service extracts metadata and performs anomaly detection on Adobe Shockwave (.swf) files link TagCheck Tag signatures YARA signatures on Assemblyline Tags (build your own signatures to hit on specific tags) link TorrentSlicer Torrent files Extracts information from torrent files link Unpacker UPX Unpacker This service unpacks UPX packed executables for further analysis link Unpac.me Unpacker Integrate with unpac.me link ViperMonkey Office documents ViperMonkey is a VBA Emulation engine by http://www.decalage.info link VirusTotalDynamic Anti-virus This service submits the files/URLs to VirusTotal and returns the results link VirusTotalStatic Anti-virus This service performs a hash check against the VirusTotal API and returns the result link XLMMacroDeobfuscator Office documents Analyze Excel 4.0 macros link YARA File signatures Signature for file link","title":"Assemblyline services"},{"location":"overview/services/#assemblyline-services","text":"Services currently installed on a system can be found under Help > Service Listing . This is the list of all the services that are bundled with Assemblyline and that are maintained by the Assemblyline team: Service Name Speciality Description Source APKaye Android APK APKs are decompiled and inspected. Network indicators and information found in the APK manifest file are displayed link Anti-virus Anti-virus Generic ICAP client to integrate with most Anti-virus enterprise scanners link Characterize Entropy analysis Partitions the file and calculates visual entropy for each partition, extract Exif metadata link ConfigExtractor IoC extraction Extract malware configuration file, allowing to get list of C2, encryption material etc. link Cuckoo Sandbox Provides dynamic malware analysis through sandboxing. link DeobfuScripter Deobfuscation Static script de-obfuscator. The purpose is not to get surgical de-obfuscation, but rather to extract obfuscated IOCs. link EmlParser Email Parse emails using GOVCERT-LU eml_parser library while extracting header information, attachments, URIs link Espresso Java All classes are extracted, decompiled, and analyzed for malicious behavior link Extract Compressed file This service extracts embedded files from file containers (like ZIP, RAR, 7z, ...) link Floss IoC extraction Automatically extract obfuscated strings from malware using FireEye Labs Obfuscated String Solver link FrankenStrings IoC extraction This service performs file and IOC extractions using pattern matching, simple encoding decoder and script de-obfuscators link IPArse Apple IOS Analyze Apple apps link JSJaws Javascript Analyze malicious Javascript link MetaDefender Anti-virus Service for OPSWAT MetaDefender anti-virus (multi-engine) link MetaPeek Meta data analysis Checks submission metadata for indicators of potential malicious behavior (double file extensions, ...) link Oletools Office documents This service extracts metadata, network information and reports anomalies in Microsoft OLE and XML documents using the Python library py-oletools by Philippe Lagadec - http://www.decalage.info link Overpower PowerShell De-obfuscate PowerShell scripts link PDFId PDF This service extracts metadata from PDFs using Didier Stevens PDFId & PDFParse link PEFile Windows binaries This service extracts attributes (imports, exports, section names, ...) from windows PE files using the Python library pefile link PeePDF PDF This service uses the Python PeePDF library information from PDFs including JavaScript blocks which it will attempt to de-obfuscate, if necessary, for further analysis link PixAxe Images Extract text from images link Safelist Safelisting Allow for hash, IoC and signature safelisting, including support for downloading NSRL link Sigma Eventlog signatures Scan event logs (e.g. from sandbox or a compromised host) using Sigma link Suricata Network signatures Scan network capture (.pcap) submitted and extracted from analysis via Suricata link Swiffer Adobe Shockwave This service extracts metadata and performs anomaly detection on Adobe Shockwave (.swf) files link TagCheck Tag signatures YARA signatures on Assemblyline Tags (build your own signatures to hit on specific tags) link TorrentSlicer Torrent files Extracts information from torrent files link Unpacker UPX Unpacker This service unpacks UPX packed executables for further analysis link Unpac.me Unpacker Integrate with unpac.me link ViperMonkey Office documents ViperMonkey is a VBA Emulation engine by http://www.decalage.info link VirusTotalDynamic Anti-virus This service submits the files/URLs to VirusTotal and returns the results link VirusTotalStatic Anti-virus This service performs a hash check against the VirusTotal API and returns the result link XLMMacroDeobfuscator Office documents Analyze Excel 4.0 macros link YARA File signatures Signature for file link","title":"Assemblyline services"},{"location":"user_manual/results/","text":"Assemblyline results \u00b6 Heuristics \u00b6 A heuristic is a feature that is detected by the service performing the analysis. A heuristic has: id name description score (used to label heuristic as MALICIOUS / SUSPICIOUS / INFO) MITRE Att&ck ID Signatures are often raised under a heuristics to provide more context Heuristics are tracked by the system to provide statistics on the number of hits and scores. Doing so can help with adjusting well-performing and under-performing heuristics. Heuristics will be shown in the UI and will be colour-coded based on the level of their maliciousness. Tags \u00b6 Tags are important metadata extracted from a file. All tag names follow the same naming convention. They are namespaced to improve organization and searching for specific information in the system Tags are also all indexed, which allow for blazingly fast results. network.static.ip e.g: All IPs extracted statically will be found with this tag regardless of which service extracted it. All tags registered with the system will be listed under the Help menu > Current Configuration page > Tags section of your Assemblyline instance. Score \u00b6 The score of a submission (maliciousness level) is determined by the highest score of any file extracted during the analysis process. For example let's take a zip file. The zip file itself might score 0 but if it contained two files, one of which scored 100 and the other 500, then the max score of the submission will be 500. It is possible to drill down into the file structure to understand exactly what contributed to each score. Score meaning \u00b6 -1000: safe 0 - 299: informational 300 - 699: suspicious 700 - 999: highly suspicious >= 1000: malicious The source code for these mappings can be found here . Submission Report \u00b6 The first page that will appear when you view a submission is the submission report. This page is a high-level summary to allow an analyst to decide if it is worth digging deeper. General information \u00b6 At the top of the report you will find important information such as timestamps, the file type detected, file size, and various hashes representing the file contents. The analysis verdict is also given with the maximum score revealed when hovering over the verdict (ie. 'Malicious'). Heuristics \u00b6 Under this section you will find all the heuristics, categorized by maliciousness level and every file associated. Attribution \u00b6 This section will provide attribution from Yara signatures (if the actor tag is provided in your rule's metadata) and anti-virus virus names. For best results, follow the CCCS standard when writing Yara rules. Submission Details \u00b6 The \"Submission Details\" button is located at the top of the submission report. Submission details will display submission parameters such as which services were selected when the file was submitted and submission metadata . Extracted File Tree \u00b6 The extracted file tree section will show a view of all the files that were processed and extracted along with their respective score and file type. Clicking on the files will reveal Assemblyline's most interesting section the File details page. File Details \u00b6 Under the \"File Details\" section you will find everything about a specific file. Regardless of which submission it came from. In the top right corner you will find a series of useful functions Icon Description Find all related submissions Download file (by default the file will be inserted in the CaRT format ) to prevent accidental self-infection File viewer (Ascii, Strings, Hex view) Resubmit the file for analysis Add file to the safelist File Frequency \u00b6 This section will tell you how many times this file has been seen, along with a first and last seen timestamp. These values will be affected by the retention period of the file in the system. File Tags \u00b6 This section will include all the tags extracted within this file, grouped by type. This is where you will find IPs, URLs and many other IoCs (indicators of compromise) which you can harvest to support your investigation or use to start a dynamic action (e.g: issue blocks on your firewalls). If you click on one of the tags it will highlight which service it came from. Service Results \u00b6 This section lets you visualize the output of each service along with any heuristics and tags raised. You can also see which services were the source of \"extracted files\" at the end of each service result. The cached file results are ignored every time a service is updated; if multiple service result versions are available, they will be shown in a dropdown which will let you look at older analysis results. You can expand the details by clicking on a service result section.","title":"Assemblyline results"},{"location":"user_manual/results/#assemblyline-results","text":"","title":"Assemblyline results"},{"location":"user_manual/results/#heuristics","text":"A heuristic is a feature that is detected by the service performing the analysis. A heuristic has: id name description score (used to label heuristic as MALICIOUS / SUSPICIOUS / INFO) MITRE Att&ck ID Signatures are often raised under a heuristics to provide more context Heuristics are tracked by the system to provide statistics on the number of hits and scores. Doing so can help with adjusting well-performing and under-performing heuristics. Heuristics will be shown in the UI and will be colour-coded based on the level of their maliciousness.","title":"Heuristics"},{"location":"user_manual/results/#tags","text":"Tags are important metadata extracted from a file. All tag names follow the same naming convention. They are namespaced to improve organization and searching for specific information in the system Tags are also all indexed, which allow for blazingly fast results. network.static.ip e.g: All IPs extracted statically will be found with this tag regardless of which service extracted it. All tags registered with the system will be listed under the Help menu > Current Configuration page > Tags section of your Assemblyline instance.","title":"Tags"},{"location":"user_manual/results/#score","text":"The score of a submission (maliciousness level) is determined by the highest score of any file extracted during the analysis process. For example let's take a zip file. The zip file itself might score 0 but if it contained two files, one of which scored 100 and the other 500, then the max score of the submission will be 500. It is possible to drill down into the file structure to understand exactly what contributed to each score.","title":"Score"},{"location":"user_manual/results/#score-meaning","text":"-1000: safe 0 - 299: informational 300 - 699: suspicious 700 - 999: highly suspicious >= 1000: malicious The source code for these mappings can be found here .","title":"Score meaning"},{"location":"user_manual/results/#submission-report","text":"The first page that will appear when you view a submission is the submission report. This page is a high-level summary to allow an analyst to decide if it is worth digging deeper.","title":"Submission Report"},{"location":"user_manual/results/#general-information","text":"At the top of the report you will find important information such as timestamps, the file type detected, file size, and various hashes representing the file contents. The analysis verdict is also given with the maximum score revealed when hovering over the verdict (ie. 'Malicious').","title":"General information"},{"location":"user_manual/results/#heuristics_1","text":"Under this section you will find all the heuristics, categorized by maliciousness level and every file associated.","title":"Heuristics"},{"location":"user_manual/results/#attribution","text":"This section will provide attribution from Yara signatures (if the actor tag is provided in your rule's metadata) and anti-virus virus names. For best results, follow the CCCS standard when writing Yara rules.","title":"Attribution"},{"location":"user_manual/results/#submission-details","text":"The \"Submission Details\" button is located at the top of the submission report. Submission details will display submission parameters such as which services were selected when the file was submitted and submission metadata .","title":"Submission Details"},{"location":"user_manual/results/#extracted-file-tree","text":"The extracted file tree section will show a view of all the files that were processed and extracted along with their respective score and file type. Clicking on the files will reveal Assemblyline's most interesting section the File details page.","title":"Extracted File Tree"},{"location":"user_manual/results/#file-details","text":"Under the \"File Details\" section you will find everything about a specific file. Regardless of which submission it came from. In the top right corner you will find a series of useful functions Icon Description Find all related submissions Download file (by default the file will be inserted in the CaRT format ) to prevent accidental self-infection File viewer (Ascii, Strings, Hex view) Resubmit the file for analysis Add file to the safelist","title":"File Details"},{"location":"user_manual/results/#file-frequency","text":"This section will tell you how many times this file has been seen, along with a first and last seen timestamp. These values will be affected by the retention period of the file in the system.","title":"File Frequency"},{"location":"user_manual/results/#file-tags","text":"This section will include all the tags extracted within this file, grouped by type. This is where you will find IPs, URLs and many other IoCs (indicators of compromise) which you can harvest to support your investigation or use to start a dynamic action (e.g: issue blocks on your firewalls). If you click on one of the tags it will highlight which service it came from.","title":"File Tags"},{"location":"user_manual/results/#service-results","text":"This section lets you visualize the output of each service along with any heuristics and tags raised. You can also see which services were the source of \"extracted files\" at the end of each service result. The cached file results are ignored every time a service is updated; if multiple service result versions are available, they will be shown in a dropdown which will let you look at older analysis results. You can expand the details by clicking on a service result section.","title":"Service Results"},{"location":"user_manual/searching/","text":"Searching \u00b6 Assemblyline leverages the powerful capabilities of Elasticsearch which make it possible to search for almost anything. Document store \u00b6 One key concept to understand are the \" indices \" of information. These allow Assemblyline to deduplicate most of the results in the system which is a major reason why Assemblyline is able to scale so well. Searching indexed fields is also very fast. There are 5 primary \" indices \" Submissions Files Results Alerts Signatures You can view all indices and their indexed fields once you have a working Assemblyline under Help > Search Help menu. Searching behaviors and limitations \u00b6 When you search for something in the Search Bar at the top of the UI, it will run your query in all the indexes and return any matching results. You must limit your search criteria to a single index; in other words you cannot do JOIN queries with information present in two or more indexes. This limitation can be worked around by using the Assemblyline Client by performing queries on one index and then enriching or narrowing your search by searching for elements in another index. Search exemples \u00b6 One quick way to get familiar with search indices is to use the \" Find related results \" item from the tags dropdown menu. Clicking it on the av.virus_name tag ( HEUR/Macro.Downloader.MRAA.Gen ) will build the following query: result . sections . tags . av . virus_name :\"HEUR/Macro.Downloader.MRAA.Gen\" You can also build more complex searches by leveraging the full query syntax. Here are some other examples: # Find every result where the ViperMonkey service extracted the IP 10.10.10.10 result . sections . tags . network . static . ip :\"10.10.10.10\" AND response . service_name :ViperMonkey # Find all submissions with a score greater than or equal to 2000 in the last two days max_score : [ 2000 TO *] AND times . submitted : [ now - 2 d TO now ] # Find all anti-virus results with Emotet in the signature name result . sections . tags . av . virus_name :* Emotet * The system supports a wide range of search parameter such as wildcards, ranges and regex. The full syntax range can be found under Help > Search Help Search queries can also be used with the Assemblyline Client to build powerful tradecraft which can run automatically as new files get scanned by the system.","title":"Searching"},{"location":"user_manual/searching/#searching","text":"Assemblyline leverages the powerful capabilities of Elasticsearch which make it possible to search for almost anything.","title":"Searching"},{"location":"user_manual/searching/#document-store","text":"One key concept to understand are the \" indices \" of information. These allow Assemblyline to deduplicate most of the results in the system which is a major reason why Assemblyline is able to scale so well. Searching indexed fields is also very fast. There are 5 primary \" indices \" Submissions Files Results Alerts Signatures You can view all indices and their indexed fields once you have a working Assemblyline under Help > Search Help menu.","title":"Document store"},{"location":"user_manual/searching/#searching-behaviors-and-limitations","text":"When you search for something in the Search Bar at the top of the UI, it will run your query in all the indexes and return any matching results. You must limit your search criteria to a single index; in other words you cannot do JOIN queries with information present in two or more indexes. This limitation can be worked around by using the Assemblyline Client by performing queries on one index and then enriching or narrowing your search by searching for elements in another index.","title":"Searching behaviors and limitations"},{"location":"user_manual/searching/#search-exemples","text":"One quick way to get familiar with search indices is to use the \" Find related results \" item from the tags dropdown menu. Clicking it on the av.virus_name tag ( HEUR/Macro.Downloader.MRAA.Gen ) will build the following query: result . sections . tags . av . virus_name :\"HEUR/Macro.Downloader.MRAA.Gen\" You can also build more complex searches by leveraging the full query syntax. Here are some other examples: # Find every result where the ViperMonkey service extracted the IP 10.10.10.10 result . sections . tags . network . static . ip :\"10.10.10.10\" AND response . service_name :ViperMonkey # Find all submissions with a score greater than or equal to 2000 in the last two days max_score : [ 2000 TO *] AND times . submitted : [ now - 2 d TO now ] # Find all anti-virus results with Emotet in the signature name result . sections . tags . av . virus_name :* Emotet * The system supports a wide range of search parameter such as wildcards, ranges and regex. The full syntax range can be found under Help > Search Help Search queries can also be used with the Assemblyline Client to build powerful tradecraft which can run automatically as new files get scanned by the system.","title":"Search exemples"},{"location":"user_manual/submitting_file/","text":"Submitting a file for analysis \u00b6 Submission \u00b6 Submitting a file for analysis is really easy; it can be done directly using the Assemblyline WebUI. For automation and integration you can use the REST API . Sharing and classification \u00b6 If your system is configured with a sharing control (TLP) or Classification configuration, available restriction can be selected by clicking on the Classification Banner. Selecting a file to scan \u00b6 You can click the \"Select a file to scan\" or drag and drop a file to the area enclosed by the dashed line to add a file to be analyzed. Options \u00b6 Additional submission options are available to: Select which service categories or specific services to use for the analysis Specify service configuration options (such as providing a password, or dynamic analysis timeout) Ignore filtering services: Bypass safelisting services Ignore result cache: Force re-analysis even if the same file had been scanned recently with the same service versions Ignore dynamic recursion prevention: Disable iteration limit on a file Profile current scan Perform deep analysis: Provide maximum deobfuscation ( Highly recommended for known malicious or highly suspicious file to detect highly obfuscated content ) Time to live: Time (in days) before the file is purged from the system File analysis \u00b6 Once a file is submitted to Assemblyline, the system will automatically perform multiple checks to determine how to best process the file. One of Assemblyline's most powerful functionalities is its recursive analysis model. Malware and malicious documents often use multiple layers of obfuscation; recursive analysis allows the system to remove these layers and keep analyzing the file. The end result is often a cleartext script or unpacked malware which traditional anti-virus are very effective at detecting.","title":"Submitting a file for analysis"},{"location":"user_manual/submitting_file/#submitting-a-file-for-analysis","text":"","title":"Submitting a file for analysis"},{"location":"user_manual/submitting_file/#submission","text":"Submitting a file for analysis is really easy; it can be done directly using the Assemblyline WebUI. For automation and integration you can use the REST API .","title":"Submission"},{"location":"user_manual/submitting_file/#sharing-and-classification","text":"If your system is configured with a sharing control (TLP) or Classification configuration, available restriction can be selected by clicking on the Classification Banner.","title":"Sharing and classification"},{"location":"user_manual/submitting_file/#selecting-a-file-to-scan","text":"You can click the \"Select a file to scan\" or drag and drop a file to the area enclosed by the dashed line to add a file to be analyzed.","title":"Selecting a file to scan"},{"location":"user_manual/submitting_file/#options","text":"Additional submission options are available to: Select which service categories or specific services to use for the analysis Specify service configuration options (such as providing a password, or dynamic analysis timeout) Ignore filtering services: Bypass safelisting services Ignore result cache: Force re-analysis even if the same file had been scanned recently with the same service versions Ignore dynamic recursion prevention: Disable iteration limit on a file Profile current scan Perform deep analysis: Provide maximum deobfuscation ( Highly recommended for known malicious or highly suspicious file to detect highly obfuscated content ) Time to live: Time (in days) before the file is purged from the system","title":"Options"},{"location":"user_manual/submitting_file/#file-analysis","text":"Once a file is submitted to Assemblyline, the system will automatically perform multiple checks to determine how to best process the file. One of Assemblyline's most powerful functionalities is its recursive analysis model. Malware and malicious documents often use multiple layers of obfuscation; recursive analysis allows the system to remove these layers and keep analyzing the file. The end result is often a cleartext script or unpacked malware which traditional anti-virus are very effective at detecting.","title":"File analysis"},{"location":"fr/","text":"","title":"Accueil"},{"location":"fr/administration/service_management/","text":"Service management \u00b6 Assemblyline's service management interface lets you: List all the services in the system View details about those services Add/Modify/Remove services Download/Restore a backup of the current services configurations You can find the service managment interface by clicking the User Avatar then choose Services from the administrator menu. Service list \u00b6 The first page you will be taken to when loading the service management interface will list all the services of the system. From this interface you can: Add services to the system Perform service updates Download/Restore a backup of the current services configurations View the detail of a service Update services \u00b6 If the system detected that there is a container with a newer version for your current deployment type (dev/stable). The service list will show an update button. Hovering over the button will let you know which new service version is available and clicking the button will kick off the update for the service. Add a service \u00b6 From the service management page, you can add a service by clicking the circled green \" + \" sign in the top right corner. This will open a popup window with an empty textbox. Simply paste the service_manifest.yml content of the service you which to add to the system then hit the \" Add \" button to add it to the system. Tip If your manifest properly uses the following environment variables, they will be replaced by the right values by the service add API: $SERVICE_TAG : Will de replaced by the latest tag for you current deployment type (dev/stable) found in the docker registry where the service container is hosted $REGISTRY : Will be replaced by your local registry Create / Restore Service config backups \u00b6 At the top right corner of the service management page, you will also find backup and restore buttons for creating and restoring backups for the services configurations. The backup button which looks like an \" arrow pointing down \" will create a yml with a filename of the following format: <FQDN>_service_backup.yml . The file will automatically be downloaded by your browser in your download directory. Once you want to restore the backup in your system, you can simply click the restore button, \" clock with a counter-clockwise arrow \", This will open a modal window with an empty textbox. Simply paste the content of the backup created earlier in the text box and hit the \" Restore \" button to restore the services configurations to their backed up values. Service Details \u00b6 If you which to modify or remove a service, you can simply click on that service from the service list which will bring you to the service detail page. The service detail page header contains two button shown all time that will let you: Delete the service (red \" circled minus \" button) Toggle between enabled/disable state (big square button right on top of the tabs) You will then have a tabbed interface which we will describe each tab bellow. General tab \u00b6 The \" General \" tab will let you see general informations about the service. In this tab, you will be able to modify the service's: Version Description Execution Stage Category Accepted/Rejected file types Execution timeout Maximum number of instances Location Result caching Tip You can refer to the service manifest documentation for more information about those different fields. Container tab \u00b6 The \" Container \" tab will show information about containers used by the service. In this tab, you will be able to: Change the update channel (Development/Stable) Change the main service service container Add/modify/remove dependency containers Main service container \u00b6 The main service container is the container containing and running the service code. By clicking the main service container, you will be able to modify the parameters used to launch that container. The list of parameters you will be able to modify is the following: Container image name Type of container registry Resources limits (CPU/RAM) Container registry credentials (username/password) Command executed in the container Allow internet access to the container Environment variables set before loading the container Tip Check the docker config block from the service manifest documentation to know more about the different field you can modify in the docker container configuration. Dependency containers \u00b6 Dependency containers are containers use to support the main services in some ways. Either by offering an external place to store data (A database for exemple) or to perform service updates. A service can have multiple dependency containers and these containers are shared between the multiple intances of the service that can be loaded in the system i.e. there will only be one instance of each dependency containers. By either click the \" Add Dependency \" button or clicking a dependency container, you will be able to either add or modify container dependencies of the current service. The dependency container configuration window look almost the same and let you modify the same values as the main service container window. There is however an added parameter that you can configure to give the container persistent storage. Tip Check the persistent volume block from the service manifest documentation to know more about the different fields to configure to get persistent storage in a dependency container. Updates tab \u00b6 The \" Updates \" tab shows information about how the service updates itself or its signatures. Warning This tab is optional and will not be shown for all service. Only services that define and update config block in their service manifest will have that tab shown. In this tab, you will be able to view/modify the following information: Interval at which the service updates If the service generates signatures in the system or not If the service needs to wait for a successful updates to start intances of itself The different sources where the service pulls its updates from Tip Checkout the Modifying sources documentation to know more about the different values you can change in the signature sources. Parameters tab \u00b6 Finally, the \" Parameters \" tab will let you view and customize the different parameters the service can take in. Service parameters are split into two categories: User specified parameters Service variables User specified parameters \u00b6 User specified parameters are parameters that a user can modify for each specific submission it does in the system. They are often but not exclusively used for things like: Turning on/off features of a service Specifying a password used during a submission Limit what the service can and cannot do Extract more or less files when a service runs Tip When these parameters are defined for a service, they will be shown in the submission options available for the user at submission time. Service variables \u00b6 Service variable are configuration parameters only shared between the service an your deployment. They are used to help the service configure itself to run well in your environment. Service variables are often but not exclusively things like: URLs to connect to external services Credentials use to connect to external services List of default values used in a service Configuration parameter that will limit or increase scanning capabilities of a service","title":"Service management"},{"location":"fr/administration/service_management/#service-management","text":"Assemblyline's service management interface lets you: List all the services in the system View details about those services Add/Modify/Remove services Download/Restore a backup of the current services configurations You can find the service managment interface by clicking the User Avatar then choose Services from the administrator menu.","title":"Service management"},{"location":"fr/administration/service_management/#service-list","text":"The first page you will be taken to when loading the service management interface will list all the services of the system. From this interface you can: Add services to the system Perform service updates Download/Restore a backup of the current services configurations View the detail of a service","title":"Service list"},{"location":"fr/administration/service_management/#update-services","text":"If the system detected that there is a container with a newer version for your current deployment type (dev/stable). The service list will show an update button. Hovering over the button will let you know which new service version is available and clicking the button will kick off the update for the service.","title":"Update services"},{"location":"fr/administration/service_management/#add-a-service","text":"From the service management page, you can add a service by clicking the circled green \" + \" sign in the top right corner. This will open a popup window with an empty textbox. Simply paste the service_manifest.yml content of the service you which to add to the system then hit the \" Add \" button to add it to the system. Tip If your manifest properly uses the following environment variables, they will be replaced by the right values by the service add API: $SERVICE_TAG : Will de replaced by the latest tag for you current deployment type (dev/stable) found in the docker registry where the service container is hosted $REGISTRY : Will be replaced by your local registry","title":"Add a service"},{"location":"fr/administration/service_management/#create-restore-service-config-backups","text":"At the top right corner of the service management page, you will also find backup and restore buttons for creating and restoring backups for the services configurations. The backup button which looks like an \" arrow pointing down \" will create a yml with a filename of the following format: <FQDN>_service_backup.yml . The file will automatically be downloaded by your browser in your download directory. Once you want to restore the backup in your system, you can simply click the restore button, \" clock with a counter-clockwise arrow \", This will open a modal window with an empty textbox. Simply paste the content of the backup created earlier in the text box and hit the \" Restore \" button to restore the services configurations to their backed up values.","title":"Create / Restore Service config backups"},{"location":"fr/administration/service_management/#service-details","text":"If you which to modify or remove a service, you can simply click on that service from the service list which will bring you to the service detail page. The service detail page header contains two button shown all time that will let you: Delete the service (red \" circled minus \" button) Toggle between enabled/disable state (big square button right on top of the tabs) You will then have a tabbed interface which we will describe each tab bellow.","title":"Service Details"},{"location":"fr/administration/service_management/#general-tab","text":"The \" General \" tab will let you see general informations about the service. In this tab, you will be able to modify the service's: Version Description Execution Stage Category Accepted/Rejected file types Execution timeout Maximum number of instances Location Result caching Tip You can refer to the service manifest documentation for more information about those different fields.","title":"General tab"},{"location":"fr/administration/service_management/#container-tab","text":"The \" Container \" tab will show information about containers used by the service. In this tab, you will be able to: Change the update channel (Development/Stable) Change the main service service container Add/modify/remove dependency containers","title":"Container tab"},{"location":"fr/administration/service_management/#main-service-container","text":"The main service container is the container containing and running the service code. By clicking the main service container, you will be able to modify the parameters used to launch that container. The list of parameters you will be able to modify is the following: Container image name Type of container registry Resources limits (CPU/RAM) Container registry credentials (username/password) Command executed in the container Allow internet access to the container Environment variables set before loading the container Tip Check the docker config block from the service manifest documentation to know more about the different field you can modify in the docker container configuration.","title":"Main service container"},{"location":"fr/administration/service_management/#dependency-containers","text":"Dependency containers are containers use to support the main services in some ways. Either by offering an external place to store data (A database for exemple) or to perform service updates. A service can have multiple dependency containers and these containers are shared between the multiple intances of the service that can be loaded in the system i.e. there will only be one instance of each dependency containers. By either click the \" Add Dependency \" button or clicking a dependency container, you will be able to either add or modify container dependencies of the current service. The dependency container configuration window look almost the same and let you modify the same values as the main service container window. There is however an added parameter that you can configure to give the container persistent storage. Tip Check the persistent volume block from the service manifest documentation to know more about the different fields to configure to get persistent storage in a dependency container.","title":"Dependency containers"},{"location":"fr/administration/service_management/#updates-tab","text":"The \" Updates \" tab shows information about how the service updates itself or its signatures. Warning This tab is optional and will not be shown for all service. Only services that define and update config block in their service manifest will have that tab shown. In this tab, you will be able to view/modify the following information: Interval at which the service updates If the service generates signatures in the system or not If the service needs to wait for a successful updates to start intances of itself The different sources where the service pulls its updates from Tip Checkout the Modifying sources documentation to know more about the different values you can change in the signature sources.","title":"Updates tab"},{"location":"fr/administration/service_management/#parameters-tab","text":"Finally, the \" Parameters \" tab will let you view and customize the different parameters the service can take in. Service parameters are split into two categories: User specified parameters Service variables","title":"Parameters tab"},{"location":"fr/administration/service_management/#user-specified-parameters","text":"User specified parameters are parameters that a user can modify for each specific submission it does in the system. They are often but not exclusively used for things like: Turning on/off features of a service Specifying a password used during a submission Limit what the service can and cannot do Extract more or less files when a service runs Tip When these parameters are defined for a service, they will be shown in the submission options available for the user at submission time.","title":"User specified parameters"},{"location":"fr/administration/service_management/#service-variables","text":"Service variable are configuration parameters only shared between the service an your deployment. They are used to help the service configure itself to run well in your environment. Service variables are often but not exclusively things like: URLs to connect to external services Credentials use to connect to external services List of default values used in a service Configuration parameter that will limit or increase scanning capabilities of a service","title":"Service variables"},{"location":"fr/administration/signature_management/","text":"Signature Management \u00b6 Assemblyline's signature management interface lets you: List all signatures in the system Filter and search the current set of signatures View details about those signatures Set the status of a specific signature Remove signatures from the system You can find the signature management interface by clicking Manage then the Signatures menu from the navigation bar. Warning You cannot add new signatures to the system via this interface. Instead, Assemblyline has a source management interface which lets you add a variety of external sources to fetch signatures from. The updater of the different services takes care of loading the source URLs and the new signature(s) into the system. It will also sync existing signatures that have changed since the last import. Signature list \u00b6 The first page you will be taken to when loading the signature management interface will list all signatures that have been loaded into the system. From this interface you can: Page through the different signatures from the list Filter the displayed signatures with the search bar Assemblyline signatures can be searched using a Lucene query. As you start typing in the search box, the system will suggest fields that you can search into. You can also use the quick filter buttons for pre-defined searches. These pre-defined searches will help you get started with writing more complex signature searches. Download the currently viewed signature set with the download arrow on the top right View the detail of a signature by clicking on it Signature detail \u00b6 Once you click on a signature, the detail view for that signature will be shown. This page will show you the following information: ID of the signature (under the signature detail header) The raw signature Statistics about the signature A histogram of the signature for the last 30 days A list of the last ten hits for that signature On the top right, it will also show actions on the signature: You can hit the search button to find all instances where that signature hits in the system Use the red delete button to delete the signature from the system If the signature is still present in the source where it was retrieved, it will be re-added on the next update. In this case, you should disable the signature instead. Change the state of a signature Changing the signature state \u00b6 Signature states are synced with the source they are coming from but the state in your Assemblyline deployment will supersede the state that the rule updater is trying to set. This means that if you disable a rule in your Assemblyline instance, it will remain disabled even if the source where that rule is from changes. There are three different signature states: Deployed , Noisy , and Disabled Deployed : Deployed will be used for detection and will generate a score depending on how the service handles these types of signatures Noisy: Noisy will be used for detection but rules with these states will not affect the score of the file Disabled: Disabled signatures are completely ignored in the system and the service will not even realize that these signatures exist You can change the signatures by clicking the current signature state in the signature detail view. This will bring up the state-changing modal window which will let you pick a new state for the current rule.","title":"Signature Management"},{"location":"fr/administration/signature_management/#signature-management","text":"Assemblyline's signature management interface lets you: List all signatures in the system Filter and search the current set of signatures View details about those signatures Set the status of a specific signature Remove signatures from the system You can find the signature management interface by clicking Manage then the Signatures menu from the navigation bar. Warning You cannot add new signatures to the system via this interface. Instead, Assemblyline has a source management interface which lets you add a variety of external sources to fetch signatures from. The updater of the different services takes care of loading the source URLs and the new signature(s) into the system. It will also sync existing signatures that have changed since the last import.","title":"Signature Management"},{"location":"fr/administration/signature_management/#signature-list","text":"The first page you will be taken to when loading the signature management interface will list all signatures that have been loaded into the system. From this interface you can: Page through the different signatures from the list Filter the displayed signatures with the search bar Assemblyline signatures can be searched using a Lucene query. As you start typing in the search box, the system will suggest fields that you can search into. You can also use the quick filter buttons for pre-defined searches. These pre-defined searches will help you get started with writing more complex signature searches. Download the currently viewed signature set with the download arrow on the top right View the detail of a signature by clicking on it","title":"Signature list"},{"location":"fr/administration/signature_management/#signature-detail","text":"Once you click on a signature, the detail view for that signature will be shown. This page will show you the following information: ID of the signature (under the signature detail header) The raw signature Statistics about the signature A histogram of the signature for the last 30 days A list of the last ten hits for that signature On the top right, it will also show actions on the signature: You can hit the search button to find all instances where that signature hits in the system Use the red delete button to delete the signature from the system If the signature is still present in the source where it was retrieved, it will be re-added on the next update. In this case, you should disable the signature instead. Change the state of a signature","title":"Signature detail"},{"location":"fr/administration/signature_management/#changing-the-signature-state","text":"Signature states are synced with the source they are coming from but the state in your Assemblyline deployment will supersede the state that the rule updater is trying to set. This means that if you disable a rule in your Assemblyline instance, it will remain disabled even if the source where that rule is from changes. There are three different signature states: Deployed , Noisy , and Disabled Deployed : Deployed will be used for detection and will generate a score depending on how the service handles these types of signatures Noisy: Noisy will be used for detection but rules with these states will not affect the score of the file Disabled: Disabled signatures are completely ignored in the system and the service will not even realize that these signatures exist You can change the signatures by clicking the current signature state in the signature detail view. This will bring up the state-changing modal window which will let you pick a new state for the current rule.","title":"Changing the signature state"},{"location":"fr/administration/source_management/","text":"Signature Source Management \u00b6 Modifying the signature set to support analysis is very simple and can be done directly through the Assemblyline User Interface. You can access the source management interface by selecting \" Source management \" in the navigation menu. The source management interface will list all services that support external sources and will show you the different sources currently configured in the system. Modifying sources \u00b6 With the source management interface, you can add, modify, or delete any sources of any service with the following actions: To add a new source to a given service, you can simply press the \" + \" button beside the service name for which you want to add the source. To modify or delete a source, simply click on the source you want to modify/delete. Both options will bring you to an interface that looks like this: Required input \u00b6 The following sections are required to add/modify a signature source in Assemblyline: URI This is the path to your sources. In this case, we will use a GitHub repository. The URI section also accepts HTTP URLs as input. Source Name This can be labeled at the user\u2019s discretion. For this example, we have used REVERSING_LABS_EXAMPLE. Please note that input for \"Source Name\" must not have any spaces. Optional input \u00b6 Pattern The user may add a regex pattern to pull certain file types for a particular service. In this example, only .yara or .yar files will be added as signatures. Username / Password This is the username and password for the URL or git repository that you are targeting. Private Key If using SSH to connect to GitHub, you must generate a private SSH key and add it to this section. Headers Header name and Header value are for special HTTP headers that may be passed to the HTTP server, such as passing an API key. Alternate methods of updating sources \u00b6 There are alternate ways that the system administrator can use to modify the signatures in the system: Before loading the service into Assemblyline Inside the service management interface Option 1 - Before loading the service \u00b6 The updater can be configured through the service_manifest.yml , which is in the root directory of each service. If you edit the files before pasting them into the system to add that service, the correct signature source(s) will be set once the service is first loaded. Suricata's updater You can find the Suricata updater configuration in its service_manifest.yml file. Its config block looks like this: ... update_config : generates_signatures : true method : run run_options : allow_internet_access : true command : [ \"python\" , \"-m\" , \"suricata_.suricata_updater\" ] image : ${REGISTRY}cccs/assemblyline-service-suricata:$SERVICE_TAG sources : - name : emt pattern : .*\\.rules uri : https://rules.emergingthreats.net/open/suricata/emerging.rules.tar.gz update_interval_seconds : 21600 # Quarter-day (every 6 hours) ... For more information about the update config block, you should check out the update config and update source sections of the service_manifest.yml documentation. Option 2 - Inside the service management interface \u00b6 First navigate to \" User \" -> \" Administration \" -> \" Services \" through the navigation bar: Click on the relevant service that you wish to update. Navigate to the \" Updates \" tab. You can change any value related to the updates in this section. Tip The source update interface in this section is like the Source management page although there are a few added options for: Turning the \"generate signature flag\" on/off Waiting for a valid update or not Setting the updating interval","title":"Signature Source Management"},{"location":"fr/administration/source_management/#signature-source-management","text":"Modifying the signature set to support analysis is very simple and can be done directly through the Assemblyline User Interface. You can access the source management interface by selecting \" Source management \" in the navigation menu. The source management interface will list all services that support external sources and will show you the different sources currently configured in the system.","title":"Signature Source Management"},{"location":"fr/administration/source_management/#modifying-sources","text":"With the source management interface, you can add, modify, or delete any sources of any service with the following actions: To add a new source to a given service, you can simply press the \" + \" button beside the service name for which you want to add the source. To modify or delete a source, simply click on the source you want to modify/delete. Both options will bring you to an interface that looks like this:","title":"Modifying sources"},{"location":"fr/administration/source_management/#required-input","text":"The following sections are required to add/modify a signature source in Assemblyline: URI This is the path to your sources. In this case, we will use a GitHub repository. The URI section also accepts HTTP URLs as input. Source Name This can be labeled at the user\u2019s discretion. For this example, we have used REVERSING_LABS_EXAMPLE. Please note that input for \"Source Name\" must not have any spaces.","title":"Required input"},{"location":"fr/administration/source_management/#optional-input","text":"Pattern The user may add a regex pattern to pull certain file types for a particular service. In this example, only .yara or .yar files will be added as signatures. Username / Password This is the username and password for the URL or git repository that you are targeting. Private Key If using SSH to connect to GitHub, you must generate a private SSH key and add it to this section. Headers Header name and Header value are for special HTTP headers that may be passed to the HTTP server, such as passing an API key.","title":"Optional input"},{"location":"fr/administration/source_management/#alternate-methods-of-updating-sources","text":"There are alternate ways that the system administrator can use to modify the signatures in the system: Before loading the service into Assemblyline Inside the service management interface","title":"Alternate methods of updating sources"},{"location":"fr/administration/source_management/#option-1-before-loading-the-service","text":"The updater can be configured through the service_manifest.yml , which is in the root directory of each service. If you edit the files before pasting them into the system to add that service, the correct signature source(s) will be set once the service is first loaded. Suricata's updater You can find the Suricata updater configuration in its service_manifest.yml file. Its config block looks like this: ... update_config : generates_signatures : true method : run run_options : allow_internet_access : true command : [ \"python\" , \"-m\" , \"suricata_.suricata_updater\" ] image : ${REGISTRY}cccs/assemblyline-service-suricata:$SERVICE_TAG sources : - name : emt pattern : .*\\.rules uri : https://rules.emergingthreats.net/open/suricata/emerging.rules.tar.gz update_interval_seconds : 21600 # Quarter-day (every 6 hours) ... For more information about the update config block, you should check out the update config and update source sections of the service_manifest.yml documentation.","title":"Option 1 - Before loading the service"},{"location":"fr/administration/source_management/#option-2-inside-the-service-management-interface","text":"First navigate to \" User \" -> \" Administration \" -> \" Services \" through the navigation bar: Click on the relevant service that you wish to update. Navigate to the \" Updates \" tab. You can change any value related to the updates in this section. Tip The source update interface in this section is like the Source management page although there are a few added options for: Turning the \"generate signature flag\" on/off Waiting for a valid update or not Setting the updating interval","title":"Option 2 - Inside the service management interface"},{"location":"fr/administration/system_safelist/","text":"System safelist \u00b6 Assemblyline includes a safelisting system that will let you ignore certain tags generated by services. Although safelisting is available to all users throughout the interface, you can specify more complex rules via the administration interface. Editing the safelist \u00b6 You can access the system safelist management page by clicking the \" System Safelist \" menu item in the user dropdown menu. Once the safelist management interface is open you will be greeted with a YAML file in an editor. From here you can edit the YAML directly in the interface and hit the \" Save changes \" button to apply the changes to the system. The system safelist is composed of two sections and each of those sections are composed of tag types with lists of values: match is where you list values for specific tag types that you want to safelist by using a direct string comparison ( == ) regex is where you list regular expressions for specific tag types that you want to safelist by using regular expression matching ( .match() ) Example match : <tag-type> : - <value> regex : <tag-type> : - <regular expression> Default system safelist \u00b6 There is a safelist installed in the system by default which covers some basic cases. These are the tags that are safelisted by default: Default system safelist # Default tag_safelist.yml file # # The following tags are safelisted: # - Domains pointing to localhost # - Domains commonly found in XML files, certificates, and during dynamic analysis runs # - IPs in the private network IP space # - URIs pointing to IPs in the private network IP space # - URIs commonly found in XML files, certificates, and during dynamic analysis runs # # Note: - You can override the default tag_safelist.yml by putting an # updated version in /etc/assemblyline/tag_safelist.yml. # - If you want to add values to one of the following tag types, # you must copy the default values to the new file. # - You can nullify values by putting an empty object or an empty list # in your new file # Match section contains tag types where each tag type is # a list of values that should be safelisted by using a direct # string comparison. match : # Direct match to dynamic domains network.dynamic.domain : - localhost - android.googlesource.com - play.google.com - schemas.android.com - xmlpull.org - schemas.openxmlformats.org - schemas.microsoft.com - settings-win.data.microsoft.com - vortex-win.data.microsoft.com - wpad.reddog.microsoft.com - verisign.com - csc3-2010-crl.verisign.com - csc3-2010-aia.verisign.com - ocsp.verisign.com - logo.verisign.com - crl.verisign.com - ctldl.windowsupdate.com - ns.adobe.com - www.w3.org - purl.org # Direct match to static domains network.static.domain : - localhost - android.googlesource.com - play.google.com - schemas.android.com - xmlpull.org - schemas.openxmlformats.org - schemas.microsoft.com - settings-win.data.microsoft.com - vortex-win.data.microsoft.com - wpad.reddog.microsoft.com - verisign.com - csc3-2010-crl.verisign.com - csc3-2010-aia.verisign.com - ocsp.verisign.com - logo.verisign.com - crl.verisign.com - ctldl.windowsupdate.com - ns.adobe.com - www.w3.org - purl.org # Regex section contains tag types where each tag type is # a list of regular expressions to be run to safelist # the associated tags. regex : # Regular expressions to safelist dynamic IPs (Private IPs) # Note: Since IPs have already been validated, the regular expression is simpler network.dynamic.ip : - (?:127\\.|10\\.|192\\.168|172\\.1[6-9]\\.|172\\.2[0-9]\\.|172\\.3[01]\\.).* # Regular expression to safelist static IPs (Private IPs) # Note: Since IPs have already been validated, the regular expression is simpler network.static.ip : - (?:127\\.|10\\.|192\\.168|172\\.1[6-9]\\.|172\\.2[0-9]\\.|172\\.3[01]\\.).* # Regular expression to safelist dynamic URIs network.dynamic.uri : - (?:ftp|http)s?://localhost(?:$|/.*) - (?:ftp|http)s?://(?:(?:(?:10|127)(?:\\.(?:[2](?:[0-5][0-5]|[01234][6-9])|[1][0-9][0-9]|[1-9][0-9]|[0-9])){3})|(?:172\\.(?:1[6-9]|2[0-9]|3[0-1])(?:\\.(?:2[0-4][0-9]|25[0-5]|[1][0-9][0-9]|[1-9][0-9]|[0-9])){2}|(?:192\\.168(?:\\.(?:25[0-5]|2[0-4][0-9]|1[0-9][0-9]|[1-9][0-9]|[0-9])){2})))(?:$|/.*) - https?://schemas\\.android\\.com/apk/res(-auto|/android) - https?://xmlpull\\.org/v1/doc/features\\.html(?:$|.*) - https?://android\\.googlesource\\.com/toolchain/llvm-project - https?://schemas\\.microsoft\\.com(?:$|/.*) - https?://schemas\\.openxmlformats\\.org(?:$|/.*) - https?://schemas\\.openxmlformats\\.org/officeDocument/2006/relationships/(image|attachedTemplate|header|footnotes|fontTable|customXml|endnotes|theme|settings|webSettings|glossaryDocument|numbering|footer|styles) - https?://schemas\\.microsoft\\.com/office/word/2010/(wordml|wordprocessingCanvas|wordprocessingInk|wordprocessingGroup|wordprocessingDrawing) - https?://schemas\\.microsoft\\.com/office/word/(2012|2006)/wordml - https?://schemas\\.microsoft\\.com/office/word/2015/wordml/symex - https?://schemas\\.microsoft\\.com/office/drawing/2014/chartex - https?://schemas\\.microsoft\\.com/office/drawing/2015/9/8/chartex - https?://schemas\\.openxmlformats\\.org/drawingml/2006/(main|wordprocessingDrawing) - https?://schemas\\.openxmlformats\\.org/package/2006/relationships - https?://schemas\\.openxmlformats\\.org/markup-compatibility/2006 - https?://schemas\\.openxmlformats\\.org/officeDocument/2006/(relationships|math) - https?://schemas\\.openxmlformats\\.org/word/2010/wordprocessingShape - https?://schemas\\.openxmlformats\\.org/wordprocessingml/2006/main - https?://www\\.verisign\\.com/(rpa0|rpa|cps0) - https?://wpad\\.reddog\\.microsoft\\.com/wpad\\.dat - https?://ocsp\\.verisign\\.com - https?://logo\\.verisign\\.com/vslogo\\.gif04 - https?://crl\\.verisign\\.com/pca3-g5\\.crl04 - https?://csc3-2010-crl\\.verisign\\.com/CSC3-2010\\.crl0D - https?://csc3-2010-aia\\.verisign\\.com/CSC3-2010\\.cer0 - https?://ctldl\\.windowsupdate\\.com/.* - https?://ns\\.adobe\\.com/photoshop/1\\.0/ - https?://ns\\.adobe\\.com/xap/1\\.0/ - https?://ns\\.adobe\\.com/xap/1\\.0/mm/ - https?://ns\\.adobe\\.com/xap/1\\.0/sType/ResourceEvent# - https?://purl\\.org/dc/elements/1\\.1/ - https?://www\\.w3\\.org/1999/02/22-rdf-syntax-ns# # Regular expression to safelist static URIs network.static.uri : - (?:ftp|http)s?://localhost(?:$|/.*) - (?:ftp|http)s?://(?:(?:(?:10|127)(?:\\.(?:[2](?:[0-5][0-5]|[01234][6-9])|[1][0-9][0-9]|[1-9][0-9]|[0-9])){3})|(?:172\\.(?:1[6-9]|2[0-9]|3[0-1])(?:\\.(?:2[0-4][0-9]|25[0-5]|[1][0-9][0-9]|[1-9][0-9]|[0-9])){2}|(?:192\\.168(?:\\.(?:25[0-5]|2[0-4][0-9]|1[0-9][0-9]|[1-9][0-9]|[0-9])){2})))(?:$|/.*) - https?://schemas\\.android\\.com/apk/res(-auto|/android) - https?://xmlpull\\.org/v1/doc/features\\.html(?:$|.*) - https?://android\\.googlesource\\.com/toolchain/llvm-project - https?://schemas\\.microsoft\\.com(?:$|/.*) - https?://schemas\\.openxmlformats\\.org(?:$|/.*) - https?://schemas\\.openxmlformats\\.org/officeDocument/2006/relationships/(image|attachedTemplate|header|footnotes|fontTable|customXml|endnotes|theme|settings|webSettings|glossaryDocument|numbering|footer|styles) - https?://schemas\\.microsoft\\.com/office/word/2010/(wordml|wordprocessingCanvas|wordprocessingInk|wordprocessingGroup|wordprocessingDrawing) - https?://schemas\\.microsoft\\.com/office/word/(2012|2006)/wordml - https?://schemas\\.microsoft\\.com/office/word/2015/wordml/symex - https?://schemas\\.microsoft\\.com/office/drawing/2014/chartex - https?://schemas\\.microsoft\\.com/office/drawing/2015/9/8/chartex - https?://schemas\\.openxmlformats\\.org/drawingml/2006/(main|wordprocessingDrawing) - https?://schemas\\.openxmlformats\\.org/package/2006/relationships - https?://schemas\\.openxmlformats\\.org/markup-compatibility/2006 - https?://schemas\\.openxmlformats\\.org/officeDocument/2006/(relationships|math) - https?://schemas\\.openxmlformats\\.org/word/2010/wordprocessingShape - https?://schemas\\.openxmlformats\\.org/wordprocessingml/2006/main - https?://www\\.verisign\\.com/(rpa0|rpa|cps0) - https?://wpad\\.reddog\\.microsoft\\.com/wpad\\.dat - https?://ocsp\\.verisign\\.com - https?://logo\\.verisign\\.com/vslogo\\.gif04 - https?://crl\\.verisign\\.com/pca3-g5\\.crl04 - https?://csc3-2010-crl\\.verisign\\.com/CSC3-2010\\.crl0D - https?://csc3-2010-aia\\.verisign\\.com/CSC3-2010\\.cer0 - https?://ctldl\\.windowsupdate\\.com/.* - https?://ns\\.adobe\\.com/photoshop/1\\.0/ - https?://ns\\.adobe\\.com/xap/1\\.0/ - https?://ns\\.adobe\\.com/xap/1\\.0/mm/ - https?://ns\\.adobe\\.com/xap/1\\.0/sType/ResourceEvent# - https?://purl\\.org/dc/elements/1\\.1/ - https?://www\\.w3\\.org/1999/02/22-rdf-syntax-ns# Tip You can also find the default safelist in the code: tag_safelist.yml","title":"System safelist"},{"location":"fr/administration/system_safelist/#system-safelist","text":"Assemblyline includes a safelisting system that will let you ignore certain tags generated by services. Although safelisting is available to all users throughout the interface, you can specify more complex rules via the administration interface.","title":"System safelist"},{"location":"fr/administration/system_safelist/#editing-the-safelist","text":"You can access the system safelist management page by clicking the \" System Safelist \" menu item in the user dropdown menu. Once the safelist management interface is open you will be greeted with a YAML file in an editor. From here you can edit the YAML directly in the interface and hit the \" Save changes \" button to apply the changes to the system. The system safelist is composed of two sections and each of those sections are composed of tag types with lists of values: match is where you list values for specific tag types that you want to safelist by using a direct string comparison ( == ) regex is where you list regular expressions for specific tag types that you want to safelist by using regular expression matching ( .match() ) Example match : <tag-type> : - <value> regex : <tag-type> : - <regular expression>","title":"Editing the safelist"},{"location":"fr/administration/system_safelist/#default-system-safelist","text":"There is a safelist installed in the system by default which covers some basic cases. These are the tags that are safelisted by default: Default system safelist # Default tag_safelist.yml file # # The following tags are safelisted: # - Domains pointing to localhost # - Domains commonly found in XML files, certificates, and during dynamic analysis runs # - IPs in the private network IP space # - URIs pointing to IPs in the private network IP space # - URIs commonly found in XML files, certificates, and during dynamic analysis runs # # Note: - You can override the default tag_safelist.yml by putting an # updated version in /etc/assemblyline/tag_safelist.yml. # - If you want to add values to one of the following tag types, # you must copy the default values to the new file. # - You can nullify values by putting an empty object or an empty list # in your new file # Match section contains tag types where each tag type is # a list of values that should be safelisted by using a direct # string comparison. match : # Direct match to dynamic domains network.dynamic.domain : - localhost - android.googlesource.com - play.google.com - schemas.android.com - xmlpull.org - schemas.openxmlformats.org - schemas.microsoft.com - settings-win.data.microsoft.com - vortex-win.data.microsoft.com - wpad.reddog.microsoft.com - verisign.com - csc3-2010-crl.verisign.com - csc3-2010-aia.verisign.com - ocsp.verisign.com - logo.verisign.com - crl.verisign.com - ctldl.windowsupdate.com - ns.adobe.com - www.w3.org - purl.org # Direct match to static domains network.static.domain : - localhost - android.googlesource.com - play.google.com - schemas.android.com - xmlpull.org - schemas.openxmlformats.org - schemas.microsoft.com - settings-win.data.microsoft.com - vortex-win.data.microsoft.com - wpad.reddog.microsoft.com - verisign.com - csc3-2010-crl.verisign.com - csc3-2010-aia.verisign.com - ocsp.verisign.com - logo.verisign.com - crl.verisign.com - ctldl.windowsupdate.com - ns.adobe.com - www.w3.org - purl.org # Regex section contains tag types where each tag type is # a list of regular expressions to be run to safelist # the associated tags. regex : # Regular expressions to safelist dynamic IPs (Private IPs) # Note: Since IPs have already been validated, the regular expression is simpler network.dynamic.ip : - (?:127\\.|10\\.|192\\.168|172\\.1[6-9]\\.|172\\.2[0-9]\\.|172\\.3[01]\\.).* # Regular expression to safelist static IPs (Private IPs) # Note: Since IPs have already been validated, the regular expression is simpler network.static.ip : - (?:127\\.|10\\.|192\\.168|172\\.1[6-9]\\.|172\\.2[0-9]\\.|172\\.3[01]\\.).* # Regular expression to safelist dynamic URIs network.dynamic.uri : - (?:ftp|http)s?://localhost(?:$|/.*) - (?:ftp|http)s?://(?:(?:(?:10|127)(?:\\.(?:[2](?:[0-5][0-5]|[01234][6-9])|[1][0-9][0-9]|[1-9][0-9]|[0-9])){3})|(?:172\\.(?:1[6-9]|2[0-9]|3[0-1])(?:\\.(?:2[0-4][0-9]|25[0-5]|[1][0-9][0-9]|[1-9][0-9]|[0-9])){2}|(?:192\\.168(?:\\.(?:25[0-5]|2[0-4][0-9]|1[0-9][0-9]|[1-9][0-9]|[0-9])){2})))(?:$|/.*) - https?://schemas\\.android\\.com/apk/res(-auto|/android) - https?://xmlpull\\.org/v1/doc/features\\.html(?:$|.*) - https?://android\\.googlesource\\.com/toolchain/llvm-project - https?://schemas\\.microsoft\\.com(?:$|/.*) - https?://schemas\\.openxmlformats\\.org(?:$|/.*) - https?://schemas\\.openxmlformats\\.org/officeDocument/2006/relationships/(image|attachedTemplate|header|footnotes|fontTable|customXml|endnotes|theme|settings|webSettings|glossaryDocument|numbering|footer|styles) - https?://schemas\\.microsoft\\.com/office/word/2010/(wordml|wordprocessingCanvas|wordprocessingInk|wordprocessingGroup|wordprocessingDrawing) - https?://schemas\\.microsoft\\.com/office/word/(2012|2006)/wordml - https?://schemas\\.microsoft\\.com/office/word/2015/wordml/symex - https?://schemas\\.microsoft\\.com/office/drawing/2014/chartex - https?://schemas\\.microsoft\\.com/office/drawing/2015/9/8/chartex - https?://schemas\\.openxmlformats\\.org/drawingml/2006/(main|wordprocessingDrawing) - https?://schemas\\.openxmlformats\\.org/package/2006/relationships - https?://schemas\\.openxmlformats\\.org/markup-compatibility/2006 - https?://schemas\\.openxmlformats\\.org/officeDocument/2006/(relationships|math) - https?://schemas\\.openxmlformats\\.org/word/2010/wordprocessingShape - https?://schemas\\.openxmlformats\\.org/wordprocessingml/2006/main - https?://www\\.verisign\\.com/(rpa0|rpa|cps0) - https?://wpad\\.reddog\\.microsoft\\.com/wpad\\.dat - https?://ocsp\\.verisign\\.com - https?://logo\\.verisign\\.com/vslogo\\.gif04 - https?://crl\\.verisign\\.com/pca3-g5\\.crl04 - https?://csc3-2010-crl\\.verisign\\.com/CSC3-2010\\.crl0D - https?://csc3-2010-aia\\.verisign\\.com/CSC3-2010\\.cer0 - https?://ctldl\\.windowsupdate\\.com/.* - https?://ns\\.adobe\\.com/photoshop/1\\.0/ - https?://ns\\.adobe\\.com/xap/1\\.0/ - https?://ns\\.adobe\\.com/xap/1\\.0/mm/ - https?://ns\\.adobe\\.com/xap/1\\.0/sType/ResourceEvent# - https?://purl\\.org/dc/elements/1\\.1/ - https?://www\\.w3\\.org/1999/02/22-rdf-syntax-ns# # Regular expression to safelist static URIs network.static.uri : - (?:ftp|http)s?://localhost(?:$|/.*) - (?:ftp|http)s?://(?:(?:(?:10|127)(?:\\.(?:[2](?:[0-5][0-5]|[01234][6-9])|[1][0-9][0-9]|[1-9][0-9]|[0-9])){3})|(?:172\\.(?:1[6-9]|2[0-9]|3[0-1])(?:\\.(?:2[0-4][0-9]|25[0-5]|[1][0-9][0-9]|[1-9][0-9]|[0-9])){2}|(?:192\\.168(?:\\.(?:25[0-5]|2[0-4][0-9]|1[0-9][0-9]|[1-9][0-9]|[0-9])){2})))(?:$|/.*) - https?://schemas\\.android\\.com/apk/res(-auto|/android) - https?://xmlpull\\.org/v1/doc/features\\.html(?:$|.*) - https?://android\\.googlesource\\.com/toolchain/llvm-project - https?://schemas\\.microsoft\\.com(?:$|/.*) - https?://schemas\\.openxmlformats\\.org(?:$|/.*) - https?://schemas\\.openxmlformats\\.org/officeDocument/2006/relationships/(image|attachedTemplate|header|footnotes|fontTable|customXml|endnotes|theme|settings|webSettings|glossaryDocument|numbering|footer|styles) - https?://schemas\\.microsoft\\.com/office/word/2010/(wordml|wordprocessingCanvas|wordprocessingInk|wordprocessingGroup|wordprocessingDrawing) - https?://schemas\\.microsoft\\.com/office/word/(2012|2006)/wordml - https?://schemas\\.microsoft\\.com/office/word/2015/wordml/symex - https?://schemas\\.microsoft\\.com/office/drawing/2014/chartex - https?://schemas\\.microsoft\\.com/office/drawing/2015/9/8/chartex - https?://schemas\\.openxmlformats\\.org/drawingml/2006/(main|wordprocessingDrawing) - https?://schemas\\.openxmlformats\\.org/package/2006/relationships - https?://schemas\\.openxmlformats\\.org/markup-compatibility/2006 - https?://schemas\\.openxmlformats\\.org/officeDocument/2006/(relationships|math) - https?://schemas\\.openxmlformats\\.org/word/2010/wordprocessingShape - https?://schemas\\.openxmlformats\\.org/wordprocessingml/2006/main - https?://www\\.verisign\\.com/(rpa0|rpa|cps0) - https?://wpad\\.reddog\\.microsoft\\.com/wpad\\.dat - https?://ocsp\\.verisign\\.com - https?://logo\\.verisign\\.com/vslogo\\.gif04 - https?://crl\\.verisign\\.com/pca3-g5\\.crl04 - https?://csc3-2010-crl\\.verisign\\.com/CSC3-2010\\.crl0D - https?://csc3-2010-aia\\.verisign\\.com/CSC3-2010\\.cer0 - https?://ctldl\\.windowsupdate\\.com/.* - https?://ns\\.adobe\\.com/photoshop/1\\.0/ - https?://ns\\.adobe\\.com/xap/1\\.0/ - https?://ns\\.adobe\\.com/xap/1\\.0/mm/ - https?://ns\\.adobe\\.com/xap/1\\.0/sType/ResourceEvent# - https?://purl\\.org/dc/elements/1\\.1/ - https?://www\\.w3\\.org/1999/02/22-rdf-syntax-ns# Tip You can also find the default safelist in the code: tag_safelist.yml","title":"Default system safelist"},{"location":"fr/administration/troubleshooting/","text":"Troubleshooting / FAQ \u00b6 We will update this page with typical issues and solutions. You can post your question to our Assemblyline Google Group or join our Discord community! Core Statistics Why is the hits counter for a certain signature not incrementing even though it hit 5000 times in the last hour? Rules' hit count are not calculated live. There is an external process that calculates them daily for performance optimization. However, you can click on the rule itself and it does calculate the stats for that specific rule and updates it right away. Updater Failed to establish a new connection: [Errno 110] Connection timed out Depending on the host mentioned in the error, ensure the deployment has access to the registry and its able to call the associated API. The following modifications will have to be made to your values.yaml: External Registry Let's say the domain of the registry is 'registry.local' and is hosted on port 443 configuration : services : image_variables : REGISTRY : \"registry.local/\" allow_insecure_registry : true Local Registry Let's say the local registry is hosted on port 32000 configuration : services : image_variables : REGISTRY : \"localhost:32000/\" update_image_variables : REGISTRY : \"<HOST_IP>:32000/\" allow_insecure_registry : true Kubernetes Connection timeouts to external domains By default, coreDNS is configured to the Google's Public DNS when trying to resolve external domains outside the cluster. You can configure it to use a different DNS via: sudo microk8s disable dns && sudo microk8s enabled dns:1.1.1.1 If this still poses an issue, refer to: Teleport - Troubleshooting Kubernetes Networking Issues How can I monitor the status of the deployment/cluster? The quickest way to monitor the status of your cluster is: kubectl get pods -n <assemblyline_namespace> There are other tools such as k9s and Lens that allow you to monitor your cluster in a more user-friendly manner. Are HPAs adjustable? Depending on the amount of activity you're receiving, you'll likely have to tweak the TargetUsage and ReqRam settings in your values.yaml for your particular deployment, either to cause it to scale faster or slower. Refer to: Kubernetes - Horizontal Pod Autoscaler for more information. NGINX 504 but everything seems to be running It's possible the domain you're accessing the UI with doesn't match the setting configuration.ui.fqdn in your values.yaml. If your setting is set to 'localhost' but you're accessing the UI using '192.0.0.1.nip.io', there is no ingress path using '192.0.0.1.nip.io' as a base. The simplest solution is to update your values.yaml to the appropriate FQDN and redeploy. Is it possible to mount an internal root CA bundle into core components to use? Yes! This would involve creating a configmap containing your CA bundle and using coreVolumes, coreMounts, and coreEnv in your values.yaml to pass that information onto the core deployments. An example of this configuration would be: coreEnv : - name : REQUESTS_CA_BUNDLE value : /usr/share/internal-ca coreMounts : - name : internal-certs mountPath : /usr/share/internal-ca coreVolumes : - name : internal-certs configMap : name : internal-certs-config Services General TASK PRE-EMPTED A service task can pre-empt for a number of reasons, such as: Container being killed due to OOM (Out of Memory) Increase service memory limits Debug service memory usage with sample Service instance ran beyond the service timeout Increase service timeout Debug service runtime with sample The service result didn't make it back to service-server Check the state of service-server deployments Inspect network policies Is there a limit resources to allocate to a service instance? No! The sky's the limit (or more accurately), you're bound to the resources allocated on your cluster. That being said, it's best practice to use what you need rather than what you want (especially if it's unwarranted). Can I deploy a 4.X service on a 4.Y Assemblyline system (where X,Y are system versions: X < Y) Yes and no. You can add a service with to the system with a lesser system version but it won't be enabled due to potential compatibility issues. It's advised to rebuild the service and tag with the system version that you want to deploy on. Service Updater My service doesn't seem to be getting any signatures for analysis.. Check the following on your service's updater instance: Does your service have the following environment variables set: updates_host, updates_port, updates_key Edit the deployment manually Disable the service, delete the related deployments from Kubernetes, restart scaler, then re-enable service Is there any downloaded signatures in /tmp/updater/update_dir*/*/ of the updater container? If empty, it suggests there was a problem pulling the signatures from Assemblyline If some sources are missing, it suggests there was a problem pulling signatures from that source I've added my signature sources, but nothing shows up when searching the Signature index.. Are the source links accessible from the cluster? Is the authentication for each source valid? How frequently is the service updater configured to check for source updates? If I modify signatures using the Assemblyline client or the API, will the service get these rules? Yes! Changes made involving our API trigger messaging events made to other components to Assemblyline causing the system to be more responsive. In the case of services with updaters, they'll be notified immediately and the service will gather the new rules after a submissions has processed.","title":"Troubleshooting / FAQ"},{"location":"fr/administration/troubleshooting/#troubleshooting-faq","text":"We will update this page with typical issues and solutions. You can post your question to our Assemblyline Google Group or join our Discord community! Core Statistics Why is the hits counter for a certain signature not incrementing even though it hit 5000 times in the last hour? Rules' hit count are not calculated live. There is an external process that calculates them daily for performance optimization. However, you can click on the rule itself and it does calculate the stats for that specific rule and updates it right away. Updater Failed to establish a new connection: [Errno 110] Connection timed out Depending on the host mentioned in the error, ensure the deployment has access to the registry and its able to call the associated API. The following modifications will have to be made to your values.yaml: External Registry Let's say the domain of the registry is 'registry.local' and is hosted on port 443 configuration : services : image_variables : REGISTRY : \"registry.local/\" allow_insecure_registry : true Local Registry Let's say the local registry is hosted on port 32000 configuration : services : image_variables : REGISTRY : \"localhost:32000/\" update_image_variables : REGISTRY : \"<HOST_IP>:32000/\" allow_insecure_registry : true Kubernetes Connection timeouts to external domains By default, coreDNS is configured to the Google's Public DNS when trying to resolve external domains outside the cluster. You can configure it to use a different DNS via: sudo microk8s disable dns && sudo microk8s enabled dns:1.1.1.1 If this still poses an issue, refer to: Teleport - Troubleshooting Kubernetes Networking Issues How can I monitor the status of the deployment/cluster? The quickest way to monitor the status of your cluster is: kubectl get pods -n <assemblyline_namespace> There are other tools such as k9s and Lens that allow you to monitor your cluster in a more user-friendly manner. Are HPAs adjustable? Depending on the amount of activity you're receiving, you'll likely have to tweak the TargetUsage and ReqRam settings in your values.yaml for your particular deployment, either to cause it to scale faster or slower. Refer to: Kubernetes - Horizontal Pod Autoscaler for more information. NGINX 504 but everything seems to be running It's possible the domain you're accessing the UI with doesn't match the setting configuration.ui.fqdn in your values.yaml. If your setting is set to 'localhost' but you're accessing the UI using '192.0.0.1.nip.io', there is no ingress path using '192.0.0.1.nip.io' as a base. The simplest solution is to update your values.yaml to the appropriate FQDN and redeploy. Is it possible to mount an internal root CA bundle into core components to use? Yes! This would involve creating a configmap containing your CA bundle and using coreVolumes, coreMounts, and coreEnv in your values.yaml to pass that information onto the core deployments. An example of this configuration would be: coreEnv : - name : REQUESTS_CA_BUNDLE value : /usr/share/internal-ca coreMounts : - name : internal-certs mountPath : /usr/share/internal-ca coreVolumes : - name : internal-certs configMap : name : internal-certs-config Services General TASK PRE-EMPTED A service task can pre-empt for a number of reasons, such as: Container being killed due to OOM (Out of Memory) Increase service memory limits Debug service memory usage with sample Service instance ran beyond the service timeout Increase service timeout Debug service runtime with sample The service result didn't make it back to service-server Check the state of service-server deployments Inspect network policies Is there a limit resources to allocate to a service instance? No! The sky's the limit (or more accurately), you're bound to the resources allocated on your cluster. That being said, it's best practice to use what you need rather than what you want (especially if it's unwarranted). Can I deploy a 4.X service on a 4.Y Assemblyline system (where X,Y are system versions: X < Y) Yes and no. You can add a service with to the system with a lesser system version but it won't be enabled due to potential compatibility issues. It's advised to rebuild the service and tag with the system version that you want to deploy on. Service Updater My service doesn't seem to be getting any signatures for analysis.. Check the following on your service's updater instance: Does your service have the following environment variables set: updates_host, updates_port, updates_key Edit the deployment manually Disable the service, delete the related deployments from Kubernetes, restart scaler, then re-enable service Is there any downloaded signatures in /tmp/updater/update_dir*/*/ of the updater container? If empty, it suggests there was a problem pulling the signatures from Assemblyline If some sources are missing, it suggests there was a problem pulling signatures from that source I've added my signature sources, but nothing shows up when searching the Signature index.. Are the source links accessible from the cluster? Is the authentication for each source valid? How frequently is the service updater configured to check for source updates? If I modify signatures using the Assemblyline client or the API, will the service get these rules? Yes! Changes made involving our API trigger messaging events made to other components to Assemblyline causing the system to be more responsive. In the case of services with updaters, they'll be notified immediately and the service will gather the new rules after a submissions has processed.","title":"Troubleshooting / FAQ"},{"location":"fr/administration/user_management/","text":"User Management \u00b6 Assemblyline's user management interface lets you: List all the users in the system Filter and search the current user list View details about users Remove users Enable/disable users Change the users' roles, quotas, and passwords You can find the user management interface by clicking the User Avatar then choose Users from the administrator menu. User list \u00b6 The first page you will be taken to when loading the user management interface will list all the users of the system. From this interface you can: Page through the different users of the system Filter the displayed users with the search bar Assemblyline users can be searched using a Lucene query. As you start typing in the search box, the system will suggest to you fields that you can search into. You can also use the quick filter buttons for pre-defined searches. These pre-defined searches will help you get started writing more complex user searches. Add users to the system using the top right \" Add User \" button View the detail of a user User detail \u00b6 Once you click on a user in the user list, the detail view for that user will be shown. This page will show you the following information: Identity of the user (username, avatar, name, groups, email address) Roles of the user Allowed quotas for the user Status of the user From this page, you will be able to take the following actions on the current user: Remove it Enable/disable access its access to the system Set its roles Set its quotas Change its password","title":"User Management"},{"location":"fr/administration/user_management/#user-management","text":"Assemblyline's user management interface lets you: List all the users in the system Filter and search the current user list View details about users Remove users Enable/disable users Change the users' roles, quotas, and passwords You can find the user management interface by clicking the User Avatar then choose Users from the administrator menu.","title":"User Management"},{"location":"fr/administration/user_management/#user-list","text":"The first page you will be taken to when loading the user management interface will list all the users of the system. From this interface you can: Page through the different users of the system Filter the displayed users with the search bar Assemblyline users can be searched using a Lucene query. As you start typing in the search box, the system will suggest to you fields that you can search into. You can also use the quick filter buttons for pre-defined searches. These pre-defined searches will help you get started writing more complex user searches. Add users to the system using the top right \" Add User \" button View the detail of a user","title":"User list"},{"location":"fr/administration/user_management/#user-detail","text":"Once you click on a user in the user list, the detail view for that user will be shown. This page will show you the following information: Identity of the user (username, avatar, name, groups, email address) Roles of the user Allowed quotas for the user Status of the user From this page, you will be able to take the following actions on the current user: Remove it Enable/disable access its access to the system Set its roles Set its quotas Change its password","title":"User detail"},{"location":"fr/developer_manual/core/infrastructure/","text":"Infrastructure \u00b6 This section gives you a complete overview of all the different components of Assemblyline, so you have a better idea of what they are used for. Dependencies \u00b6 The components listed here are external dependencies used by Assemblyline: Dependency Description Docker Docker is now at the heart of Assemblyline because all Assemblyline's components are now running as Docker containers. https://www.docker.com/ Kubernetes For multi-computer installations, Assemblyline uses Kubernetes to deploy the different Docker containers and keep them healthy. https://kubernetes.io/ Helm Helm is used to easily deploy and maintain our Kubernetes instance. https://helm.sh/ Elastic Stack Assemblyline uses the full Elastic stack to store results, logs, metrics, and APMs. It consists of the following components: https://www.elastic.co/elastic-stack Elasticsearch Elasticsearch is used for storing results, logs, and metrics of the system. It also provides search capability to Assemblyline. Kibana (optional) Provides dashboards to monitor your Assemblyline cluster. APM (optional) Gather Application Performance Metrics so that we can pinpoint potential performance issues with the system and fix them. Filebeat (optional) Gather all the logs for the different components into Elasticsearch to be displayed in Kibana. Metricbeat (optional) Gather metrics for the different hosts where the Docker containers are run. Redis We are using Redis for the queueing system, for messaging between the components, and as a remote data structure to keep multiple instances of a given component working in sync. https://redis.io/ Nginx Nginx is used by Assemblyline as a proxy to give the user access to the different user-facing components: UI, API, Socket Server, Kibana. https://www.nginx.com/ Minio For our default file storage, we use Minio which perfectly replicates the Amazon S3 API and is built to work with Kubernetes. https://min.io/ Core Components \u00b6 The components listed here are Assemblyline-made processes that perform various tasks in the system: Core components Description Link Alerter Create alerts for the different submissions in the system. Source code Dispatcher Route the files in the system while a submission is taking place. Make sure all files during a submission are completed by all required services. Source code Expiry Delete submissions and their results when their TTL (Time-to-live) expires. Source code Frontend Provides the user interface to interact with Assemblyline. Source code Ingester Move ingested files from the priority queues to the processing queues. Source code Metrics Aggregator Aggregate metrics of the different components in the system to save them into an ELK (Elasticsearch-Logstash-Kibana) stack. Source code Metrics Heartbeat Provide live metrics in the system for the dashboard. Source code Scaler Spin up and down services in the system depending on the load. Source code Statistics Aggregator Generate daily statistics about signatures and heuristics. Source code Updater Make sure the different services get their latest update files. Source code Workflow Run the different workflows in the system and apply their labels, priority, and status. Source code Service Server Provides an API for services to get tasks and post their results. Source code UI / Socket Server Provides the APIs and a socket.io interface to interact with Assemblyline. Source code Service interfaces \u00b6 The interfaces listed here are used by Assemblyline's services to process files, generate results, and communicate back to the Service Server: Service Interface Description Link Python 2 Compatibility Library A library that gives services Python 2 compatibility. Source code Result Class used by a service to generate results in the system. Source code Service Base Base Assemblyline service class. Source code Service Request Class that defines a request to scan a file for a given service. Source code Task Handler A Python wrapper that communicates with the service server to get a task, download files, and publish results. It communicates with the service via named pipes so that the service can execute the received tasks. Source code","title":"Infrastructure"},{"location":"fr/developer_manual/core/infrastructure/#infrastructure","text":"This section gives you a complete overview of all the different components of Assemblyline, so you have a better idea of what they are used for.","title":"Infrastructure"},{"location":"fr/developer_manual/core/infrastructure/#dependencies","text":"The components listed here are external dependencies used by Assemblyline: Dependency Description Docker Docker is now at the heart of Assemblyline because all Assemblyline's components are now running as Docker containers. https://www.docker.com/ Kubernetes For multi-computer installations, Assemblyline uses Kubernetes to deploy the different Docker containers and keep them healthy. https://kubernetes.io/ Helm Helm is used to easily deploy and maintain our Kubernetes instance. https://helm.sh/ Elastic Stack Assemblyline uses the full Elastic stack to store results, logs, metrics, and APMs. It consists of the following components: https://www.elastic.co/elastic-stack Elasticsearch Elasticsearch is used for storing results, logs, and metrics of the system. It also provides search capability to Assemblyline. Kibana (optional) Provides dashboards to monitor your Assemblyline cluster. APM (optional) Gather Application Performance Metrics so that we can pinpoint potential performance issues with the system and fix them. Filebeat (optional) Gather all the logs for the different components into Elasticsearch to be displayed in Kibana. Metricbeat (optional) Gather metrics for the different hosts where the Docker containers are run. Redis We are using Redis for the queueing system, for messaging between the components, and as a remote data structure to keep multiple instances of a given component working in sync. https://redis.io/ Nginx Nginx is used by Assemblyline as a proxy to give the user access to the different user-facing components: UI, API, Socket Server, Kibana. https://www.nginx.com/ Minio For our default file storage, we use Minio which perfectly replicates the Amazon S3 API and is built to work with Kubernetes. https://min.io/","title":"Dependencies"},{"location":"fr/developer_manual/core/infrastructure/#core-components","text":"The components listed here are Assemblyline-made processes that perform various tasks in the system: Core components Description Link Alerter Create alerts for the different submissions in the system. Source code Dispatcher Route the files in the system while a submission is taking place. Make sure all files during a submission are completed by all required services. Source code Expiry Delete submissions and their results when their TTL (Time-to-live) expires. Source code Frontend Provides the user interface to interact with Assemblyline. Source code Ingester Move ingested files from the priority queues to the processing queues. Source code Metrics Aggregator Aggregate metrics of the different components in the system to save them into an ELK (Elasticsearch-Logstash-Kibana) stack. Source code Metrics Heartbeat Provide live metrics in the system for the dashboard. Source code Scaler Spin up and down services in the system depending on the load. Source code Statistics Aggregator Generate daily statistics about signatures and heuristics. Source code Updater Make sure the different services get their latest update files. Source code Workflow Run the different workflows in the system and apply their labels, priority, and status. Source code Service Server Provides an API for services to get tasks and post their results. Source code UI / Socket Server Provides the APIs and a socket.io interface to interact with Assemblyline. Source code","title":"Core Components"},{"location":"fr/developer_manual/core/infrastructure/#service-interfaces","text":"The interfaces listed here are used by Assemblyline's services to process files, generate results, and communicate back to the Service Server: Service Interface Description Link Python 2 Compatibility Library A library that gives services Python 2 compatibility. Source code Result Class used by a service to generate results in the system. Source code Service Base Base Assemblyline service class. Source code Service Request Class that defines a request to scan a file for a given service. Source code Task Handler A Python wrapper that communicates with the service server to get a task, download files, and publish results. It communicates with the service via named pipes so that the service can execute the received tasks. Source code","title":"Service interfaces"},{"location":"fr/developer_manual/docs/docs/","text":"Updating documentation \u00b6 This documentation is built using the awesome project: mkdocs-material Clone the documentation \u00b6 The documentation can be cloned from the following repository: git clone git@github.com:CybercentreCanada/assemblyline4_docs.git Install dependencies \u00b6 Create a virtual environment for your documentation in the /venv/ directory right in the assemblyline4_docs source and install mkdocs dependencies: cd assemblyline4_docs python -m venv venv pip install mkdocs-material == 7 .3.6 pip install mkdocs-static-i18n Run the documentation locally \u00b6 You can manually run the documentation from a shell: cd assemblyline4_docs mkdocs serve Alternatively, if you are using VSCode, you can launch the pre-configured task: Launch Assemblyline Documentation","title":"Documentation"},{"location":"fr/developer_manual/docs/docs/#updating-documentation","text":"This documentation is built using the awesome project: mkdocs-material","title":"Updating documentation"},{"location":"fr/developer_manual/docs/docs/#clone-the-documentation","text":"The documentation can be cloned from the following repository: git clone git@github.com:CybercentreCanada/assemblyline4_docs.git","title":"Clone the documentation"},{"location":"fr/developer_manual/docs/docs/#install-dependencies","text":"Create a virtual environment for your documentation in the /venv/ directory right in the assemblyline4_docs source and install mkdocs dependencies: cd assemblyline4_docs python -m venv venv pip install mkdocs-material == 7 .3.6 pip install mkdocs-static-i18n","title":"Install dependencies"},{"location":"fr/developer_manual/docs/docs/#run-the-documentation-locally","text":"You can manually run the documentation from a shell: cd assemblyline4_docs mkdocs serve Alternatively, if you are using VSCode, you can launch the pre-configured task: Launch Assemblyline Documentation","title":"Run the documentation locally"},{"location":"fr/developer_manual/env/getting_started/","text":"Getting Started \u00b6 Before starting to develop for Assemblyline, you will need to set up your environment. We have a couple of options to assist you, from a free-to-use and easy-to-setup script to more complex setups. Development virtual machine \u00b6 Whether you are developing a new service or working on core components, Assemblyline requires specific external packages and files installed in specific directories in the containers. For this reason, it is recommended that you do not develop directly from your desktop unless you use remote debugging features. The minimum specifications for development virtual machine \u00b6 There are quite a few containers to run to spin-up the Assemblyline dependencies. For this reason, the development VM should have at least the following specifications: 2 cores 8 GB of RAM 40 GB of disk space Operating system \u00b6 We recommend that you use Ubuntu 20.04 for your development VM, because all instructions that we provide in the document has been built with this OS in mind. There are two versions that you can pick from: Ubuntu 20.04 Desktop - For local development where your IDE runs in the same VM as the Assemblyline containers (Definitely the easiest setup) Ubuntu 20.04 Server - For remote development where your IDE runs on your local computer and the Assemblyline containers run on the VM Choosing your IDE \u00b6 We have two IDEs for you to pick from, both of which support local and remote development: VSCode \u00b6 This is our recommended IDE since it is free and very easy to setup. Most of the Assemblyline team has moved to this IDE and we use a mix of local and remote development with it. Once you're done installing your VM, you can follow the instructions to get VSCode up and running using our simple setup script . For instructions on how to use VSCode, refer to the \" use VSCode \" documentation. PyCharm \u00b6 This IDE is much more robust in terms Python development but the setup is more complex. There is both a paid and free version of this IDE. The free Community edition of PyCharm will allow you to do local develpment only and you will have to run the dependencies by hand using docker-compose . The paid version, PyCharm Professional, will let you manage Docker dependencies inside the IDE and utilize remote development. Here are the installation instructions for the different setups: Local development (minimum requirement: PyCharm Community edition) Remote development (minimum requirement: PyCharm Professional edition) For instructions on how to use PyCharm, refer to the \" use PyCharm \" documentation.","title":"Getting Started"},{"location":"fr/developer_manual/env/getting_started/#getting-started","text":"Before starting to develop for Assemblyline, you will need to set up your environment. We have a couple of options to assist you, from a free-to-use and easy-to-setup script to more complex setups.","title":"Getting Started"},{"location":"fr/developer_manual/env/getting_started/#development-virtual-machine","text":"Whether you are developing a new service or working on core components, Assemblyline requires specific external packages and files installed in specific directories in the containers. For this reason, it is recommended that you do not develop directly from your desktop unless you use remote debugging features.","title":"Development virtual machine"},{"location":"fr/developer_manual/env/getting_started/#the-minimum-specifications-for-development-virtual-machine","text":"There are quite a few containers to run to spin-up the Assemblyline dependencies. For this reason, the development VM should have at least the following specifications: 2 cores 8 GB of RAM 40 GB of disk space","title":"The minimum specifications for development virtual machine"},{"location":"fr/developer_manual/env/getting_started/#operating-system","text":"We recommend that you use Ubuntu 20.04 for your development VM, because all instructions that we provide in the document has been built with this OS in mind. There are two versions that you can pick from: Ubuntu 20.04 Desktop - For local development where your IDE runs in the same VM as the Assemblyline containers (Definitely the easiest setup) Ubuntu 20.04 Server - For remote development where your IDE runs on your local computer and the Assemblyline containers run on the VM","title":"Operating system"},{"location":"fr/developer_manual/env/getting_started/#choosing-your-ide","text":"We have two IDEs for you to pick from, both of which support local and remote development:","title":"Choosing your IDE"},{"location":"fr/developer_manual/env/getting_started/#vscode","text":"This is our recommended IDE since it is free and very easy to setup. Most of the Assemblyline team has moved to this IDE and we use a mix of local and remote development with it. Once you're done installing your VM, you can follow the instructions to get VSCode up and running using our simple setup script . For instructions on how to use VSCode, refer to the \" use VSCode \" documentation.","title":"VSCode"},{"location":"fr/developer_manual/env/getting_started/#pycharm","text":"This IDE is much more robust in terms Python development but the setup is more complex. There is both a paid and free version of this IDE. The free Community edition of PyCharm will allow you to do local develpment only and you will have to run the dependencies by hand using docker-compose . The paid version, PyCharm Professional, will let you manage Docker dependencies inside the IDE and utilize remote development. Here are the installation instructions for the different setups: Local development (minimum requirement: PyCharm Community edition) Remote development (minimum requirement: PyCharm Professional edition) For instructions on how to use PyCharm, refer to the \" use PyCharm \" documentation.","title":"PyCharm"},{"location":"fr/developer_manual/env/pycharm/local_development/","text":"Local development \u00b6 This documentation will show you how to set up your development virtual machine for local development using the PyCharm Community Edition IDE (This would also work with PyCharm Professional). In this setup, you will run your IDE inside the virtual machine where the Assemblyline containers are running. This is by far the easiest setup to get PyCharm working and removes a lot of headaches. Operating system \u00b6 For this documentation, we will assume that you are working on a fresh installation of Ubuntu 20.04 Desktop . Update VM \u00b6 Make sure Ubuntu is running the latest software sudo apt update sudo apt dist-upgrade sudo snap refresh Reboot if needed sudo reboot Installing pre-requisite software \u00b6 Install Assemblyline APT dependencies \u00b6 sudo apt update sudo apt-get install -yy libfuzzy2 libmagic1 libldap-2.4-2 libsasl2-2 build-essential libffi-dev libfuzzy-dev libldap2-dev libsasl2-dev libssl-dev Install Python 3.9 \u00b6 Assemblyline 4 containers are now all built on Python 3.9 therefore we will install Python 3.9. sudo apt install -y software-properties-common sudo add-apt-repository -y ppa:deadsnakes/ppa sudo apt-get install -yy python3-venv python3.9 python3.9-dev python3.9-venv libffi7 Installing Docker \u00b6 Follow these simple commands to get Docker running on your machine: # Add Docker repository sudo apt-get update sudo apt-get install -y apt-transport-https ca-certificates curl gnupg-agent software-properties-common curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - sudo apt-key fingerprint 0EBFCD88 sudo add-apt-repository \"deb [arch=amd64] https://download.docker.com/linux/ubuntu $( lsb_release -cs ) stable\" # Install Docker sudo apt-get install -y docker-ce docker-ce-cli containerd.io # Test Docker installation sudo docker run hello-world Installing docker-compose \u00b6 Installing docker-compose is done the same way on all Linux distros. Follow these simple instructions: # Install docker-compose sudo curl -L \"https://github.com/docker/compose/releases/download/1.28.5/docker-compose- $( uname -s ) - $( uname -m ) \" -o /usr/local/bin/docker-compose sudo chmod +x /usr/local/bin/docker-compose # Test docker-compose installation docker-compose --version Installing PyCharm \u00b6 Let's install the desired PyCharm version. The Professional version is not needed but if you have a licence you can use it. PyCharm Community sudo snap install --classic pycharm-community PyCharm Professional sudo snap install --classic pycharm-professional Adding Assemblyline specific configuration \u00b6 Data directories \u00b6 Because Assemblyline uses its own set of folders inside the core, service-server, and UI containers, we must create the same folder structure here so we can run the components in debug mode. sudo mkdir -p /etc/assemblyline sudo mkdir -p /var/cache/assemblyline sudo mkdir -p /var/lib/assemblyline sudo mkdir -p /var/log/assemblyline sudo chown $USER /etc/assemblyline sudo chown $USER /var/cache/assemblyline sudo chown $USER /var/lib/assemblyline sudo chown $USER /var/log/assemblyline Dev default configuration files \u00b6 Here we will create configuration files that match the default dev docker-compose configuration files so that we can swap any of the components to the one that is being debugged. echo \"enforce: true\" > /etc/assemblyline/classification.yml echo \" auth: internal: enabled: true core: alerter: delay: 0 metrics: apm_server: server_url: http://localhost:8200/ elasticsearch: hosts: [http://elastic:devpass@localhost] datastore: ilm: indexes: alert: unit: m error: unit: m file: unit: m result: unit: m submission: unit: m filestore: cache: - file:///var/cache/assemblyline/ logging: log_level: INFO log_as_json: false ui: audit: false debug: false enforce_quota: false fqdn: 127.0.0.1.nip.io \" > /etc/assemblyline/config.yml Tip As you can see in the last command we are setting the FQDN to 127.0.0.1.nip.io. NIP.IO is a service that will resolve the first part of the domain 127.0.0.1 .nip.io to its IP value. We use this to fake DNS when there are none. This is especially useful for oAuth because some providers are forbidding redirect URLs to IPs. Setup Assemblyline source \u00b6 Install git \u00b6 Since your VM is running Ubuntu 20.04 you can just install it with APT: sudo apt install -y git Tip You should add your desktop SSH keys to your GitHub account to use Git via SSH. Follow these instructions to do so: GitHub Help Clone core repositories \u00b6 Create the core working directory mkdir -p ~/git/alv4 cd ~/git/alv4 Clone Assemblyline's repositories Git via SSH Use SSH if you have your SSH id_rsa file configured to your GitHub account git clone git@github.com:CybercentreCanada/assemblyline-base.git git clone git@github.com:CybercentreCanada/assemblyline-core.git git clone git@github.com:CybercentreCanada/assemblyline-service-client.git git clone git@github.com:CybercentreCanada/assemblyline-service-server.git git clone git@github.com:CybercentreCanada/assemblyline-ui.git git clone git@github.com:CybercentreCanada/assemblyline-v4-service.git Git via HTTPS Use HTTPS if you don't have your GitHub account configured with an SSH key git clone https://github.com/CybercentreCanada/assemblyline-base.git git clone https://github.com/CybercentreCanada/assemblyline-core.git git clone https://github.com/CybercentreCanada/assemblyline-service-client.git git clone https://github.com/CybercentreCanada/assemblyline-service-server.git git clone https://github.com/CybercentreCanada/assemblyline-ui.git git clone https://github.com/CybercentreCanada/assemblyline-v4-service.git Virtual Environment \u00b6 # Directly in the alv4 source directory cd ~/git/alv4 # Create the virtualenv python3.9 -m venv venv # Install Assemblyline packages with their test dependencies ~/git/alv4/venv/bin/pip install assemblyline [ test ] assemblyline-core [ test ] assemblyline-service-server [ test ] assemblyline-ui [ test ] # Remove Assemblyline packages because we will use the live code ~/git/alv4/venv/bin/pip uninstall -y assemblyline assemblyline-core assemblyline-service-server assemblyline-ui Setting up Services (Optional) \u00b6 If you plan on doing service development in PyCharm you will need a dedicated directory for services with its own virtual environment. Clone service repositories \u00b6 Create the service working directory mkdir -p ~/git/services cd ~/git/services Clone Assemblyline's services repositories Git via SSH Use SSH if you have your SSH id_rsa file configured to your GitHub account git clone git@github.com:CybercentreCanada/assemblyline-service-antivirus.git git clone git@github.com:CybercentreCanada/assemblyline-service-apkaye.git git clone git@github.com:CybercentreCanada/assemblyline-service-avclass.git git clone git@github.com:CybercentreCanada/assemblyline-service-beaver.git git clone git@github.com:CybercentreCanada/assemblyline-service-characterize.git git clone git@github.com:CybercentreCanada/assemblyline-service-configextractor.git git clone git@github.com:CybercentreCanada/assemblyline-service-cuckoo.git git clone git@github.com:CybercentreCanada/assemblyline-service-deobfuscripter.git git clone git@github.com:CybercentreCanada/assemblyline-service-emlparser.git git clone git@github.com:CybercentreCanada/assemblyline-service-espresso.git git clone git@github.com:CybercentreCanada/assemblyline-service-extract.git git clone git@github.com:CybercentreCanada/assemblyline-service-floss.git git clone git@github.com:CybercentreCanada/assemblyline-service-frankenstrings.git git clone git@github.com:CybercentreCanada/assemblyline-service-iparse.git git clone git@github.com:CybercentreCanada/assemblyline-service-metadefender.git git clone git@github.com:CybercentreCanada/assemblyline-service-metapeek.git git clone git@github.com:CybercentreCanada/assemblyline-service-oletools.git git clone git@github.com:CybercentreCanada/assemblyline-service-pdfid.git git clone git@github.com:CybercentreCanada/assemblyline-service-peepdf.git git clone git@github.com:CybercentreCanada/assemblyline-service-pefile.git git clone git@github.com:CybercentreCanada/assemblyline-service-pixaxe.git git clone git@github.com:CybercentreCanada/assemblyline-service-safelist.git git clone git@github.com:CybercentreCanada/assemblyline-service-sigma.git git clone git@github.com:CybercentreCanada/assemblyline-service-suricata.git git clone git@github.com:CybercentreCanada/assemblyline-service-swiffer.git git clone git@github.com:CybercentreCanada/assemblyline-service-torrentslicer.git git clone git@github.com:CybercentreCanada/assemblyline-service-unpacker.git git clone git@github.com:CybercentreCanada/assemblyline-service-unpacme.git git clone git@github.com:CybercentreCanada/assemblyline-service-vipermonkey.git git clone git@github.com:CybercentreCanada/assemblyline-service-virustotal-dynamic.git git clone git@github.com:CybercentreCanada/assemblyline-service-virustotal-static.git git clone git@github.com:CybercentreCanada/assemblyline-service-XLMMacroDeobfuscator.git git clone git@github.com:CybercentreCanada/assemblyline-service-yara.git Git via HTTPS Use HTTPS if you don't have your GitHub account configured with an SSH key git clone https://github.com/CybercentreCanada/assemblyline-service-antivirus.git git clone https://github.com/CybercentreCanada/assemblyline-service-apkaye.git git clone https://github.com/CybercentreCanada/assemblyline-service-avclass.git git clone https://github.com/CybercentreCanada/assemblyline-service-beaver.git git clone https://github.com/CybercentreCanada/assemblyline-service-characterize.git git clone https://github.com/CybercentreCanada/assemblyline-service-configextractor.git git clone https://github.com/CybercentreCanada/assemblyline-service-cuckoo.git git clone https://github.com/CybercentreCanada/assemblyline-service-deobfuscripter.git git clone https://github.com/CybercentreCanada/assemblyline-service-emlparser.git git clone https://github.com/CybercentreCanada/assemblyline-service-espresso.git git clone https://github.com/CybercentreCanada/assemblyline-service-extract.git git clone https://github.com/CybercentreCanada/assemblyline-service-floss.git git clone https://github.com/CybercentreCanada/assemblyline-service-frankenstrings.git git clone https://github.com/CybercentreCanada/assemblyline-service-iparse.git git clone https://github.com/CybercentreCanada/assemblyline-service-metadefender.git git clone https://github.com/CybercentreCanada/assemblyline-service-metapeek.git git clone https://github.com/CybercentreCanada/assemblyline-service-oletools.git git clone https://github.com/CybercentreCanada/assemblyline-service-pdfid.git git clone https://github.com/CybercentreCanada/assemblyline-service-peepdf.git git clone https://github.com/CybercentreCanada/assemblyline-service-pefile.git git clone https://github.com/CybercentreCanada/assemblyline-service-pixaxe.git git clone https://github.com/CybercentreCanada/assemblyline-service-safelist.git git clone https://github.com/CybercentreCanada/assemblyline-service-sigma.git git clone https://github.com/CybercentreCanada/assemblyline-service-suricata.git git clone https://github.com/CybercentreCanada/assemblyline-service-swiffer.git git clone https://github.com/CybercentreCanada/assemblyline-service-torrentslicer.git git clone https://github.com/CybercentreCanada/assemblyline-service-unpacker.git git clone https://github.com/CybercentreCanada/assemblyline-service-unpacme.git git clone https://github.com/CybercentreCanada/assemblyline-service-vipermonkey.git git clone https://github.com/CybercentreCanada/assemblyline-service-virustotal-dynamic.git git clone https://github.com/CybercentreCanada/assemblyline-service-virustotal-static.git git clone https://github.com/CybercentreCanada/assemblyline-service-XLMMacroDeobfuscator.git git clone https://github.com/CybercentreCanada/assemblyline-service-yara.git Virtual Environment \u00b6 # Directly in the services source directory cd ~/git/services # Create the virtualenv python3.9 -m venv venv # Install Assemblyline packages from source in the services virtualenv ~/git/services/venv/bin/pip install -e ~/git/alv4/assemblyline-base ~/git/services/venv/bin/pip install -e ~/git/alv4/assemblyline-core ~/git/services/venv/bin/pip install -e ~/git/alv4/assemblyline-service-client ~/git/services/venv/bin/pip install -e ~/git/alv4/assemblyline-v4-service Setup PyCharm for core \u00b6 Load PyCharm Choose whatever configuration option you want until the Welcome screen Click the Open button Choose the ~/git/alv4 directory Info Your Python interpreter shows as No Interpreter in the bottom right corner of the window, do the following: Click on it Click add interpreter Choose \"Existing Environment\" Click \"OK\" Setup PyCharm for service (optional) \u00b6 From your core PyCharm window open the File menu then click Open Choose the ~/git/services directory Select New Window Info Your Python interpreter shows as No Interpreter in the bottom right corner of the window, do the following: Click on it Click add interpreter Choose \"Existing Environment\" Click \"OK\" Use PyCharm \u00b6 Now that your Local development VM is set up you should read the use PyCharm documentation to get you started.","title":"Local development"},{"location":"fr/developer_manual/env/pycharm/local_development/#local-development","text":"This documentation will show you how to set up your development virtual machine for local development using the PyCharm Community Edition IDE (This would also work with PyCharm Professional). In this setup, you will run your IDE inside the virtual machine where the Assemblyline containers are running. This is by far the easiest setup to get PyCharm working and removes a lot of headaches.","title":"Local development"},{"location":"fr/developer_manual/env/pycharm/local_development/#operating-system","text":"For this documentation, we will assume that you are working on a fresh installation of Ubuntu 20.04 Desktop .","title":"Operating system"},{"location":"fr/developer_manual/env/pycharm/local_development/#update-vm","text":"Make sure Ubuntu is running the latest software sudo apt update sudo apt dist-upgrade sudo snap refresh Reboot if needed sudo reboot","title":"Update VM"},{"location":"fr/developer_manual/env/pycharm/local_development/#installing-pre-requisite-software","text":"","title":"Installing pre-requisite software"},{"location":"fr/developer_manual/env/pycharm/local_development/#install-assemblyline-apt-dependencies","text":"sudo apt update sudo apt-get install -yy libfuzzy2 libmagic1 libldap-2.4-2 libsasl2-2 build-essential libffi-dev libfuzzy-dev libldap2-dev libsasl2-dev libssl-dev","title":"Install Assemblyline APT dependencies"},{"location":"fr/developer_manual/env/pycharm/local_development/#install-python-39","text":"Assemblyline 4 containers are now all built on Python 3.9 therefore we will install Python 3.9. sudo apt install -y software-properties-common sudo add-apt-repository -y ppa:deadsnakes/ppa sudo apt-get install -yy python3-venv python3.9 python3.9-dev python3.9-venv libffi7","title":"Install Python 3.9"},{"location":"fr/developer_manual/env/pycharm/local_development/#installing-docker","text":"Follow these simple commands to get Docker running on your machine: # Add Docker repository sudo apt-get update sudo apt-get install -y apt-transport-https ca-certificates curl gnupg-agent software-properties-common curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - sudo apt-key fingerprint 0EBFCD88 sudo add-apt-repository \"deb [arch=amd64] https://download.docker.com/linux/ubuntu $( lsb_release -cs ) stable\" # Install Docker sudo apt-get install -y docker-ce docker-ce-cli containerd.io # Test Docker installation sudo docker run hello-world","title":"Installing Docker"},{"location":"fr/developer_manual/env/pycharm/local_development/#installing-docker-compose","text":"Installing docker-compose is done the same way on all Linux distros. Follow these simple instructions: # Install docker-compose sudo curl -L \"https://github.com/docker/compose/releases/download/1.28.5/docker-compose- $( uname -s ) - $( uname -m ) \" -o /usr/local/bin/docker-compose sudo chmod +x /usr/local/bin/docker-compose # Test docker-compose installation docker-compose --version","title":"Installing docker-compose"},{"location":"fr/developer_manual/env/pycharm/local_development/#installing-pycharm","text":"Let's install the desired PyCharm version. The Professional version is not needed but if you have a licence you can use it. PyCharm Community sudo snap install --classic pycharm-community PyCharm Professional sudo snap install --classic pycharm-professional","title":"Installing PyCharm"},{"location":"fr/developer_manual/env/pycharm/local_development/#adding-assemblyline-specific-configuration","text":"","title":"Adding Assemblyline specific configuration"},{"location":"fr/developer_manual/env/pycharm/local_development/#data-directories","text":"Because Assemblyline uses its own set of folders inside the core, service-server, and UI containers, we must create the same folder structure here so we can run the components in debug mode. sudo mkdir -p /etc/assemblyline sudo mkdir -p /var/cache/assemblyline sudo mkdir -p /var/lib/assemblyline sudo mkdir -p /var/log/assemblyline sudo chown $USER /etc/assemblyline sudo chown $USER /var/cache/assemblyline sudo chown $USER /var/lib/assemblyline sudo chown $USER /var/log/assemblyline","title":"Data directories"},{"location":"fr/developer_manual/env/pycharm/local_development/#dev-default-configuration-files","text":"Here we will create configuration files that match the default dev docker-compose configuration files so that we can swap any of the components to the one that is being debugged. echo \"enforce: true\" > /etc/assemblyline/classification.yml echo \" auth: internal: enabled: true core: alerter: delay: 0 metrics: apm_server: server_url: http://localhost:8200/ elasticsearch: hosts: [http://elastic:devpass@localhost] datastore: ilm: indexes: alert: unit: m error: unit: m file: unit: m result: unit: m submission: unit: m filestore: cache: - file:///var/cache/assemblyline/ logging: log_level: INFO log_as_json: false ui: audit: false debug: false enforce_quota: false fqdn: 127.0.0.1.nip.io \" > /etc/assemblyline/config.yml Tip As you can see in the last command we are setting the FQDN to 127.0.0.1.nip.io. NIP.IO is a service that will resolve the first part of the domain 127.0.0.1 .nip.io to its IP value. We use this to fake DNS when there are none. This is especially useful for oAuth because some providers are forbidding redirect URLs to IPs.","title":"Dev default configuration files"},{"location":"fr/developer_manual/env/pycharm/local_development/#setup-assemblyline-source","text":"","title":"Setup Assemblyline source"},{"location":"fr/developer_manual/env/pycharm/local_development/#install-git","text":"Since your VM is running Ubuntu 20.04 you can just install it with APT: sudo apt install -y git Tip You should add your desktop SSH keys to your GitHub account to use Git via SSH. Follow these instructions to do so: GitHub Help","title":"Install git"},{"location":"fr/developer_manual/env/pycharm/local_development/#clone-core-repositories","text":"Create the core working directory mkdir -p ~/git/alv4 cd ~/git/alv4 Clone Assemblyline's repositories Git via SSH Use SSH if you have your SSH id_rsa file configured to your GitHub account git clone git@github.com:CybercentreCanada/assemblyline-base.git git clone git@github.com:CybercentreCanada/assemblyline-core.git git clone git@github.com:CybercentreCanada/assemblyline-service-client.git git clone git@github.com:CybercentreCanada/assemblyline-service-server.git git clone git@github.com:CybercentreCanada/assemblyline-ui.git git clone git@github.com:CybercentreCanada/assemblyline-v4-service.git Git via HTTPS Use HTTPS if you don't have your GitHub account configured with an SSH key git clone https://github.com/CybercentreCanada/assemblyline-base.git git clone https://github.com/CybercentreCanada/assemblyline-core.git git clone https://github.com/CybercentreCanada/assemblyline-service-client.git git clone https://github.com/CybercentreCanada/assemblyline-service-server.git git clone https://github.com/CybercentreCanada/assemblyline-ui.git git clone https://github.com/CybercentreCanada/assemblyline-v4-service.git","title":"Clone core repositories"},{"location":"fr/developer_manual/env/pycharm/local_development/#virtual-environment","text":"# Directly in the alv4 source directory cd ~/git/alv4 # Create the virtualenv python3.9 -m venv venv # Install Assemblyline packages with their test dependencies ~/git/alv4/venv/bin/pip install assemblyline [ test ] assemblyline-core [ test ] assemblyline-service-server [ test ] assemblyline-ui [ test ] # Remove Assemblyline packages because we will use the live code ~/git/alv4/venv/bin/pip uninstall -y assemblyline assemblyline-core assemblyline-service-server assemblyline-ui","title":"Virtual Environment"},{"location":"fr/developer_manual/env/pycharm/local_development/#setting-up-services-optional","text":"If you plan on doing service development in PyCharm you will need a dedicated directory for services with its own virtual environment.","title":"Setting up Services (Optional)"},{"location":"fr/developer_manual/env/pycharm/local_development/#clone-service-repositories","text":"Create the service working directory mkdir -p ~/git/services cd ~/git/services Clone Assemblyline's services repositories Git via SSH Use SSH if you have your SSH id_rsa file configured to your GitHub account git clone git@github.com:CybercentreCanada/assemblyline-service-antivirus.git git clone git@github.com:CybercentreCanada/assemblyline-service-apkaye.git git clone git@github.com:CybercentreCanada/assemblyline-service-avclass.git git clone git@github.com:CybercentreCanada/assemblyline-service-beaver.git git clone git@github.com:CybercentreCanada/assemblyline-service-characterize.git git clone git@github.com:CybercentreCanada/assemblyline-service-configextractor.git git clone git@github.com:CybercentreCanada/assemblyline-service-cuckoo.git git clone git@github.com:CybercentreCanada/assemblyline-service-deobfuscripter.git git clone git@github.com:CybercentreCanada/assemblyline-service-emlparser.git git clone git@github.com:CybercentreCanada/assemblyline-service-espresso.git git clone git@github.com:CybercentreCanada/assemblyline-service-extract.git git clone git@github.com:CybercentreCanada/assemblyline-service-floss.git git clone git@github.com:CybercentreCanada/assemblyline-service-frankenstrings.git git clone git@github.com:CybercentreCanada/assemblyline-service-iparse.git git clone git@github.com:CybercentreCanada/assemblyline-service-metadefender.git git clone git@github.com:CybercentreCanada/assemblyline-service-metapeek.git git clone git@github.com:CybercentreCanada/assemblyline-service-oletools.git git clone git@github.com:CybercentreCanada/assemblyline-service-pdfid.git git clone git@github.com:CybercentreCanada/assemblyline-service-peepdf.git git clone git@github.com:CybercentreCanada/assemblyline-service-pefile.git git clone git@github.com:CybercentreCanada/assemblyline-service-pixaxe.git git clone git@github.com:CybercentreCanada/assemblyline-service-safelist.git git clone git@github.com:CybercentreCanada/assemblyline-service-sigma.git git clone git@github.com:CybercentreCanada/assemblyline-service-suricata.git git clone git@github.com:CybercentreCanada/assemblyline-service-swiffer.git git clone git@github.com:CybercentreCanada/assemblyline-service-torrentslicer.git git clone git@github.com:CybercentreCanada/assemblyline-service-unpacker.git git clone git@github.com:CybercentreCanada/assemblyline-service-unpacme.git git clone git@github.com:CybercentreCanada/assemblyline-service-vipermonkey.git git clone git@github.com:CybercentreCanada/assemblyline-service-virustotal-dynamic.git git clone git@github.com:CybercentreCanada/assemblyline-service-virustotal-static.git git clone git@github.com:CybercentreCanada/assemblyline-service-XLMMacroDeobfuscator.git git clone git@github.com:CybercentreCanada/assemblyline-service-yara.git Git via HTTPS Use HTTPS if you don't have your GitHub account configured with an SSH key git clone https://github.com/CybercentreCanada/assemblyline-service-antivirus.git git clone https://github.com/CybercentreCanada/assemblyline-service-apkaye.git git clone https://github.com/CybercentreCanada/assemblyline-service-avclass.git git clone https://github.com/CybercentreCanada/assemblyline-service-beaver.git git clone https://github.com/CybercentreCanada/assemblyline-service-characterize.git git clone https://github.com/CybercentreCanada/assemblyline-service-configextractor.git git clone https://github.com/CybercentreCanada/assemblyline-service-cuckoo.git git clone https://github.com/CybercentreCanada/assemblyline-service-deobfuscripter.git git clone https://github.com/CybercentreCanada/assemblyline-service-emlparser.git git clone https://github.com/CybercentreCanada/assemblyline-service-espresso.git git clone https://github.com/CybercentreCanada/assemblyline-service-extract.git git clone https://github.com/CybercentreCanada/assemblyline-service-floss.git git clone https://github.com/CybercentreCanada/assemblyline-service-frankenstrings.git git clone https://github.com/CybercentreCanada/assemblyline-service-iparse.git git clone https://github.com/CybercentreCanada/assemblyline-service-metadefender.git git clone https://github.com/CybercentreCanada/assemblyline-service-metapeek.git git clone https://github.com/CybercentreCanada/assemblyline-service-oletools.git git clone https://github.com/CybercentreCanada/assemblyline-service-pdfid.git git clone https://github.com/CybercentreCanada/assemblyline-service-peepdf.git git clone https://github.com/CybercentreCanada/assemblyline-service-pefile.git git clone https://github.com/CybercentreCanada/assemblyline-service-pixaxe.git git clone https://github.com/CybercentreCanada/assemblyline-service-safelist.git git clone https://github.com/CybercentreCanada/assemblyline-service-sigma.git git clone https://github.com/CybercentreCanada/assemblyline-service-suricata.git git clone https://github.com/CybercentreCanada/assemblyline-service-swiffer.git git clone https://github.com/CybercentreCanada/assemblyline-service-torrentslicer.git git clone https://github.com/CybercentreCanada/assemblyline-service-unpacker.git git clone https://github.com/CybercentreCanada/assemblyline-service-unpacme.git git clone https://github.com/CybercentreCanada/assemblyline-service-vipermonkey.git git clone https://github.com/CybercentreCanada/assemblyline-service-virustotal-dynamic.git git clone https://github.com/CybercentreCanada/assemblyline-service-virustotal-static.git git clone https://github.com/CybercentreCanada/assemblyline-service-XLMMacroDeobfuscator.git git clone https://github.com/CybercentreCanada/assemblyline-service-yara.git","title":"Clone service repositories"},{"location":"fr/developer_manual/env/pycharm/local_development/#virtual-environment_1","text":"# Directly in the services source directory cd ~/git/services # Create the virtualenv python3.9 -m venv venv # Install Assemblyline packages from source in the services virtualenv ~/git/services/venv/bin/pip install -e ~/git/alv4/assemblyline-base ~/git/services/venv/bin/pip install -e ~/git/alv4/assemblyline-core ~/git/services/venv/bin/pip install -e ~/git/alv4/assemblyline-service-client ~/git/services/venv/bin/pip install -e ~/git/alv4/assemblyline-v4-service","title":"Virtual Environment"},{"location":"fr/developer_manual/env/pycharm/local_development/#setup-pycharm-for-core","text":"Load PyCharm Choose whatever configuration option you want until the Welcome screen Click the Open button Choose the ~/git/alv4 directory Info Your Python interpreter shows as No Interpreter in the bottom right corner of the window, do the following: Click on it Click add interpreter Choose \"Existing Environment\" Click \"OK\"","title":"Setup PyCharm for core"},{"location":"fr/developer_manual/env/pycharm/local_development/#setup-pycharm-for-service-optional","text":"From your core PyCharm window open the File menu then click Open Choose the ~/git/services directory Select New Window Info Your Python interpreter shows as No Interpreter in the bottom right corner of the window, do the following: Click on it Click add interpreter Choose \"Existing Environment\" Click \"OK\"","title":"Setup PyCharm for service (optional)"},{"location":"fr/developer_manual/env/pycharm/local_development/#use-pycharm","text":"Now that your Local development VM is set up you should read the use PyCharm documentation to get you started.","title":"Use PyCharm"},{"location":"fr/developer_manual/env/pycharm/remote_development/","text":"Remote development \u00b6 Warning To use this setup, we assume that you have a paid version of PyCharm ( Pycharm professional ) because we will be using features that are exclusive to the Professional version. If you don't, use the Local development setup instead. This document will show you how to set up your target virtual machine for remote development which means that you will run your IDE on your desktop and run the Assemblyline containers on the remote target VM. On the target VM \u00b6 Operating system \u00b6 For this document, we will assume that you are working on a fresh installation of Ubuntu 20.04 Server . Update target VM \u00b6 Make sure Ubuntu is running the latest software sudo apt update sudo apt dist-upgrade Reboot if needed sudo reboot Installing pre-requisite software \u00b6 Install SSH Daemon \u00b6 We need to make sure the remote target has an SSH daemon installed for remote debugging sudo apt update sudo apt install -y ssh Install Assemblyline APT dependencies \u00b6 sudo apt update sudo apt-get install -yy libfuzzy2 libmagic1 libldap-2.4-2 libsasl2-2 build-essential libffi-dev libfuzzy-dev libldap2-dev libsasl2-dev libssl-dev Install Python 3.9 \u00b6 Assemblyline 4 containers are now all built on Python 3.9 therefore we will install Python 3.9. sudo apt install -y software-properties-common sudo add-apt-repository -y ppa:deadsnakes/ppa sudo apt-get install -yy python3-venv python3.9 python3.9-dev python3.9-venv libffi7 Installing Docker \u00b6 Follow these simple commands to get Docker running on your machine: # Add Docker repository sudo apt-get update sudo apt-get install -y apt-transport-https ca-certificates curl gnupg-agent software-properties-common curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - sudo apt-key fingerprint 0EBFCD88 sudo add-apt-repository \"deb [arch=amd64] https://download.docker.com/linux/ubuntu $( lsb_release -cs ) stable\" # Install Docker sudo apt-get install -y docker-ce docker-ce-cli containerd.io # Test Docker installation sudo docker run hello-world Installing docker-compose \u00b6 Installing docker-compose is done the same way on all Linux distributions. Follow these simple instructions: # Install docker-compose sudo curl -L \"https://github.com/docker/compose/releases/download/1.28.5/docker-compose- $( uname -s ) - $( uname -m ) \" -o /usr/local/bin/docker-compose sudo chmod +x /usr/local/bin/docker-compose # Test docker-compose installation docker-compose --version Securing Docker for remote access \u00b6 We are going to make your Docker server accessible from the internet. To make it secure, we need to enable TLS authentication in the Docker daemon. Anywhere that you see assemblyline.local, you can change that value to your own DNS name. If you're planning on using an IP, you'll have to set a static IP to the remote VM because your certificate (cert) will only allow connections to that IP. # Create a cert directory mkdir ~/certs cd ~/certs # Create a CA (Remember the password you've set) openssl genrsa -aes256 -out ca-key.pem 4096 # Create a certificate-signing request (ignore the .rng error) openssl req -new -x509 -days 365 -key ca-key.pem -sha256 -out ca.pem -subj \"/C=CA/ST=Ontario/L=Ottawa/O=CCCS/CN=assemblyline.local\" # Creating the server public/private key openssl genrsa -out server-key.pem 4096 openssl req -subj \"/CN=assemblyline.local\" -sha256 -new -key server-key.pem -out server.csr echo subjectAltName = DNS:assemblyline.local,IP: ` ip route get 8 .8.8.8 | grep 8 .8.8.8 | awk '{ print $7 }' ` ,IP:127.0.0.1 >> extfile.cnf echo extendedKeyUsage = serverAuth >> extfile.cnf openssl x509 -req -days 365 -sha256 -in server.csr -CA ca.pem -CAkey ca-key.pem -CAcreateserial -out server-cert.pem -extfile extfile.cnf # Creating the client public/private key openssl genrsa -out key.pem 4096 openssl req -subj '/CN=client' -new -key key.pem -out client.csr echo extendedKeyUsage = clientAuth > extfile-client.cnf openssl x509 -req -days 365 -sha256 -in client.csr -CA ca.pem -CAkey ca-key.pem -CAcreateserial -out cert.pem -extfile extfile-client.cnf # Remove unnecessary files rm -v client.csr server.csr extfile.cnf extfile-client.cnf # Change private and public key permissions chmod -v 0400 ca-key.pem key.pem server-key.pem chmod -v 0444 ca.pem server-cert.pem cert.pem # Moving server certs to their permanent location sudo mkdir -p /etc/docker/certs sudo mv server*.pem /etc/docker/certs sudo cp ca.pem /etc/docker/certs # Add system.d override configuration for Docker to start the TCP with TLS port sudo mkdir -p /etc/systemd/system/docker.service.d/ sudo su -c 'echo \"# /etc/systemd/system/docker.service.d/override.conf [Service] ExecStart= ExecStart=/usr/bin/dockerd -H fd:// --tlsverify --tlscacert=/etc/docker/certs/ca.pem --tlscert=/etc/docker/certs/server-cert.pem --tlskey=/etc/docker/certs/server-key.pem -H 0.0.0.0:2376\" >> /etc/systemd/system/docker.service.d/override.conf' sudo systemctl daemon-reload sudo systemctl restart docker # Test the TLS connection with curl curl https://127.0.0.1:2376/images/json --cert ~/certs/cert.pem --key ~/certs/key.pem --cacert ~/certs/ca.pem # Create an archive with the client certs tar czvf certs.tgz ca.pem cert.pem key.pem The archive file ~/certs/certs.tgz will have to be transferred to your desktop. We will use its content to log into the Docker daemon from your desktop. Adding Assemblyline specific configuration \u00b6 Assemblyline folders \u00b6 Because Assemblyline uses its own set of folders inside the core, service-server, and UI container, we have to create the same folder structure here so that we can run the components in debug mode. sudo mkdir -p ~/git sudo mkdir -p /etc/assemblyline sudo mkdir -p /var/cache/assemblyline sudo mkdir -p /var/lib/assemblyline sudo mkdir -p /var/log/assemblyline sudo chown $USER /etc/assemblyline sudo chown $USER /var/cache/assemblyline sudo chown $USER /var/lib/assemblyline sudo chown $USER /var/log/assemblyline Assemblyline dev configuration files \u00b6 Here we will create configuration files that match the default dev docker-compose configuration files so that we can swap any of the components to the one that is being debugged. echo \"enforce: true\" > /etc/assemblyline/classification.yml echo \" auth: internal: enabled: true core: alerter: delay: 0 metrics: apm_server: server_url: http://localhost:8200/ elasticsearch: hosts: [http://elastic:devpass@localhost] datastore: ilm: indexes: alert: unit: m error: unit: m file: unit: m result: unit: m submission: unit: m filestore: cache: - file:///var/cache/assemblyline/ logging: log_level: INFO log_as_json: false ui: audit: false debug: false enforce_quota: false fqdn: `ip route get 8.8.8.8 | grep 8.8.8.8 | awk '{ print $7 }'`.nip.io \" > /etc/assemblyline/config.yml Tip As you can see in the last command we are setting the FQDN to YOUR_IP.nip.io. NIP.IO is a service that will resolve the first part of the domain YOUR_IP .nip.io to its IP value. We use this to fake DNS when there are none. This is especially useful for oAuth because some providers are forbidding redirect URLs to IPs. You can also replace the FQDN with your own DNS name if you have one. Setup Python Virtual Environments \u00b6 We will make two Python virtual environments: One for the core components One for services That should be enough to cover most cases. If a service has conflicting dependencies with another, I suggest you create a separate virtualenv for it when you try to debug it. The core components should all be fine in the same environment. Setting up Core Virtualenv \u00b6 # Make sure the venv directory exists and we are in it mkdir -p ~/venv cd ~/venv # Create the virtualenv python3.9 -m venv core # Install Assemblyline packages with their test dependencies ~/venv/core/bin/pip install assemblyline [ test ] assemblyline-core [ test ] assemblyline-service-server [ test ] assemblyline-ui [ test ] # Remove Assemblyline packages because we will use the live code ~/venv/core/bin/pip uninstall -y assemblyline assemblyline-core assemblyline-service-server assemblyline-ui Setting up Service Virtualenv (optional) \u00b6 # Make sure the venv directory exists and we are in it mkdir -p ~/venv cd ~/venv # Create the virtualenv python3.9 -m venv services # Install Assemblyline Python client ~/venv/services/bin/pip install assemblyline-client # Install Assemblyline service packages ~/venv/services/bin/pip install assemblyline-service-client assemblyline-v4-service # Remove Assemblyline packages because we will use the live code ~/venv/services/bin/pip uninstall -y assemblyline assemblyline-core assemblyline-service-client assemblyline-v4-service On your desktop \u00b6 We are now done setting up the target VM. For the rest of the instructions, we will mainly setup your PyCharm IDE to interface with the target VM. Get your Docker certs and install them \u00b6 mkdir -p ~/docker_certs cd ~/docker_certs scp USER_OF_TARGET_VM@IP_OF_TARGET_VM:certs/certs.tgz ~/docker_certs/ tar zxvf certs.tgz rm certs.tgz Install PyCharm \u00b6 You can download PyCharm Professional directly from JetBrains 's website but if your desktop is running Ubuntu 20.04, you can just install it with snap : sudo snap install --classic pycharm-professional Install Git \u00b6 You can get Git directly from GIT 's website but if your desktop is running Ubuntu 20.04 you can just install it with APT : sudo apt install -y git Tip You should add your desktop SSH keys to your GitHub account to use Git via SSH. Follow these instructions to do so: GitHub Help Clone repositories \u00b6 Core components \u00b6 Create the core working directory mkdir -p ~/git/alv4 cd ~/git/alv4 Clone Assemblyline's repositories Git via SSH Use SSH if you have your SSH id_rsa file configured to your GitHub account git clone git@github.com:CybercentreCanada/assemblyline-base.git git clone git@github.com:CybercentreCanada/assemblyline-core.git git clone git@github.com:CybercentreCanada/assemblyline-service-client.git git clone git@github.com:CybercentreCanada/assemblyline-service-server.git git clone git@github.com:CybercentreCanada/assemblyline-ui.git git clone git@github.com:CybercentreCanada/assemblyline-v4-service.git Git via HTTPS Use HTTPS if you don't have your GitHub account configured with an SSH key git clone https://github.com/CybercentreCanada/assemblyline-base.git git clone https://github.com/CybercentreCanada/assemblyline-core.git git clone https://github.com/CybercentreCanada/assemblyline-service-client.git git clone https://github.com/CybercentreCanada/assemblyline-service-server.git git clone https://github.com/CybercentreCanada/assemblyline-ui.git git clone https://github.com/CybercentreCanada/assemblyline-v4-service.git Services (optional) \u00b6 Create the service working directory mkdir -p ~/git/services cd ~/git/services Clone Assemblyline's services repositories Git via SSH Use SSH if you have your SSH id_rsa file configured to your GitHub account git clone git@github.com:CybercentreCanada/assemblyline-service-antivirus.git git clone git@github.com:CybercentreCanada/assemblyline-service-apkaye.git git clone git@github.com:CybercentreCanada/assemblyline-service-avclass.git git clone git@github.com:CybercentreCanada/assemblyline-service-beaver.git git clone git@github.com:CybercentreCanada/assemblyline-service-characterize.git git clone git@github.com:CybercentreCanada/assemblyline-service-configextractor.git git clone git@github.com:CybercentreCanada/assemblyline-service-cuckoo.git git clone git@github.com:CybercentreCanada/assemblyline-service-deobfuscripter.git git clone git@github.com:CybercentreCanada/assemblyline-service-emlparser.git git clone git@github.com:CybercentreCanada/assemblyline-service-espresso.git git clone git@github.com:CybercentreCanada/assemblyline-service-extract.git git clone git@github.com:CybercentreCanada/assemblyline-service-floss.git git clone git@github.com:CybercentreCanada/assemblyline-service-frankenstrings.git git clone git@github.com:CybercentreCanada/assemblyline-service-iparse.git git clone git@github.com:CybercentreCanada/assemblyline-service-metadefender.git git clone git@github.com:CybercentreCanada/assemblyline-service-metapeek.git git clone git@github.com:CybercentreCanada/assemblyline-service-oletools.git git clone git@github.com:CybercentreCanada/assemblyline-service-pdfid.git git clone git@github.com:CybercentreCanada/assemblyline-service-peepdf.git git clone git@github.com:CybercentreCanada/assemblyline-service-pefile.git git clone git@github.com:CybercentreCanada/assemblyline-service-pixaxe.git git clone git@github.com:CybercentreCanada/assemblyline-service-safelist.git git clone git@github.com:CybercentreCanada/assemblyline-service-sigma.git git clone git@github.com:CybercentreCanada/assemblyline-service-suricata.git git clone git@github.com:CybercentreCanada/assemblyline-service-swiffer.git git clone git@github.com:CybercentreCanada/assemblyline-service-torrentslicer.git git clone git@github.com:CybercentreCanada/assemblyline-service-unpacker.git git clone git@github.com:CybercentreCanada/assemblyline-service-unpacme.git git clone git@github.com:CybercentreCanada/assemblyline-service-vipermonkey.git git clone git@github.com:CybercentreCanada/assemblyline-service-virustotal-dynamic.git git clone git@github.com:CybercentreCanada/assemblyline-service-virustotal-static.git git clone git@github.com:CybercentreCanada/assemblyline-service-XLMMacroDeobfuscator.git git clone git@github.com:CybercentreCanada/assemblyline-service-yara.git Git via HTTPS Use HTTPS if you don't have your GitHub account configured with an SSH key git clone https://github.com/CybercentreCanada/assemblyline-service-antivirus.git git clone https://github.com/CybercentreCanada/assemblyline-service-apkaye.git git clone https://github.com/CybercentreCanada/assemblyline-service-avclass.git git clone https://github.com/CybercentreCanada/assemblyline-service-beaver.git git clone https://github.com/CybercentreCanada/assemblyline-service-characterize.git git clone https://github.com/CybercentreCanada/assemblyline-service-configextractor.git git clone https://github.com/CybercentreCanada/assemblyline-service-cuckoo.git git clone https://github.com/CybercentreCanada/assemblyline-service-deobfuscripter.git git clone https://github.com/CybercentreCanada/assemblyline-service-emlparser.git git clone https://github.com/CybercentreCanada/assemblyline-service-espresso.git git clone https://github.com/CybercentreCanada/assemblyline-service-extract.git git clone https://github.com/CybercentreCanada/assemblyline-service-floss.git git clone https://github.com/CybercentreCanada/assemblyline-service-frankenstrings.git git clone https://github.com/CybercentreCanada/assemblyline-service-iparse.git git clone https://github.com/CybercentreCanada/assemblyline-service-metadefender.git git clone https://github.com/CybercentreCanada/assemblyline-service-metapeek.git git clone https://github.com/CybercentreCanada/assemblyline-service-oletools.git git clone https://github.com/CybercentreCanada/assemblyline-service-pdfid.git git clone https://github.com/CybercentreCanada/assemblyline-service-peepdf.git git clone https://github.com/CybercentreCanada/assemblyline-service-pefile.git git clone https://github.com/CybercentreCanada/assemblyline-service-pixaxe.git git clone https://github.com/CybercentreCanada/assemblyline-service-safelist.git git clone https://github.com/CybercentreCanada/assemblyline-service-sigma.git git clone https://github.com/CybercentreCanada/assemblyline-service-suricata.git git clone https://github.com/CybercentreCanada/assemblyline-service-swiffer.git git clone https://github.com/CybercentreCanada/assemblyline-service-torrentslicer.git git clone https://github.com/CybercentreCanada/assemblyline-service-unpacker.git git clone https://github.com/CybercentreCanada/assemblyline-service-unpacme.git git clone https://github.com/CybercentreCanada/assemblyline-service-vipermonkey.git git clone https://github.com/CybercentreCanada/assemblyline-service-virustotal-dynamic.git git clone https://github.com/CybercentreCanada/assemblyline-service-virustotal-static.git git clone https://github.com/CybercentreCanada/assemblyline-service-XLMMacroDeobfuscator.git git clone https://github.com/CybercentreCanada/assemblyline-service-yara.git Setup PyCharm for core \u00b6 Start with loading the core directory in Pycharm: Load core folder Load Pycharm Professional Choose whatever configuration option you want until the Welcome screen Click the Open button Choose the ~/git/alv4 directory The setup of the remote deployment interpreter: Setup core remote interpreter Click Files -> Settings Select Project: alv4 -> Python Interpreter Click the cog wheel on the top right -> Add Select SSH Interpreter -> New Configuration Host: IP or DNS name of your target VM Username: username of the user on the target VM Port: 22 unless you changed it... Click Next Put your target VM password in the box, check Save password , and click Next In the next window, do the following: For the interpreter box , click the little folder and select your core venv ( /home/YOUR_TARGET_USER/venv/core/bin/python3.9 ) For the Sync folders box, click the little folder and for the remote path set the path to: /home/YOUR_TARGET_USER/git/alv4 then click OK (ensure target directory has write permissions for all users) Make sure Automatically upload files to the server is checked Make sure Execute code using this interpreter with root privileges via sudo is checked Hit Finish Click Ok Let it load the interpreter and do the transfers Finally link Docker for remote management: Setup Docker remote management Click Files -> Settings Select build, Execution, Deployment -> Docker Click the little + on top left Select TCP Socket In engine API URL put: https://TARGET_VM_IP:2376 In Certificates folder, click the little folder and browse to ~/docker_certs directory Click OK Setup PyCharm for service (optional) \u00b6 Start with loading the core directory in PyCharm: Load services folder From your core PyCharm window open the File menu then click Open Choose the ~/git/services directory Select New Window The setup of the remote deployment interpreter: Setup services remote interpreter Click Files -> Settings Select Project: services -> Python Interpreter Click the cog wheel on the top right -> Add Select SSH Interpreter -> New Configuration Host: IP or DNS name of your target VM Username: username of the user on the target VM Port: 22 unless you changed it... Click Next Put your target VM password in the box, check Save password , and click Next In the next window, do the following: For the interpreter box , click the little folder and select your core venv ( /home/YOUR_TARGET_USER/venv/services/bin/python3.9 ) For the Sync folders box, click the little folder and for the remote path set the path to: /home/YOUR_TARGET_USER/git/services then click OK (ensure target directory has write permissions for all users) Make sure Automatically upload files to the server is checked Make sure Execute code using this interpreter with root privileges via sudo is checked Hit Finish Click Ok Let it load the interpreter and do the transfers Use Pycharm \u00b6 Now that your remote development VM is set up you should read the use PyCharm documentation to get yourself started.","title":"Remote development"},{"location":"fr/developer_manual/env/pycharm/remote_development/#remote-development","text":"Warning To use this setup, we assume that you have a paid version of PyCharm ( Pycharm professional ) because we will be using features that are exclusive to the Professional version. If you don't, use the Local development setup instead. This document will show you how to set up your target virtual machine for remote development which means that you will run your IDE on your desktop and run the Assemblyline containers on the remote target VM.","title":"Remote development"},{"location":"fr/developer_manual/env/pycharm/remote_development/#on-the-target-vm","text":"","title":"On the target VM"},{"location":"fr/developer_manual/env/pycharm/remote_development/#operating-system","text":"For this document, we will assume that you are working on a fresh installation of Ubuntu 20.04 Server .","title":"Operating system"},{"location":"fr/developer_manual/env/pycharm/remote_development/#update-target-vm","text":"Make sure Ubuntu is running the latest software sudo apt update sudo apt dist-upgrade Reboot if needed sudo reboot","title":"Update target VM"},{"location":"fr/developer_manual/env/pycharm/remote_development/#installing-pre-requisite-software","text":"","title":"Installing pre-requisite software"},{"location":"fr/developer_manual/env/pycharm/remote_development/#install-ssh-daemon","text":"We need to make sure the remote target has an SSH daemon installed for remote debugging sudo apt update sudo apt install -y ssh","title":"Install SSH Daemon"},{"location":"fr/developer_manual/env/pycharm/remote_development/#install-assemblyline-apt-dependencies","text":"sudo apt update sudo apt-get install -yy libfuzzy2 libmagic1 libldap-2.4-2 libsasl2-2 build-essential libffi-dev libfuzzy-dev libldap2-dev libsasl2-dev libssl-dev","title":"Install Assemblyline APT dependencies"},{"location":"fr/developer_manual/env/pycharm/remote_development/#install-python-39","text":"Assemblyline 4 containers are now all built on Python 3.9 therefore we will install Python 3.9. sudo apt install -y software-properties-common sudo add-apt-repository -y ppa:deadsnakes/ppa sudo apt-get install -yy python3-venv python3.9 python3.9-dev python3.9-venv libffi7","title":"Install Python 3.9"},{"location":"fr/developer_manual/env/pycharm/remote_development/#installing-docker","text":"Follow these simple commands to get Docker running on your machine: # Add Docker repository sudo apt-get update sudo apt-get install -y apt-transport-https ca-certificates curl gnupg-agent software-properties-common curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - sudo apt-key fingerprint 0EBFCD88 sudo add-apt-repository \"deb [arch=amd64] https://download.docker.com/linux/ubuntu $( lsb_release -cs ) stable\" # Install Docker sudo apt-get install -y docker-ce docker-ce-cli containerd.io # Test Docker installation sudo docker run hello-world","title":"Installing Docker"},{"location":"fr/developer_manual/env/pycharm/remote_development/#installing-docker-compose","text":"Installing docker-compose is done the same way on all Linux distributions. Follow these simple instructions: # Install docker-compose sudo curl -L \"https://github.com/docker/compose/releases/download/1.28.5/docker-compose- $( uname -s ) - $( uname -m ) \" -o /usr/local/bin/docker-compose sudo chmod +x /usr/local/bin/docker-compose # Test docker-compose installation docker-compose --version","title":"Installing docker-compose"},{"location":"fr/developer_manual/env/pycharm/remote_development/#securing-docker-for-remote-access","text":"We are going to make your Docker server accessible from the internet. To make it secure, we need to enable TLS authentication in the Docker daemon. Anywhere that you see assemblyline.local, you can change that value to your own DNS name. If you're planning on using an IP, you'll have to set a static IP to the remote VM because your certificate (cert) will only allow connections to that IP. # Create a cert directory mkdir ~/certs cd ~/certs # Create a CA (Remember the password you've set) openssl genrsa -aes256 -out ca-key.pem 4096 # Create a certificate-signing request (ignore the .rng error) openssl req -new -x509 -days 365 -key ca-key.pem -sha256 -out ca.pem -subj \"/C=CA/ST=Ontario/L=Ottawa/O=CCCS/CN=assemblyline.local\" # Creating the server public/private key openssl genrsa -out server-key.pem 4096 openssl req -subj \"/CN=assemblyline.local\" -sha256 -new -key server-key.pem -out server.csr echo subjectAltName = DNS:assemblyline.local,IP: ` ip route get 8 .8.8.8 | grep 8 .8.8.8 | awk '{ print $7 }' ` ,IP:127.0.0.1 >> extfile.cnf echo extendedKeyUsage = serverAuth >> extfile.cnf openssl x509 -req -days 365 -sha256 -in server.csr -CA ca.pem -CAkey ca-key.pem -CAcreateserial -out server-cert.pem -extfile extfile.cnf # Creating the client public/private key openssl genrsa -out key.pem 4096 openssl req -subj '/CN=client' -new -key key.pem -out client.csr echo extendedKeyUsage = clientAuth > extfile-client.cnf openssl x509 -req -days 365 -sha256 -in client.csr -CA ca.pem -CAkey ca-key.pem -CAcreateserial -out cert.pem -extfile extfile-client.cnf # Remove unnecessary files rm -v client.csr server.csr extfile.cnf extfile-client.cnf # Change private and public key permissions chmod -v 0400 ca-key.pem key.pem server-key.pem chmod -v 0444 ca.pem server-cert.pem cert.pem # Moving server certs to their permanent location sudo mkdir -p /etc/docker/certs sudo mv server*.pem /etc/docker/certs sudo cp ca.pem /etc/docker/certs # Add system.d override configuration for Docker to start the TCP with TLS port sudo mkdir -p /etc/systemd/system/docker.service.d/ sudo su -c 'echo \"# /etc/systemd/system/docker.service.d/override.conf [Service] ExecStart= ExecStart=/usr/bin/dockerd -H fd:// --tlsverify --tlscacert=/etc/docker/certs/ca.pem --tlscert=/etc/docker/certs/server-cert.pem --tlskey=/etc/docker/certs/server-key.pem -H 0.0.0.0:2376\" >> /etc/systemd/system/docker.service.d/override.conf' sudo systemctl daemon-reload sudo systemctl restart docker # Test the TLS connection with curl curl https://127.0.0.1:2376/images/json --cert ~/certs/cert.pem --key ~/certs/key.pem --cacert ~/certs/ca.pem # Create an archive with the client certs tar czvf certs.tgz ca.pem cert.pem key.pem The archive file ~/certs/certs.tgz will have to be transferred to your desktop. We will use its content to log into the Docker daemon from your desktop.","title":"Securing Docker for remote access"},{"location":"fr/developer_manual/env/pycharm/remote_development/#adding-assemblyline-specific-configuration","text":"","title":"Adding Assemblyline specific configuration"},{"location":"fr/developer_manual/env/pycharm/remote_development/#assemblyline-folders","text":"Because Assemblyline uses its own set of folders inside the core, service-server, and UI container, we have to create the same folder structure here so that we can run the components in debug mode. sudo mkdir -p ~/git sudo mkdir -p /etc/assemblyline sudo mkdir -p /var/cache/assemblyline sudo mkdir -p /var/lib/assemblyline sudo mkdir -p /var/log/assemblyline sudo chown $USER /etc/assemblyline sudo chown $USER /var/cache/assemblyline sudo chown $USER /var/lib/assemblyline sudo chown $USER /var/log/assemblyline","title":"Assemblyline folders"},{"location":"fr/developer_manual/env/pycharm/remote_development/#assemblyline-dev-configuration-files","text":"Here we will create configuration files that match the default dev docker-compose configuration files so that we can swap any of the components to the one that is being debugged. echo \"enforce: true\" > /etc/assemblyline/classification.yml echo \" auth: internal: enabled: true core: alerter: delay: 0 metrics: apm_server: server_url: http://localhost:8200/ elasticsearch: hosts: [http://elastic:devpass@localhost] datastore: ilm: indexes: alert: unit: m error: unit: m file: unit: m result: unit: m submission: unit: m filestore: cache: - file:///var/cache/assemblyline/ logging: log_level: INFO log_as_json: false ui: audit: false debug: false enforce_quota: false fqdn: `ip route get 8.8.8.8 | grep 8.8.8.8 | awk '{ print $7 }'`.nip.io \" > /etc/assemblyline/config.yml Tip As you can see in the last command we are setting the FQDN to YOUR_IP.nip.io. NIP.IO is a service that will resolve the first part of the domain YOUR_IP .nip.io to its IP value. We use this to fake DNS when there are none. This is especially useful for oAuth because some providers are forbidding redirect URLs to IPs. You can also replace the FQDN with your own DNS name if you have one.","title":"Assemblyline dev configuration files"},{"location":"fr/developer_manual/env/pycharm/remote_development/#setup-python-virtual-environments","text":"We will make two Python virtual environments: One for the core components One for services That should be enough to cover most cases. If a service has conflicting dependencies with another, I suggest you create a separate virtualenv for it when you try to debug it. The core components should all be fine in the same environment.","title":"Setup Python Virtual Environments"},{"location":"fr/developer_manual/env/pycharm/remote_development/#setting-up-core-virtualenv","text":"# Make sure the venv directory exists and we are in it mkdir -p ~/venv cd ~/venv # Create the virtualenv python3.9 -m venv core # Install Assemblyline packages with their test dependencies ~/venv/core/bin/pip install assemblyline [ test ] assemblyline-core [ test ] assemblyline-service-server [ test ] assemblyline-ui [ test ] # Remove Assemblyline packages because we will use the live code ~/venv/core/bin/pip uninstall -y assemblyline assemblyline-core assemblyline-service-server assemblyline-ui","title":"Setting up Core Virtualenv"},{"location":"fr/developer_manual/env/pycharm/remote_development/#setting-up-service-virtualenv-optional","text":"# Make sure the venv directory exists and we are in it mkdir -p ~/venv cd ~/venv # Create the virtualenv python3.9 -m venv services # Install Assemblyline Python client ~/venv/services/bin/pip install assemblyline-client # Install Assemblyline service packages ~/venv/services/bin/pip install assemblyline-service-client assemblyline-v4-service # Remove Assemblyline packages because we will use the live code ~/venv/services/bin/pip uninstall -y assemblyline assemblyline-core assemblyline-service-client assemblyline-v4-service","title":"Setting up Service Virtualenv (optional)"},{"location":"fr/developer_manual/env/pycharm/remote_development/#on-your-desktop","text":"We are now done setting up the target VM. For the rest of the instructions, we will mainly setup your PyCharm IDE to interface with the target VM.","title":"On your desktop"},{"location":"fr/developer_manual/env/pycharm/remote_development/#get-your-docker-certs-and-install-them","text":"mkdir -p ~/docker_certs cd ~/docker_certs scp USER_OF_TARGET_VM@IP_OF_TARGET_VM:certs/certs.tgz ~/docker_certs/ tar zxvf certs.tgz rm certs.tgz","title":"Get your Docker certs and install them"},{"location":"fr/developer_manual/env/pycharm/remote_development/#install-pycharm","text":"You can download PyCharm Professional directly from JetBrains 's website but if your desktop is running Ubuntu 20.04, you can just install it with snap : sudo snap install --classic pycharm-professional","title":"Install PyCharm"},{"location":"fr/developer_manual/env/pycharm/remote_development/#install-git","text":"You can get Git directly from GIT 's website but if your desktop is running Ubuntu 20.04 you can just install it with APT : sudo apt install -y git Tip You should add your desktop SSH keys to your GitHub account to use Git via SSH. Follow these instructions to do so: GitHub Help","title":"Install Git"},{"location":"fr/developer_manual/env/pycharm/remote_development/#clone-repositories","text":"","title":"Clone repositories"},{"location":"fr/developer_manual/env/pycharm/remote_development/#core-components","text":"Create the core working directory mkdir -p ~/git/alv4 cd ~/git/alv4 Clone Assemblyline's repositories Git via SSH Use SSH if you have your SSH id_rsa file configured to your GitHub account git clone git@github.com:CybercentreCanada/assemblyline-base.git git clone git@github.com:CybercentreCanada/assemblyline-core.git git clone git@github.com:CybercentreCanada/assemblyline-service-client.git git clone git@github.com:CybercentreCanada/assemblyline-service-server.git git clone git@github.com:CybercentreCanada/assemblyline-ui.git git clone git@github.com:CybercentreCanada/assemblyline-v4-service.git Git via HTTPS Use HTTPS if you don't have your GitHub account configured with an SSH key git clone https://github.com/CybercentreCanada/assemblyline-base.git git clone https://github.com/CybercentreCanada/assemblyline-core.git git clone https://github.com/CybercentreCanada/assemblyline-service-client.git git clone https://github.com/CybercentreCanada/assemblyline-service-server.git git clone https://github.com/CybercentreCanada/assemblyline-ui.git git clone https://github.com/CybercentreCanada/assemblyline-v4-service.git","title":"Core components"},{"location":"fr/developer_manual/env/pycharm/remote_development/#services-optional","text":"Create the service working directory mkdir -p ~/git/services cd ~/git/services Clone Assemblyline's services repositories Git via SSH Use SSH if you have your SSH id_rsa file configured to your GitHub account git clone git@github.com:CybercentreCanada/assemblyline-service-antivirus.git git clone git@github.com:CybercentreCanada/assemblyline-service-apkaye.git git clone git@github.com:CybercentreCanada/assemblyline-service-avclass.git git clone git@github.com:CybercentreCanada/assemblyline-service-beaver.git git clone git@github.com:CybercentreCanada/assemblyline-service-characterize.git git clone git@github.com:CybercentreCanada/assemblyline-service-configextractor.git git clone git@github.com:CybercentreCanada/assemblyline-service-cuckoo.git git clone git@github.com:CybercentreCanada/assemblyline-service-deobfuscripter.git git clone git@github.com:CybercentreCanada/assemblyline-service-emlparser.git git clone git@github.com:CybercentreCanada/assemblyline-service-espresso.git git clone git@github.com:CybercentreCanada/assemblyline-service-extract.git git clone git@github.com:CybercentreCanada/assemblyline-service-floss.git git clone git@github.com:CybercentreCanada/assemblyline-service-frankenstrings.git git clone git@github.com:CybercentreCanada/assemblyline-service-iparse.git git clone git@github.com:CybercentreCanada/assemblyline-service-metadefender.git git clone git@github.com:CybercentreCanada/assemblyline-service-metapeek.git git clone git@github.com:CybercentreCanada/assemblyline-service-oletools.git git clone git@github.com:CybercentreCanada/assemblyline-service-pdfid.git git clone git@github.com:CybercentreCanada/assemblyline-service-peepdf.git git clone git@github.com:CybercentreCanada/assemblyline-service-pefile.git git clone git@github.com:CybercentreCanada/assemblyline-service-pixaxe.git git clone git@github.com:CybercentreCanada/assemblyline-service-safelist.git git clone git@github.com:CybercentreCanada/assemblyline-service-sigma.git git clone git@github.com:CybercentreCanada/assemblyline-service-suricata.git git clone git@github.com:CybercentreCanada/assemblyline-service-swiffer.git git clone git@github.com:CybercentreCanada/assemblyline-service-torrentslicer.git git clone git@github.com:CybercentreCanada/assemblyline-service-unpacker.git git clone git@github.com:CybercentreCanada/assemblyline-service-unpacme.git git clone git@github.com:CybercentreCanada/assemblyline-service-vipermonkey.git git clone git@github.com:CybercentreCanada/assemblyline-service-virustotal-dynamic.git git clone git@github.com:CybercentreCanada/assemblyline-service-virustotal-static.git git clone git@github.com:CybercentreCanada/assemblyline-service-XLMMacroDeobfuscator.git git clone git@github.com:CybercentreCanada/assemblyline-service-yara.git Git via HTTPS Use HTTPS if you don't have your GitHub account configured with an SSH key git clone https://github.com/CybercentreCanada/assemblyline-service-antivirus.git git clone https://github.com/CybercentreCanada/assemblyline-service-apkaye.git git clone https://github.com/CybercentreCanada/assemblyline-service-avclass.git git clone https://github.com/CybercentreCanada/assemblyline-service-beaver.git git clone https://github.com/CybercentreCanada/assemblyline-service-characterize.git git clone https://github.com/CybercentreCanada/assemblyline-service-configextractor.git git clone https://github.com/CybercentreCanada/assemblyline-service-cuckoo.git git clone https://github.com/CybercentreCanada/assemblyline-service-deobfuscripter.git git clone https://github.com/CybercentreCanada/assemblyline-service-emlparser.git git clone https://github.com/CybercentreCanada/assemblyline-service-espresso.git git clone https://github.com/CybercentreCanada/assemblyline-service-extract.git git clone https://github.com/CybercentreCanada/assemblyline-service-floss.git git clone https://github.com/CybercentreCanada/assemblyline-service-frankenstrings.git git clone https://github.com/CybercentreCanada/assemblyline-service-iparse.git git clone https://github.com/CybercentreCanada/assemblyline-service-metadefender.git git clone https://github.com/CybercentreCanada/assemblyline-service-metapeek.git git clone https://github.com/CybercentreCanada/assemblyline-service-oletools.git git clone https://github.com/CybercentreCanada/assemblyline-service-pdfid.git git clone https://github.com/CybercentreCanada/assemblyline-service-peepdf.git git clone https://github.com/CybercentreCanada/assemblyline-service-pefile.git git clone https://github.com/CybercentreCanada/assemblyline-service-pixaxe.git git clone https://github.com/CybercentreCanada/assemblyline-service-safelist.git git clone https://github.com/CybercentreCanada/assemblyline-service-sigma.git git clone https://github.com/CybercentreCanada/assemblyline-service-suricata.git git clone https://github.com/CybercentreCanada/assemblyline-service-swiffer.git git clone https://github.com/CybercentreCanada/assemblyline-service-torrentslicer.git git clone https://github.com/CybercentreCanada/assemblyline-service-unpacker.git git clone https://github.com/CybercentreCanada/assemblyline-service-unpacme.git git clone https://github.com/CybercentreCanada/assemblyline-service-vipermonkey.git git clone https://github.com/CybercentreCanada/assemblyline-service-virustotal-dynamic.git git clone https://github.com/CybercentreCanada/assemblyline-service-virustotal-static.git git clone https://github.com/CybercentreCanada/assemblyline-service-XLMMacroDeobfuscator.git git clone https://github.com/CybercentreCanada/assemblyline-service-yara.git","title":"Services (optional)"},{"location":"fr/developer_manual/env/pycharm/remote_development/#setup-pycharm-for-core","text":"Start with loading the core directory in Pycharm: Load core folder Load Pycharm Professional Choose whatever configuration option you want until the Welcome screen Click the Open button Choose the ~/git/alv4 directory The setup of the remote deployment interpreter: Setup core remote interpreter Click Files -> Settings Select Project: alv4 -> Python Interpreter Click the cog wheel on the top right -> Add Select SSH Interpreter -> New Configuration Host: IP or DNS name of your target VM Username: username of the user on the target VM Port: 22 unless you changed it... Click Next Put your target VM password in the box, check Save password , and click Next In the next window, do the following: For the interpreter box , click the little folder and select your core venv ( /home/YOUR_TARGET_USER/venv/core/bin/python3.9 ) For the Sync folders box, click the little folder and for the remote path set the path to: /home/YOUR_TARGET_USER/git/alv4 then click OK (ensure target directory has write permissions for all users) Make sure Automatically upload files to the server is checked Make sure Execute code using this interpreter with root privileges via sudo is checked Hit Finish Click Ok Let it load the interpreter and do the transfers Finally link Docker for remote management: Setup Docker remote management Click Files -> Settings Select build, Execution, Deployment -> Docker Click the little + on top left Select TCP Socket In engine API URL put: https://TARGET_VM_IP:2376 In Certificates folder, click the little folder and browse to ~/docker_certs directory Click OK","title":"Setup PyCharm for core"},{"location":"fr/developer_manual/env/pycharm/remote_development/#setup-pycharm-for-service-optional","text":"Start with loading the core directory in PyCharm: Load services folder From your core PyCharm window open the File menu then click Open Choose the ~/git/services directory Select New Window The setup of the remote deployment interpreter: Setup services remote interpreter Click Files -> Settings Select Project: services -> Python Interpreter Click the cog wheel on the top right -> Add Select SSH Interpreter -> New Configuration Host: IP or DNS name of your target VM Username: username of the user on the target VM Port: 22 unless you changed it... Click Next Put your target VM password in the box, check Save password , and click Next In the next window, do the following: For the interpreter box , click the little folder and select your core venv ( /home/YOUR_TARGET_USER/venv/services/bin/python3.9 ) For the Sync folders box, click the little folder and for the remote path set the path to: /home/YOUR_TARGET_USER/git/services then click OK (ensure target directory has write permissions for all users) Make sure Automatically upload files to the server is checked Make sure Execute code using this interpreter with root privileges via sudo is checked Hit Finish Click Ok Let it load the interpreter and do the transfers","title":"Setup PyCharm for service (optional)"},{"location":"fr/developer_manual/env/pycharm/remote_development/#use-pycharm","text":"Now that your remote development VM is set up you should read the use PyCharm documentation to get yourself started.","title":"Use Pycharm"},{"location":"fr/developer_manual/env/pycharm/use_pycharm/","text":"Use PyCharm \u00b6 Here are some pointers on how to run most of the core components live in PyCharm when either your local or remote development VM is ready. Load docker-compose files \u00b6 Minimal Dependencies \u00b6 When dependencies are loaded, you can launch any core components and their dependencies should be satisfied. PyCharm Professional In your project file viewer Browse to assemblyline-base/dev/depends Right-click on docker-compose-minimal.yml Select Create 'depends/docker-compose...' Click OK You'll notice on the top right that a new Compose deployment has been created for dependencies. Hit the play button next to it to launch the containers. Tip From now on you can just select that Compose deployment for dependencies from the dropdown up top and hit the run button to launch it. It is good practice to Edit the configuration and give it a proper name. Docker-compose (PyCharm Community) In a new terminal on the VM, run the following commands: cd ~/git/alv4/assemblyline-base/dev/depends/ sudo docker-compose -f docker-compose-minimal.yml up Tip This will take over the terminal and the logs will be displayed there. Hit ctrl-c when you want to shut the containers down. If you close the terminal and the containers keep running, run the following commands to shut down the containers: cd ~/git/alv4/assemblyline-base/dev/depends/ sudo docker-compose -f docker-compose-minimal.yml down Core services \u00b6 Core services depend on the dependencies in the docker-compose file. When the core services are loaded you should be able to point a browser at https://IP_OF_VM and you'll be greeted with a working version of Assemblyline with no services configured. Note Default admin user credentials are: Username: admin Password: admin PyCharm Professional In your project file viewer Browse to assemblyline-base/dev/core Right-click on docker-compose.yml Select Create 'core: Compose Deploy...' Click OK You'll notice on the top right that a new Compose deployment has been created for core services, hit the play button next to it to launch the containers. Tip From now on you can just select that Compose deployment for core components from the dropdown up top and hit the run button to launch it. It is good practice to Edit the configuration and give it a proper name. Docker-compose (PyCharm Community) In a new terminal on the VM, run the following commands: cd ~/git/alv4/assemblyline-base/dev/core/ sudo docker-compose up Tip This will take over the terminal and the logs will be displayed there. Hit ctrl-c when you want to shut the containers down. If you close the terminal and the containers keep running, run the following commands to shut down the containers: cd ~/git/alv4/assemblyline-base/dev/core/ sudo docker-compose down Load a single container live \u00b6 Loading a single container is very useful to test a newly create production service container or to launch the frontend while the backend is being debugged. You can easily load any container live in your Assemblyline Dev environment by following these instructions: Note For this demo we will assume that you want to run the service container from the developing an Assemblyline service documentation. PyCharm Professional Click the Run menu then select Edit Configurations... Click the + button at the top left` Select \"Docker Image\" In the Name field at the top, set the name to: Sample Service - Container Set the Image ID to: testing/assemblyline-service-sample Set the Container Name to SampleService Click the modify options and select Environment variables Add the following environment variable: SERVICE_API_HOST=http://172.17.0.1:5003 Click the modify options and select Run options Add the following Run option : -network host Click OK Tip From now on you can just select the Sample Service - Container run configuration from the dropdown up top and hit the run button to launch it. Docker (PyCharm Community) Pycharm Community does not have support for managing Docker containers, therefore you will have to run your containers using a shell . In a new terminal on the VM, run the following commands: docker run --env SERVICE_API_HOST = http://172.17.0.1:5003 --network = host --name SampleService testing/assemblyline-service-sample Tip This will take over the terminal and the logs will be displayed there. Hit ctrl-c when you want to shut the containers down. Run components live from PyCharm \u00b6 Core Components \u00b6 Most of the core components are going to be as easy to run as finding the component main file then hit Run or Debug ... All core components require you to run the Minimal Dependencies docker-compose file before launching them. Launching the API Server Find the UI main file in the project files browser assemblyline-ui/assemblyline_ui/app.py Right-click on it Select either Run 'app' or Debug 'app' Live Services \u00b6 The main exception to very easy-to-run components is debugging services live in the system. Services require you to run two separate components to process files. The two components talk via named pipes in the /tmp folder. Inside the Docker container, this is very easy but debugging this live requires some sacrifices. Limitations You can only run one live debugging service at the time. Depending on where you stop them and how you stop them, they don't fully clean up after themselves. They require extra configuration, it's not just \"click-and-go\". We will show you how to run the Sample service live in the system created in the developing and Assemblyline service documentation. This requires you to run the full infrastructure using the docker-compose files before executing the steps. ( Minimal dependencies and Core services ) Important Run the following example inside the PyCharm window pointing to the services ( ~/git/services ) Example Setup Task Handler run configuration if it does not exist: Click the \"Run\" menu then select \"Edit Configurations...\" Click the + button at the top left Click Python The first option in the configuration tab is a drop-down that says Script path: click it and choose Module Name: In the box beside it, write: assemblyline_service_client.task_handler In the name box at the top, write Task Handler Click the OK Button Now if you click the green play button beside the newly created Task Handler run configuration, Task Handler will be waiting for the service to start. Setup the new service configuration: Click the \"Run\" menu then select \"Edit Configurations...\" Click the + button at the top left Click Python The first option in the configuration tab is a drop-down that says Script path: click it and choose Module Name: In the box beside it, write: assemblyline_v4_service.run_service Add ;SERVICE_PATH=sample.Sample to the Environment variables Set the working directory to: assemblyline-service-sample by pressing the little folder on the right and browsing to that directory In the name box at the top, write Sample Service - LIVE Click OK Now that dropdown near the top says Sample Service - LIVE , click the play or the bug button to either Run or Debug the service In the Run or Debug window, the service will be stuck at Waiting for receive task named pipe to be ready... . This is because task_handler shut down after registering the service for the first time. Select task_handler from the dropdown near the top and click the play or bug button beside it again. Now both Sample Service - LIVE and Task Handler will stay up and poll for tasks from the service server.","title":"Use PyCharm"},{"location":"fr/developer_manual/env/pycharm/use_pycharm/#use-pycharm","text":"Here are some pointers on how to run most of the core components live in PyCharm when either your local or remote development VM is ready.","title":"Use PyCharm"},{"location":"fr/developer_manual/env/pycharm/use_pycharm/#load-docker-compose-files","text":"","title":"Load docker-compose files"},{"location":"fr/developer_manual/env/pycharm/use_pycharm/#minimal-dependencies","text":"When dependencies are loaded, you can launch any core components and their dependencies should be satisfied. PyCharm Professional In your project file viewer Browse to assemblyline-base/dev/depends Right-click on docker-compose-minimal.yml Select Create 'depends/docker-compose...' Click OK You'll notice on the top right that a new Compose deployment has been created for dependencies. Hit the play button next to it to launch the containers. Tip From now on you can just select that Compose deployment for dependencies from the dropdown up top and hit the run button to launch it. It is good practice to Edit the configuration and give it a proper name. Docker-compose (PyCharm Community) In a new terminal on the VM, run the following commands: cd ~/git/alv4/assemblyline-base/dev/depends/ sudo docker-compose -f docker-compose-minimal.yml up Tip This will take over the terminal and the logs will be displayed there. Hit ctrl-c when you want to shut the containers down. If you close the terminal and the containers keep running, run the following commands to shut down the containers: cd ~/git/alv4/assemblyline-base/dev/depends/ sudo docker-compose -f docker-compose-minimal.yml down","title":"Minimal Dependencies"},{"location":"fr/developer_manual/env/pycharm/use_pycharm/#core-services","text":"Core services depend on the dependencies in the docker-compose file. When the core services are loaded you should be able to point a browser at https://IP_OF_VM and you'll be greeted with a working version of Assemblyline with no services configured. Note Default admin user credentials are: Username: admin Password: admin PyCharm Professional In your project file viewer Browse to assemblyline-base/dev/core Right-click on docker-compose.yml Select Create 'core: Compose Deploy...' Click OK You'll notice on the top right that a new Compose deployment has been created for core services, hit the play button next to it to launch the containers. Tip From now on you can just select that Compose deployment for core components from the dropdown up top and hit the run button to launch it. It is good practice to Edit the configuration and give it a proper name. Docker-compose (PyCharm Community) In a new terminal on the VM, run the following commands: cd ~/git/alv4/assemblyline-base/dev/core/ sudo docker-compose up Tip This will take over the terminal and the logs will be displayed there. Hit ctrl-c when you want to shut the containers down. If you close the terminal and the containers keep running, run the following commands to shut down the containers: cd ~/git/alv4/assemblyline-base/dev/core/ sudo docker-compose down","title":"Core services"},{"location":"fr/developer_manual/env/pycharm/use_pycharm/#load-a-single-container-live","text":"Loading a single container is very useful to test a newly create production service container or to launch the frontend while the backend is being debugged. You can easily load any container live in your Assemblyline Dev environment by following these instructions: Note For this demo we will assume that you want to run the service container from the developing an Assemblyline service documentation. PyCharm Professional Click the Run menu then select Edit Configurations... Click the + button at the top left` Select \"Docker Image\" In the Name field at the top, set the name to: Sample Service - Container Set the Image ID to: testing/assemblyline-service-sample Set the Container Name to SampleService Click the modify options and select Environment variables Add the following environment variable: SERVICE_API_HOST=http://172.17.0.1:5003 Click the modify options and select Run options Add the following Run option : -network host Click OK Tip From now on you can just select the Sample Service - Container run configuration from the dropdown up top and hit the run button to launch it. Docker (PyCharm Community) Pycharm Community does not have support for managing Docker containers, therefore you will have to run your containers using a shell . In a new terminal on the VM, run the following commands: docker run --env SERVICE_API_HOST = http://172.17.0.1:5003 --network = host --name SampleService testing/assemblyline-service-sample Tip This will take over the terminal and the logs will be displayed there. Hit ctrl-c when you want to shut the containers down.","title":"Load a single container live"},{"location":"fr/developer_manual/env/pycharm/use_pycharm/#run-components-live-from-pycharm","text":"","title":"Run components live from PyCharm"},{"location":"fr/developer_manual/env/pycharm/use_pycharm/#core-components","text":"Most of the core components are going to be as easy to run as finding the component main file then hit Run or Debug ... All core components require you to run the Minimal Dependencies docker-compose file before launching them. Launching the API Server Find the UI main file in the project files browser assemblyline-ui/assemblyline_ui/app.py Right-click on it Select either Run 'app' or Debug 'app'","title":"Core Components"},{"location":"fr/developer_manual/env/pycharm/use_pycharm/#live-services","text":"The main exception to very easy-to-run components is debugging services live in the system. Services require you to run two separate components to process files. The two components talk via named pipes in the /tmp folder. Inside the Docker container, this is very easy but debugging this live requires some sacrifices. Limitations You can only run one live debugging service at the time. Depending on where you stop them and how you stop them, they don't fully clean up after themselves. They require extra configuration, it's not just \"click-and-go\". We will show you how to run the Sample service live in the system created in the developing and Assemblyline service documentation. This requires you to run the full infrastructure using the docker-compose files before executing the steps. ( Minimal dependencies and Core services ) Important Run the following example inside the PyCharm window pointing to the services ( ~/git/services ) Example Setup Task Handler run configuration if it does not exist: Click the \"Run\" menu then select \"Edit Configurations...\" Click the + button at the top left Click Python The first option in the configuration tab is a drop-down that says Script path: click it and choose Module Name: In the box beside it, write: assemblyline_service_client.task_handler In the name box at the top, write Task Handler Click the OK Button Now if you click the green play button beside the newly created Task Handler run configuration, Task Handler will be waiting for the service to start. Setup the new service configuration: Click the \"Run\" menu then select \"Edit Configurations...\" Click the + button at the top left Click Python The first option in the configuration tab is a drop-down that says Script path: click it and choose Module Name: In the box beside it, write: assemblyline_v4_service.run_service Add ;SERVICE_PATH=sample.Sample to the Environment variables Set the working directory to: assemblyline-service-sample by pressing the little folder on the right and browsing to that directory In the name box at the top, write Sample Service - LIVE Click OK Now that dropdown near the top says Sample Service - LIVE , click the play or the bug button to either Run or Debug the service In the Run or Debug window, the service will be stuck at Waiting for receive task named pipe to be ready... . This is because task_handler shut down after registering the service for the first time. Select task_handler from the dropdown near the top and click the play or bug button beside it again. Now both Sample Service - LIVE and Task Handler will stay up and poll for tasks from the service server.","title":"Live Services"},{"location":"fr/developer_manual/env/vscode/setup_script/","text":"Setup script \u00b6 Assemblyline's VSCode installation is entirely scripted. It will set up the following things for you: Install VSCode via snap (Optional) Install AL4 development dependencies Clone all core component sources from GitHub Clone all service sources from GitHub (Optional) Create a virtual Python environment for core component development Create a virtual Python environment for service development (optional) Create Run targets inside VSCode for all core components and other important scripts Create Tasks inside VSCode for development using Docker-Compose Setup our code formatting standards Deploy a local Docker registry on port 32000 Note We recommend installing the VSCode extensions needed to use this environment once VSCode is launched in the workspace. Pre-requisites \u00b6 The setup script assumes the following: You are running this on an Ubuntu machine / VM (20.04 and up). VSCode does not have to be running on the same host where you run this script so run the setup script on the target VM of a remote development setup. You have read the setup_vscode.sh script. This script will install and configure packages for ease of use. Important If you are uncomfortable with some of the changes that the script makes, you should comment them out before running the script. Installation instruction \u00b6 Create your repository directory mkdir -p ~/git cd ~/git Clone repository git clone https://github.com/CybercentreCanada/assemblyline-development-setup alv4 Run setup script. Choose the type of development you want to do and on what type of system. Core only (Local) cd alv4 ./setup_vscode.sh -c Core and Services (Local) cd alv4 ./setup_vscode.sh -c -s Core only (Remote) cd alv4 ./setup_vscode.sh Core and Services (Remote) cd alv4 ./setup_vscode.sh -s Important When running the setup script for the Core and Services installation you will get two dev folders: ~/git/alv4 and ~/git/services . The reason for this is that we want to make sure that service Python dependencies don't interfere with core component dependencies. Therefore, two separate venv are created with different sets of dependencies. The service venv will point to the core components' live code to install assemblyline-base , assemblyline-core , assemblyline-v4-service , and assemblyline-client . That way, any modification you do to the core package code will be reflected in your service instantly. Post-installation instructions \u00b6 When the installation is complete, you will be asked to reboot the VM. This is required for sudo-less Docker to work. After the VM has finished rebooted, you can use a shell to open VSCode: code ~/git/alv4 Note If you've installed the services, you should open another VSCode window pointing to the services folder. code ~/git/services Install recommended extensions \u00b6 To take full advantage of this setup, we strongly advise installing the recommended extensions when prompted or typing @recommended in the Extensions tab . Start using VSCode \u00b6 You can now refer to the \" use VSCode \" instructions to get you started using the VSCode environment with Assemblyline.","title":"Setup script"},{"location":"fr/developer_manual/env/vscode/setup_script/#setup-script","text":"Assemblyline's VSCode installation is entirely scripted. It will set up the following things for you: Install VSCode via snap (Optional) Install AL4 development dependencies Clone all core component sources from GitHub Clone all service sources from GitHub (Optional) Create a virtual Python environment for core component development Create a virtual Python environment for service development (optional) Create Run targets inside VSCode for all core components and other important scripts Create Tasks inside VSCode for development using Docker-Compose Setup our code formatting standards Deploy a local Docker registry on port 32000 Note We recommend installing the VSCode extensions needed to use this environment once VSCode is launched in the workspace.","title":"Setup script"},{"location":"fr/developer_manual/env/vscode/setup_script/#pre-requisites","text":"The setup script assumes the following: You are running this on an Ubuntu machine / VM (20.04 and up). VSCode does not have to be running on the same host where you run this script so run the setup script on the target VM of a remote development setup. You have read the setup_vscode.sh script. This script will install and configure packages for ease of use. Important If you are uncomfortable with some of the changes that the script makes, you should comment them out before running the script.","title":"Pre-requisites"},{"location":"fr/developer_manual/env/vscode/setup_script/#installation-instruction","text":"Create your repository directory mkdir -p ~/git cd ~/git Clone repository git clone https://github.com/CybercentreCanada/assemblyline-development-setup alv4 Run setup script. Choose the type of development you want to do and on what type of system. Core only (Local) cd alv4 ./setup_vscode.sh -c Core and Services (Local) cd alv4 ./setup_vscode.sh -c -s Core only (Remote) cd alv4 ./setup_vscode.sh Core and Services (Remote) cd alv4 ./setup_vscode.sh -s Important When running the setup script for the Core and Services installation you will get two dev folders: ~/git/alv4 and ~/git/services . The reason for this is that we want to make sure that service Python dependencies don't interfere with core component dependencies. Therefore, two separate venv are created with different sets of dependencies. The service venv will point to the core components' live code to install assemblyline-base , assemblyline-core , assemblyline-v4-service , and assemblyline-client . That way, any modification you do to the core package code will be reflected in your service instantly.","title":"Installation instruction"},{"location":"fr/developer_manual/env/vscode/setup_script/#post-installation-instructions","text":"When the installation is complete, you will be asked to reboot the VM. This is required for sudo-less Docker to work. After the VM has finished rebooted, you can use a shell to open VSCode: code ~/git/alv4 Note If you've installed the services, you should open another VSCode window pointing to the services folder. code ~/git/services","title":"Post-installation instructions"},{"location":"fr/developer_manual/env/vscode/setup_script/#install-recommended-extensions","text":"To take full advantage of this setup, we strongly advise installing the recommended extensions when prompted or typing @recommended in the Extensions tab .","title":"Install recommended extensions"},{"location":"fr/developer_manual/env/vscode/setup_script/#start-using-vscode","text":"You can now refer to the \" use VSCode \" instructions to get you started using the VSCode environment with Assemblyline.","title":"Start using VSCode"},{"location":"fr/developer_manual/env/vscode/use_vscode/","text":"Use VSCode \u00b6 Once you are done running the setup script , your installation of VSCode will be ready to run and debug any components of Assemblyline and most launch targets are already pre-configured. This page will point you in the right direction to perform some of the more common tasks that you'll have to do when developing an aspect of Assemblyline. Running Tasks \u00b6 After all recommended extensions are finished installing in VSCode, the task button on the sidebar should be revealed and will give you quick access to the most important tasks in the system. These tasks are split into 3 categories: Container - Run a single container for a single task Docker-compose - Execute a set of containers for a specific task dependency Pytest dependencies - Run necessary dependencies to run tests Tip You can edit the task list by modifying the .vscode/tasks.json . The default task.json file can be found in the assemblyline-development-setup repository. Container tasks \u00b6 A container task executes one specific container in the system. Two of these tasks are predefined: Frontend \u00b6 The Frontend task is used to run the User Interface of Assemblyline. It is only useful for when you launch the Assemblyline API Server in debug mode and are NOT using the core components task. Assemblyline's frontend is now built using ReactJS and is served via NPM serve which is why it is not part of this setup. Refer to the frontend development page for more information on how to do development on the Assemblyline frontend. ResultSample \u00b6 The ResultSample task was created to show the developers how to run newly created service containers in the system. Deep dive in the ResultSample Task This is what the JSON block for executing the ResultSample service in VSCode looks like: ... { \"label\" : \"Container - ResultSample\" , \"type\" : \"shell\" , \"options\" : { \"env\" : { \"LOCAL_IP\" : \"172.17.0.1\" } }, \"command\" : \"docker run --env SERVICE_API_HOST=http://${LOCAL_IP}:5003 --network=host cccs/assemblyline-service-resultsample\" , \"runOptions\" : { \"instanceLimit\" : 1 } }, ... Essentially, this runs the docker run command and specifies where the service server API is located. You can change the LOCAL_IP environment variable if your Docker subnet is different. If you want to make sure Docker's local IP is indeed the default, 172.17.0.1 , just run this command: ip addr show docker0 | grep \"inet \" | awk '{print $2}' | cut -f1 -d \"/\" Docker-compose tasks \u00b6 The docker-compose tasks are used to run sets of predefined dependencies in the system. Dependencies \u00b6 Before trying to run anything, at the bare minimum you will need one of the two Dependencies tasks running. Dependencies (Basic) will run the bare minimum set of containers to start components in the system: Elasticsearch, Redis, Minio, and Nginx. Dependencies (Basic + Kibana) will run the same containers as the Basic task but will add Kibana, Filebeat, and APM so that you can have access to the Kibana dashboard and debug your system more efficiently. Core components \u00b6 The core components docker-compose task runs all Assemblyline core components: Service server Frontend API Server Socket IO Server Alerter Expiry Metrics Heartbeats Statistics Workflow Plumber Dispatcher Ingester Tip If you are wondering what each component does, you should read the Infrastructure documentation which gives a brief description of each component. The core components task will also add two test users to the system: uid password is_admin apikey user user no devkey:user admin admin yes devkey:admin Scaler and Updater \u00b6 This task is an extra core component task that will launch Updater and Scaler. This task requires you to run one of the dependency tasks as well as the core components task to run properly. Scaler and Updater have been separated from the core components task because they interfere with running services live in the system. You are unlikely to ever run this task unless you are working on an issue loading containers from Scaler or Updater. Services \u00b6 This task will register all services to the system and allow them to be instantiated by Scaler and Updater. You are unlikely to run this task unless you want a working dev system that can scan files just like an appliance can. Pytest dependencies \u00b6 The Pytest dependencies tasks are used to set up the environment properly to be able to run the tests like if you were building the packages. There are 4 possible dependencies for the tests: Base - To run the tests for the assemblyline-base repository Core - To run the tests for the assemblyline-core repository Service Server - To run the tests for the assemblyline-service-server repository UI or All - To run the tests for the assemblyline-ui repository or any other repository for that matter Tip In most cases, if you are running tests you can use UI or All tasks because all the tests will work with it, but you might want to use the other tasks to save on resources and on dependency loading time. Launching components in debug mode \u00b6 Using debug mode on all of Assemblyline's components will probably be the single most useful thing that is pre-configured by the setup script . All Assemblyline's components get a launch target out of the box to simplify your debugging needs. Simply click the Run/Debug quick access side menu, select the component that you want to launch, and click the play button. The configured launch targets have been pre-fixed with category to help you identify what they do: Core - Core components unrelated to services Data - Script that generates random data in the system Service Server - Service API server Service - Service Live debugging UI - Assemblyline API Servers Warning To be able to successfully run these components, you will have to run at minimum the Dependencies (Basic) tasks Core, Service Server, and UI \u00b6 The core, service server, and UI categories of launch targets are mostly self-explanatory. They will launch in debug mode any of the core components from the system. Tip If you are wondering what each component does, you should read the Infrastructure which gives a brief description of every single one of them. CLI - Command Line Interface \u00b6 This launch target from the core category launches an interactive console that will let perform a specific task in the system. You can use the help command to find out what possible commands are available. Warning The commands that are found in this interactive console can be very dangerous and most of them should not be run on a production system. Data Launch target \u00b6 The launch targets in the data category are used to create random data in the system. Create default users \u00b6 This launch target will generate the same default user as the Core components tasks does: uid password is_admin apikey user user no devkey:user admin admin yes devkey:admin Generate random data \u00b6 In addition to creating the default users, this task will also use the random data generator to fill the different indexes of the system with data that has been randomly generated. This is especially useful when testing APIs or Frontend which require you to have data in the different indices. Running live services \u00b6 Running services in a live Assemblyline dev environment is the most important thing to know while building a service. It will allow you to send files via the user interface and put breakpoints in your service to catch files as they come through for processing. To run a service live in the system you will need to launch two components: Task Handler Your service Warning Here are a few things to consider before running a service live in debug mode: You need to start both the Dependencies (Basic) and the Core components tasks before starting your service to ensure that it will receive files. You can have one Task handler instance running at the time therefore can only debug one service at the time. Task handler communicates with the service via fixed named pipes in the /tmp directory which is the reason for that limitation. The first time you start a service, you'll have to start the task handler and your service twice because the service stops itself after registering in the DB. You should launch your service from the VSCode window pointing to your services git folder ( ~/git/services ) and for it to exist, you should have run the setup_script.sh with the -s option. Create a launch target for your service \u00b6 To add a launch target for your service, you will have to modify the .vscode/launch.json file in your ~/git/services directory. You can mimic the Safelist launch target as a baseline. Demo Safelist launch target ... { \"name\" : \"[Service] Safelist - LIVE\" , \"type\" : \"python\" , \"request\" : \"launch\" , \"module\" : \"assemblyline_v4_service.run_service\" , \"env\" : { \"SERVICE_PATH\" : \"safelist.Safelist\" }, \"console\" : \"internalConsole\" , \"cwd\" : \"${workspaceFolder}/assemblyline-service-safelist\" }, ... The only three things you will have to change for launching your service are the following The name of the launch target so you can pick it from the list later The environment variable SERVICE_PATH . This should be the Python path to the service class relative to your current working directory. The current working directory. This should be the directory where your service code is, a new directory you've created for your service inside the ~/git/services folder. Launch the service \u00b6 Launching the service will take place again in the VSCode window dedicated to services. From the Run/Debug quick access sidebar, you will launch the Task handler first then the launch target for your service that you've just created. Your service should now be running in live mode! Running Tests \u00b6 After running the setup script for VSCode, the testing interface from the VSCode Python extension will be shown in your quick access side menu and will present you with all available unit tests. You can then simply click the test or group of tests you want to run and hit the play button to run the tests. Warning Do not forget to first launch the appropriate Pytest Dependency from the tasks otherwise all the tests will fail. No tests available? If the testing interface is not showing any tests, load up the 'UI and All' Pytest Dependency task and hit the \"Reload tests\" button.","title":"Use VSCode"},{"location":"fr/developer_manual/env/vscode/use_vscode/#use-vscode","text":"Once you are done running the setup script , your installation of VSCode will be ready to run and debug any components of Assemblyline and most launch targets are already pre-configured. This page will point you in the right direction to perform some of the more common tasks that you'll have to do when developing an aspect of Assemblyline.","title":"Use VSCode"},{"location":"fr/developer_manual/env/vscode/use_vscode/#running-tasks","text":"After all recommended extensions are finished installing in VSCode, the task button on the sidebar should be revealed and will give you quick access to the most important tasks in the system. These tasks are split into 3 categories: Container - Run a single container for a single task Docker-compose - Execute a set of containers for a specific task dependency Pytest dependencies - Run necessary dependencies to run tests Tip You can edit the task list by modifying the .vscode/tasks.json . The default task.json file can be found in the assemblyline-development-setup repository.","title":"Running Tasks"},{"location":"fr/developer_manual/env/vscode/use_vscode/#container-tasks","text":"A container task executes one specific container in the system. Two of these tasks are predefined:","title":"Container tasks"},{"location":"fr/developer_manual/env/vscode/use_vscode/#frontend","text":"The Frontend task is used to run the User Interface of Assemblyline. It is only useful for when you launch the Assemblyline API Server in debug mode and are NOT using the core components task. Assemblyline's frontend is now built using ReactJS and is served via NPM serve which is why it is not part of this setup. Refer to the frontend development page for more information on how to do development on the Assemblyline frontend.","title":"Frontend"},{"location":"fr/developer_manual/env/vscode/use_vscode/#resultsample","text":"The ResultSample task was created to show the developers how to run newly created service containers in the system. Deep dive in the ResultSample Task This is what the JSON block for executing the ResultSample service in VSCode looks like: ... { \"label\" : \"Container - ResultSample\" , \"type\" : \"shell\" , \"options\" : { \"env\" : { \"LOCAL_IP\" : \"172.17.0.1\" } }, \"command\" : \"docker run --env SERVICE_API_HOST=http://${LOCAL_IP}:5003 --network=host cccs/assemblyline-service-resultsample\" , \"runOptions\" : { \"instanceLimit\" : 1 } }, ... Essentially, this runs the docker run command and specifies where the service server API is located. You can change the LOCAL_IP environment variable if your Docker subnet is different. If you want to make sure Docker's local IP is indeed the default, 172.17.0.1 , just run this command: ip addr show docker0 | grep \"inet \" | awk '{print $2}' | cut -f1 -d \"/\"","title":"ResultSample"},{"location":"fr/developer_manual/env/vscode/use_vscode/#docker-compose-tasks","text":"The docker-compose tasks are used to run sets of predefined dependencies in the system.","title":"Docker-compose tasks"},{"location":"fr/developer_manual/env/vscode/use_vscode/#dependencies","text":"Before trying to run anything, at the bare minimum you will need one of the two Dependencies tasks running. Dependencies (Basic) will run the bare minimum set of containers to start components in the system: Elasticsearch, Redis, Minio, and Nginx. Dependencies (Basic + Kibana) will run the same containers as the Basic task but will add Kibana, Filebeat, and APM so that you can have access to the Kibana dashboard and debug your system more efficiently.","title":"Dependencies"},{"location":"fr/developer_manual/env/vscode/use_vscode/#core-components","text":"The core components docker-compose task runs all Assemblyline core components: Service server Frontend API Server Socket IO Server Alerter Expiry Metrics Heartbeats Statistics Workflow Plumber Dispatcher Ingester Tip If you are wondering what each component does, you should read the Infrastructure documentation which gives a brief description of each component. The core components task will also add two test users to the system: uid password is_admin apikey user user no devkey:user admin admin yes devkey:admin","title":"Core components"},{"location":"fr/developer_manual/env/vscode/use_vscode/#scaler-and-updater","text":"This task is an extra core component task that will launch Updater and Scaler. This task requires you to run one of the dependency tasks as well as the core components task to run properly. Scaler and Updater have been separated from the core components task because they interfere with running services live in the system. You are unlikely to ever run this task unless you are working on an issue loading containers from Scaler or Updater.","title":"Scaler and Updater"},{"location":"fr/developer_manual/env/vscode/use_vscode/#services","text":"This task will register all services to the system and allow them to be instantiated by Scaler and Updater. You are unlikely to run this task unless you want a working dev system that can scan files just like an appliance can.","title":"Services"},{"location":"fr/developer_manual/env/vscode/use_vscode/#pytest-dependencies","text":"The Pytest dependencies tasks are used to set up the environment properly to be able to run the tests like if you were building the packages. There are 4 possible dependencies for the tests: Base - To run the tests for the assemblyline-base repository Core - To run the tests for the assemblyline-core repository Service Server - To run the tests for the assemblyline-service-server repository UI or All - To run the tests for the assemblyline-ui repository or any other repository for that matter Tip In most cases, if you are running tests you can use UI or All tasks because all the tests will work with it, but you might want to use the other tasks to save on resources and on dependency loading time.","title":"Pytest dependencies"},{"location":"fr/developer_manual/env/vscode/use_vscode/#launching-components-in-debug-mode","text":"Using debug mode on all of Assemblyline's components will probably be the single most useful thing that is pre-configured by the setup script . All Assemblyline's components get a launch target out of the box to simplify your debugging needs. Simply click the Run/Debug quick access side menu, select the component that you want to launch, and click the play button. The configured launch targets have been pre-fixed with category to help you identify what they do: Core - Core components unrelated to services Data - Script that generates random data in the system Service Server - Service API server Service - Service Live debugging UI - Assemblyline API Servers Warning To be able to successfully run these components, you will have to run at minimum the Dependencies (Basic) tasks","title":"Launching components in debug mode"},{"location":"fr/developer_manual/env/vscode/use_vscode/#core-service-server-and-ui","text":"The core, service server, and UI categories of launch targets are mostly self-explanatory. They will launch in debug mode any of the core components from the system. Tip If you are wondering what each component does, you should read the Infrastructure which gives a brief description of every single one of them.","title":"Core, Service Server, and UI"},{"location":"fr/developer_manual/env/vscode/use_vscode/#cli-command-line-interface","text":"This launch target from the core category launches an interactive console that will let perform a specific task in the system. You can use the help command to find out what possible commands are available. Warning The commands that are found in this interactive console can be very dangerous and most of them should not be run on a production system.","title":"CLI - Command Line Interface"},{"location":"fr/developer_manual/env/vscode/use_vscode/#data-launch-target","text":"The launch targets in the data category are used to create random data in the system.","title":"Data Launch target"},{"location":"fr/developer_manual/env/vscode/use_vscode/#create-default-users","text":"This launch target will generate the same default user as the Core components tasks does: uid password is_admin apikey user user no devkey:user admin admin yes devkey:admin","title":"Create default users"},{"location":"fr/developer_manual/env/vscode/use_vscode/#generate-random-data","text":"In addition to creating the default users, this task will also use the random data generator to fill the different indexes of the system with data that has been randomly generated. This is especially useful when testing APIs or Frontend which require you to have data in the different indices.","title":"Generate random data"},{"location":"fr/developer_manual/env/vscode/use_vscode/#running-live-services","text":"Running services in a live Assemblyline dev environment is the most important thing to know while building a service. It will allow you to send files via the user interface and put breakpoints in your service to catch files as they come through for processing. To run a service live in the system you will need to launch two components: Task Handler Your service Warning Here are a few things to consider before running a service live in debug mode: You need to start both the Dependencies (Basic) and the Core components tasks before starting your service to ensure that it will receive files. You can have one Task handler instance running at the time therefore can only debug one service at the time. Task handler communicates with the service via fixed named pipes in the /tmp directory which is the reason for that limitation. The first time you start a service, you'll have to start the task handler and your service twice because the service stops itself after registering in the DB. You should launch your service from the VSCode window pointing to your services git folder ( ~/git/services ) and for it to exist, you should have run the setup_script.sh with the -s option.","title":"Running live services"},{"location":"fr/developer_manual/env/vscode/use_vscode/#create-a-launch-target-for-your-service","text":"To add a launch target for your service, you will have to modify the .vscode/launch.json file in your ~/git/services directory. You can mimic the Safelist launch target as a baseline. Demo Safelist launch target ... { \"name\" : \"[Service] Safelist - LIVE\" , \"type\" : \"python\" , \"request\" : \"launch\" , \"module\" : \"assemblyline_v4_service.run_service\" , \"env\" : { \"SERVICE_PATH\" : \"safelist.Safelist\" }, \"console\" : \"internalConsole\" , \"cwd\" : \"${workspaceFolder}/assemblyline-service-safelist\" }, ... The only three things you will have to change for launching your service are the following The name of the launch target so you can pick it from the list later The environment variable SERVICE_PATH . This should be the Python path to the service class relative to your current working directory. The current working directory. This should be the directory where your service code is, a new directory you've created for your service inside the ~/git/services folder.","title":"Create a launch target for your service"},{"location":"fr/developer_manual/env/vscode/use_vscode/#launch-the-service","text":"Launching the service will take place again in the VSCode window dedicated to services. From the Run/Debug quick access sidebar, you will launch the Task handler first then the launch target for your service that you've just created. Your service should now be running in live mode!","title":"Launch the service"},{"location":"fr/developer_manual/env/vscode/use_vscode/#running-tests","text":"After running the setup script for VSCode, the testing interface from the VSCode Python extension will be shown in your quick access side menu and will present you with all available unit tests. You can then simply click the test or group of tests you want to run and hit the play button to run the tests. Warning Do not forget to first launch the appropriate Pytest Dependency from the tasks otherwise all the tests will fail. No tests available? If the testing interface is not showing any tests, load up the 'UI and All' Pytest Dependency task and hit the \"Reload tests\" button.","title":"Running Tests"},{"location":"fr/developer_manual/frontend/frontend/","text":"Assemblyline frontend development \u00b6 This documentation will show you how to set up your environment for Assemblyline frontend development. Install development environment prerequisites \u00b6 Clone the UI frontend code \u00b6 cd ~/git git clone https://github.com/CybercentreCanada/assemblyline-ui-frontend.git Install NodeJS (Ubuntu) \u00b6 Follow these simple commands to install NodeJS curl -sL https://deb.nodesource.com/setup_14.x | sudo -E bash - sudo apt-get install -y nodejs Install NPM dependencies \u00b6 Go to your assemblyline-ui-frontend directory and type: npm install Install Docker (Ubuntu) \u00b6 Follow these simple commands to get Docker running on your machine: # Add Docker repository sudo apt-get update sudo apt-get install -y apt-transport-https ca-certificates curl gnupg-agent software-properties-common curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - sudo apt-key fingerprint 0EBFCD88 sudo add-apt-repository \"deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\" # Install Docker sudo apt-get install -y docker-ce docker-ce-cli containerd.io # Test Docker installation sudo docker run hello-world Install docker-compose \u00b6 Installing docker-compose is done the same way on all Linux distributions. Follow these simple instructions: # Install docker-compose sudo curl -L \"https://github.com/docker/compose/releases/download/1.28.5/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker-compose sudo chmod +x /usr/local/bin/docker-compose # Test docker-compose installation docker-compose --version For reference, here are the instructions on Docker\u2019s website: https://docs.docker.com/compose/install/ Configure the development environment \u00b6 Setup Webpack for debugging behind a proxy \u00b6 Create a file named .env.local at the root of the assemblyline-ui-frontend directory. The file should only contain the following where <YOUR_IP> is replaced by your development computer IP. HOST=<YOUR_IP>.nip.io Setup docker-compose environment \u00b6 Setup IP routing \u00b6 Create a file in the docker directory named .env . This file should only contain the following where <YOUR_IP> is replaced by your development computer IP. EXTERNAL_IP=<YOUR_IP> Setup Assemblyline configuration file \u00b6 From the docker directory, copy the file config.yml.template to config.yml in the same directory. Change the <YOUR_IP> in the newly created config.yml file to the IP of your development machine. Setup Assemblyline classification engine file \u00b6 From the docker directory, copy the file classification.yml.template to classification.yml in the same directory. Change the enforce value to true in the classification.yml file to turn on the classification engine. Launch the dev environment \u00b6 Dependencies \u00b6 Go to the docker directory and run the following command to launch the Assemblyline database and user interface. docker-compose up Frontend \u00b6 Use the npm start script to launch the frontend. Once dependencies and frontend started \u00b6 Access the dev frontend at the following link: https://<YOUR_IP>.nip.io","title":"Frontend"},{"location":"fr/developer_manual/frontend/frontend/#assemblyline-frontend-development","text":"This documentation will show you how to set up your environment for Assemblyline frontend development.","title":"Assemblyline frontend development"},{"location":"fr/developer_manual/frontend/frontend/#install-development-environment-prerequisites","text":"","title":"Install development environment prerequisites"},{"location":"fr/developer_manual/frontend/frontend/#clone-the-ui-frontend-code","text":"cd ~/git git clone https://github.com/CybercentreCanada/assemblyline-ui-frontend.git","title":"Clone the UI frontend code"},{"location":"fr/developer_manual/frontend/frontend/#install-nodejs-ubuntu","text":"Follow these simple commands to install NodeJS curl -sL https://deb.nodesource.com/setup_14.x | sudo -E bash - sudo apt-get install -y nodejs","title":"Install NodeJS (Ubuntu)"},{"location":"fr/developer_manual/frontend/frontend/#install-npm-dependencies","text":"Go to your assemblyline-ui-frontend directory and type: npm install","title":"Install NPM dependencies"},{"location":"fr/developer_manual/frontend/frontend/#install-docker-ubuntu","text":"Follow these simple commands to get Docker running on your machine: # Add Docker repository sudo apt-get update sudo apt-get install -y apt-transport-https ca-certificates curl gnupg-agent software-properties-common curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - sudo apt-key fingerprint 0EBFCD88 sudo add-apt-repository \"deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\" # Install Docker sudo apt-get install -y docker-ce docker-ce-cli containerd.io # Test Docker installation sudo docker run hello-world","title":"Install Docker (Ubuntu)"},{"location":"fr/developer_manual/frontend/frontend/#install-docker-compose","text":"Installing docker-compose is done the same way on all Linux distributions. Follow these simple instructions: # Install docker-compose sudo curl -L \"https://github.com/docker/compose/releases/download/1.28.5/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker-compose sudo chmod +x /usr/local/bin/docker-compose # Test docker-compose installation docker-compose --version For reference, here are the instructions on Docker\u2019s website: https://docs.docker.com/compose/install/","title":"Install docker-compose"},{"location":"fr/developer_manual/frontend/frontend/#configure-the-development-environment","text":"","title":"Configure the development environment"},{"location":"fr/developer_manual/frontend/frontend/#setup-webpack-for-debugging-behind-a-proxy","text":"Create a file named .env.local at the root of the assemblyline-ui-frontend directory. The file should only contain the following where <YOUR_IP> is replaced by your development computer IP. HOST=<YOUR_IP>.nip.io","title":"Setup Webpack for debugging behind a proxy"},{"location":"fr/developer_manual/frontend/frontend/#setup-docker-compose-environment","text":"","title":"Setup docker-compose environment"},{"location":"fr/developer_manual/frontend/frontend/#setup-ip-routing","text":"Create a file in the docker directory named .env . This file should only contain the following where <YOUR_IP> is replaced by your development computer IP. EXTERNAL_IP=<YOUR_IP>","title":"Setup IP routing"},{"location":"fr/developer_manual/frontend/frontend/#setup-assemblyline-configuration-file","text":"From the docker directory, copy the file config.yml.template to config.yml in the same directory. Change the <YOUR_IP> in the newly created config.yml file to the IP of your development machine.","title":"Setup Assemblyline configuration file"},{"location":"fr/developer_manual/frontend/frontend/#setup-assemblyline-classification-engine-file","text":"From the docker directory, copy the file classification.yml.template to classification.yml in the same directory. Change the enforce value to true in the classification.yml file to turn on the classification engine.","title":"Setup Assemblyline classification engine file"},{"location":"fr/developer_manual/frontend/frontend/#launch-the-dev-environment","text":"","title":"Launch the dev environment"},{"location":"fr/developer_manual/frontend/frontend/#dependencies","text":"Go to the docker directory and run the following command to launch the Assemblyline database and user interface. docker-compose up","title":"Dependencies"},{"location":"fr/developer_manual/frontend/frontend/#frontend","text":"Use the npm start script to launch the frontend.","title":"Frontend"},{"location":"fr/developer_manual/frontend/frontend/#once-dependencies-and-frontend-started","text":"Access the dev frontend at the following link: https://<YOUR_IP>.nip.io","title":"Once dependencies and frontend started"},{"location":"fr/developer_manual/services/adding_a_service_updater/","text":"Adding a Service Updater \u00b6 This documentation builds on Developing an Assemblyline service in the event where you have a service that's dependent on signatures for analysis (ie. YARA or Suricata rules) Build your updater \u00b6 You will need to subclass the ServiceUpdater and implement/override functions as deemed necessary. Refer to ServiceUpdater class for more details. Let's say the following code is written in update_server.py in the root of your service directory: from assemblyline.odm.models.signature import Signature from assemblyline_v4_service.updater.updater import ServiceUpdater class SampleUpdateServer ( ServiceUpdater ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) def import_update ( self , files_sha256 , client , source , default_classification ): -> None # Purpose: Used to import a set of signatures from a source into Assemblyline for signature management # Inputs: # files_sha256: A list of tuples containing file paths and their respective sha256 # client: An Assemblyline client used to interact with the API on behalf of a service account # source: The name of the source # default_classification: The default classification given to a signature if none is provided signatures = [] for file , _ in files_sha256 : # Iterate over all the files retrieved by source and upload them as signatures in Assemblyline with open ( file , 'r' ) as fh : signatures . append ( Signature ( dict ( classification = default_classification , data = fh . read (), name = f 'sample_signature { len ( signatures ) } ' , source = source_name , status = 'DEPLOYED' , type = 'sample' ))) client . signatures . add_update_many ( signatures ) self . log . info ( f 'Successfully imported { len ( signatures ) } signatures' ) return def is_valid ( self , file_path ) -> bool : # Purpose: Used to determine if the file associated is 'valid' to be processed as a signature # Inputs: # file_path: Path to a signature file from an external source return super () . is_valid ( file_path ) #Returns true always if __name__ == '__main__' : with SampleUpdateServer ( default_pattern = \"*.json\" ) as server : server . serve_forever () Add it to the manifest! \u00b6 In addition to your service manifest, you would append the following: Warning The updater dependency needs to be named 'updates' for the service to recognize it as an updater rather than a normal dependency container. !!! critical Updaters need to run_as_core which allows them to run at the same level as other core containers. # Adding your dependency called 'updates' and specify the command the container should run dependencies : updates : container : allow_internet_access : true command : [ \"python\" , \"-m\" , \"update_server\" ] image : ${REGISTRY}testing/assemblyline-service-sample:latest ports : [ \"5003\" ] run_as_core : True # Update configuration block update_config : # list of source object from where to fetch files for update and what will be the name of those files on disk sources : - uri : https://file-examples-com.github.io/uploads/2017/02/file_example_JSON_1kb.json name : sample_1kb_file # interval in seconds at which the updater dependency runs update_interval_seconds : 300 # Should the downloaded files be used to create signatures in the system generates_signatures : true","title":"Adding a Service Updater"},{"location":"fr/developer_manual/services/adding_a_service_updater/#adding-a-service-updater","text":"This documentation builds on Developing an Assemblyline service in the event where you have a service that's dependent on signatures for analysis (ie. YARA or Suricata rules)","title":"Adding a Service Updater"},{"location":"fr/developer_manual/services/adding_a_service_updater/#build-your-updater","text":"You will need to subclass the ServiceUpdater and implement/override functions as deemed necessary. Refer to ServiceUpdater class for more details. Let's say the following code is written in update_server.py in the root of your service directory: from assemblyline.odm.models.signature import Signature from assemblyline_v4_service.updater.updater import ServiceUpdater class SampleUpdateServer ( ServiceUpdater ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) def import_update ( self , files_sha256 , client , source , default_classification ): -> None # Purpose: Used to import a set of signatures from a source into Assemblyline for signature management # Inputs: # files_sha256: A list of tuples containing file paths and their respective sha256 # client: An Assemblyline client used to interact with the API on behalf of a service account # source: The name of the source # default_classification: The default classification given to a signature if none is provided signatures = [] for file , _ in files_sha256 : # Iterate over all the files retrieved by source and upload them as signatures in Assemblyline with open ( file , 'r' ) as fh : signatures . append ( Signature ( dict ( classification = default_classification , data = fh . read (), name = f 'sample_signature { len ( signatures ) } ' , source = source_name , status = 'DEPLOYED' , type = 'sample' ))) client . signatures . add_update_many ( signatures ) self . log . info ( f 'Successfully imported { len ( signatures ) } signatures' ) return def is_valid ( self , file_path ) -> bool : # Purpose: Used to determine if the file associated is 'valid' to be processed as a signature # Inputs: # file_path: Path to a signature file from an external source return super () . is_valid ( file_path ) #Returns true always if __name__ == '__main__' : with SampleUpdateServer ( default_pattern = \"*.json\" ) as server : server . serve_forever ()","title":"Build your updater"},{"location":"fr/developer_manual/services/adding_a_service_updater/#add-it-to-the-manifest","text":"In addition to your service manifest, you would append the following: Warning The updater dependency needs to be named 'updates' for the service to recognize it as an updater rather than a normal dependency container. !!! critical Updaters need to run_as_core which allows them to run at the same level as other core containers. # Adding your dependency called 'updates' and specify the command the container should run dependencies : updates : container : allow_internet_access : true command : [ \"python\" , \"-m\" , \"update_server\" ] image : ${REGISTRY}testing/assemblyline-service-sample:latest ports : [ \"5003\" ] run_as_core : True # Update configuration block update_config : # list of source object from where to fetch files for update and what will be the name of those files on disk sources : - uri : https://file-examples-com.github.io/uploads/2017/02/file_example_JSON_1kb.json name : sample_1kb_file # interval in seconds at which the updater dependency runs update_interval_seconds : 300 # Should the downloaded files be used to create signatures in the system generates_signatures : true","title":"Add it to the manifest!"},{"location":"fr/developer_manual/services/developing_an_assemblyline_service/","text":"Developing an Assemblyline service \u00b6 This guide has been created for developers who are looking to develop services for Assemblyline. It is aimed at individuals with general software development knowledge and basic Python skills. In-depth knowledge of the Assemblyline framework is not required to develop a service. Pre-requisites \u00b6 Before getting started, ensure you have read through the setup environment documentation and created the appropriate development environment to perform service development. Build your first service \u00b6 This section will guide you through the bare minimum steps required to create a running, but functionally useless service. Each sub-section below outlines the steps required for each of the different files required to create an Assemblyline service. All files created in the following sub-sections must be placed in a common directory. Important For this documentation, we will assume that your new service directory is located at ~/git/services/assemblyline-service-sample Service Python code \u00b6 In your service directory, you will first start by creating your service's python file. Let's use sample.py . Put the following code in your service's file: ~/git/services/assemblyline-service-sample/sample.py from assemblyline_v4_service.common.base import ServiceBase from assemblyline_v4_service.common.result import Result , ResultSection class Sample ( ServiceBase ): def __init__ ( self , config = None ): super ( Sample , self ) . __init__ ( config ) def start ( self ): # ================================================================== # On Startup actions: # Your service might have to do some warming up on startup to make things faster self . log . info ( f \"start() from { self . service_attributes . name } service called\" ) def execute ( self , request ): # ================================================================== # Execute a request: # Every time your service receives a new file to scan, the execute function is called # This is where you should execute your processing code. # For this example, we will only generate results ... # ================================================================== # 1. Create a result object where all the result sections will be saved to result = Result () # 2. Create a section to be displayed for this result text_section = ResultSection ( 'Example of a default section' ) # 2.1. Add lines to your section text_section . add_line ( \"This is a line displayed in the body of the section\" ) # 2.2. Your section can generate a score. To do this it needs to fire a heuristic. # We will fire heuristic #1 text_section . set_heuristic ( 1 ) # 2.3. Your section can add tags, we will add a fake one text_section . add_tag ( \"network.static.domain\" , \"cyber.gc.ca\" ) # 3. Make sure you add your section to the result result . add_section ( text_section ) # 4. Wrap-up: Save your result object back into the request request . result = result Service manifest YAML \u00b6 Now that you have a better understanding of the python portion of a service. We will create the associated manifest file that holds the different configurations for the service. In your service directory, you will add the YAML configuration file service_manifest.yml with the following content: ~/git/services/assembyline-service-sample/service_manifest.yml # Name of the service name : Sample # Version of the service version : 4.0.0.dev0 # Description of the service description : ALv4 Sample service from the documentation # Regex defining the types of files the service accepts and rejects accepts : .* rejects : empty # At which stage the service should run (one of FILTER, EXTRACT, CORE, SECONDARY, POST) # NOTE: Stages are executed in the order defined in the list stage : CORE # Which category the service is part of (one of Antivirus, Dynamic Analysis, External, Extraction, Filtering, Networking, Static Analysis) category : Static Analysis # Does the service require access to the file to perform its task # If set to false, the service will only have access to the file metadata (e.g. Hashes, size, type, ...) file_required : true # Maximum execution time the service has before it's considered to be timed out timeout : 10 # is the service enabled by default enabled : true # Service heuristic blocks: List of heuristic objects that define the different heuristics used in the service heuristics : - description : This is a demo heuristic filetype : \"*\" heur_id : 1 name : Demo score : 100 # Docker configuration block which defines: # - the name of the docker container that will be created # - CPU and ram allocation by the container docker_config : image : ${REGISTRY}testing/assemblyline-service-sample:latest cpu_cores : 1.0 ram_mb : 1024 Important The service_manifest.yml has a lot more configurable parameters that you might be required to change depending on the service you are building. You should get familiar with the complete list by reading the service manifest advanced documentation. Dockerfile \u00b6 Finally, the last file needed to complete your assemblyline service is the Dockerfile that is used to create its Docker container. In your service directory, create a file named Dockerfile with the following content: ~/git/services/assemblyline-service-sample/Dockerfile FROM cccs/assemblyline-v4-service-base:stable # Python path to the service class from your service directory # The following example refers to the class \"Sample\" from the \"sample.py\" file ENV SERVICE_PATH sample.Sample # Install any service dependencies here # For example: RUN apt-get update && apt-get install -y libyaml-dev # RUN pip install utils # Switch to assemblyline user USER assemblyline # Copy Sample service code WORKDIR /opt/al_service COPY . . The Dockerfile is required to build a Docker container. When developing a Docker container for an Assemblyline service, the following must be ensured: The parent image must be cccs/assemblyline-v4-service-base:stable so you are using a stable build of service base. An environment variable named SERVICE_PATH must be set whose value defines the Python module path to the main service class which inherits from the ServiceBase class. Any dependency installation must be completed as the root user, which is set by default in the parent image. Once all dependency installations have been completed, you must change the user to assemblyline . The service code and any dependency files must be copied to the /opt/al_service directory. Conclusion \u00b6 After you've completed creating your first service, your ~/git/services/assemblyline-service-sample directory should have the following files at a minimum: . \u251c\u2500\u2500 Dockerfile \u251c\u2500\u2500 result_sample.py \u2514\u2500\u2500 service_manifest.yml","title":"Developing a service"},{"location":"fr/developer_manual/services/developing_an_assemblyline_service/#developing-an-assemblyline-service","text":"This guide has been created for developers who are looking to develop services for Assemblyline. It is aimed at individuals with general software development knowledge and basic Python skills. In-depth knowledge of the Assemblyline framework is not required to develop a service.","title":"Developing an Assemblyline service"},{"location":"fr/developer_manual/services/developing_an_assemblyline_service/#pre-requisites","text":"Before getting started, ensure you have read through the setup environment documentation and created the appropriate development environment to perform service development.","title":"Pre-requisites"},{"location":"fr/developer_manual/services/developing_an_assemblyline_service/#build-your-first-service","text":"This section will guide you through the bare minimum steps required to create a running, but functionally useless service. Each sub-section below outlines the steps required for each of the different files required to create an Assemblyline service. All files created in the following sub-sections must be placed in a common directory. Important For this documentation, we will assume that your new service directory is located at ~/git/services/assemblyline-service-sample","title":"Build your first service"},{"location":"fr/developer_manual/services/developing_an_assemblyline_service/#service-python-code","text":"In your service directory, you will first start by creating your service's python file. Let's use sample.py . Put the following code in your service's file: ~/git/services/assemblyline-service-sample/sample.py from assemblyline_v4_service.common.base import ServiceBase from assemblyline_v4_service.common.result import Result , ResultSection class Sample ( ServiceBase ): def __init__ ( self , config = None ): super ( Sample , self ) . __init__ ( config ) def start ( self ): # ================================================================== # On Startup actions: # Your service might have to do some warming up on startup to make things faster self . log . info ( f \"start() from { self . service_attributes . name } service called\" ) def execute ( self , request ): # ================================================================== # Execute a request: # Every time your service receives a new file to scan, the execute function is called # This is where you should execute your processing code. # For this example, we will only generate results ... # ================================================================== # 1. Create a result object where all the result sections will be saved to result = Result () # 2. Create a section to be displayed for this result text_section = ResultSection ( 'Example of a default section' ) # 2.1. Add lines to your section text_section . add_line ( \"This is a line displayed in the body of the section\" ) # 2.2. Your section can generate a score. To do this it needs to fire a heuristic. # We will fire heuristic #1 text_section . set_heuristic ( 1 ) # 2.3. Your section can add tags, we will add a fake one text_section . add_tag ( \"network.static.domain\" , \"cyber.gc.ca\" ) # 3. Make sure you add your section to the result result . add_section ( text_section ) # 4. Wrap-up: Save your result object back into the request request . result = result","title":"Service Python code"},{"location":"fr/developer_manual/services/developing_an_assemblyline_service/#service-manifest-yaml","text":"Now that you have a better understanding of the python portion of a service. We will create the associated manifest file that holds the different configurations for the service. In your service directory, you will add the YAML configuration file service_manifest.yml with the following content: ~/git/services/assembyline-service-sample/service_manifest.yml # Name of the service name : Sample # Version of the service version : 4.0.0.dev0 # Description of the service description : ALv4 Sample service from the documentation # Regex defining the types of files the service accepts and rejects accepts : .* rejects : empty # At which stage the service should run (one of FILTER, EXTRACT, CORE, SECONDARY, POST) # NOTE: Stages are executed in the order defined in the list stage : CORE # Which category the service is part of (one of Antivirus, Dynamic Analysis, External, Extraction, Filtering, Networking, Static Analysis) category : Static Analysis # Does the service require access to the file to perform its task # If set to false, the service will only have access to the file metadata (e.g. Hashes, size, type, ...) file_required : true # Maximum execution time the service has before it's considered to be timed out timeout : 10 # is the service enabled by default enabled : true # Service heuristic blocks: List of heuristic objects that define the different heuristics used in the service heuristics : - description : This is a demo heuristic filetype : \"*\" heur_id : 1 name : Demo score : 100 # Docker configuration block which defines: # - the name of the docker container that will be created # - CPU and ram allocation by the container docker_config : image : ${REGISTRY}testing/assemblyline-service-sample:latest cpu_cores : 1.0 ram_mb : 1024 Important The service_manifest.yml has a lot more configurable parameters that you might be required to change depending on the service you are building. You should get familiar with the complete list by reading the service manifest advanced documentation.","title":"Service manifest YAML"},{"location":"fr/developer_manual/services/developing_an_assemblyline_service/#dockerfile","text":"Finally, the last file needed to complete your assemblyline service is the Dockerfile that is used to create its Docker container. In your service directory, create a file named Dockerfile with the following content: ~/git/services/assemblyline-service-sample/Dockerfile FROM cccs/assemblyline-v4-service-base:stable # Python path to the service class from your service directory # The following example refers to the class \"Sample\" from the \"sample.py\" file ENV SERVICE_PATH sample.Sample # Install any service dependencies here # For example: RUN apt-get update && apt-get install -y libyaml-dev # RUN pip install utils # Switch to assemblyline user USER assemblyline # Copy Sample service code WORKDIR /opt/al_service COPY . . The Dockerfile is required to build a Docker container. When developing a Docker container for an Assemblyline service, the following must be ensured: The parent image must be cccs/assemblyline-v4-service-base:stable so you are using a stable build of service base. An environment variable named SERVICE_PATH must be set whose value defines the Python module path to the main service class which inherits from the ServiceBase class. Any dependency installation must be completed as the root user, which is set by default in the parent image. Once all dependency installations have been completed, you must change the user to assemblyline . The service code and any dependency files must be copied to the /opt/al_service directory.","title":"Dockerfile"},{"location":"fr/developer_manual/services/developing_an_assemblyline_service/#conclusion","text":"After you've completed creating your first service, your ~/git/services/assemblyline-service-sample directory should have the following files at a minimum: . \u251c\u2500\u2500 Dockerfile \u251c\u2500\u2500 result_sample.py \u2514\u2500\u2500 service_manifest.yml","title":"Conclusion"},{"location":"fr/developer_manual/services/run_your_service/","text":"Run your service \u00b6 This section of the service documentation will show you how to run your service in 3 different ways: Standalone mode directly on a file Live code on an Assemblyline system Production container on an Assemblyline system Important This documentation assumes the following: You have read through the setup environment documentation and created the appropriate development environment to perform service development. You have completed the developing an Assemblyline service documentation and your service is in the ~/git/services/assemblyline-service-sample directory on the machine where your IDE runs. Whether you are using VSCode or PyCharm as your IDE, you have a virtual environment dedicated to running services either located at ~/git/services/venv or ~/venv/services on the VM where the code runs Standalone mode \u00b6 To test an Assemblyline service in standalone mode, the run_service_once module from the assemblyline-v4-service package can be used to run a single task through the service for testing. Running the Sample service \u00b6 Load the virtual environment source ~/git/services/venv/bin/activate Ensure the current working directory is the root of the service directory of the service to be run. cd ~/git/services/assemblyline-service-sample From a terminal, run the run_service_once module, specifying the service path for the service to run and the path to the file to scan. For this example, we will have the service scan itself. python -m assemblyline_v4_service.dev.run_service_once sample.Sample sample.py The run_service_once module creates a directory at the same spot where the file is found with the service name that scanned the file appended to it. In the previous example, the output of the service should be located at ~/git/services/assemblyline-service-sample/sample.py_sample . The directory will contain a result.json file containing the result from the service. You can view the result.json file using the following command: cat ~/git/services/assemblyline-service-sample/sample.py_sample/result.json | json_pp It will look something like this: ~/git/services/assemblyline-service-sample/sample.py_sample/result.json pretty printed { \"classification\" : \"TLP:W\" , \"drop_file\" : false , \"response\" : { \"extracted\" : [], \"milestones\" : { \"service_completed\" : \"2021-09-15T15:52:47.024909Z\" , \"service_started\" : \"2021-09-15T15:52:47.024761Z\" }, \"service_context\" : null , \"service_debug_info\" : null , \"service_name\" : \"sample\" , \"service_tool_version\" : null , \"service_version\" : \"1\" , \"supplementary\" : [] }, \"result\" : { \"score\" : 100 , \"sections\" : [ { \"auto_collapse\" : false , \"body\" : \"This is a line displayed in the body of the section\" , \"body_format\" : \"TEXT\" , \"classification\" : \"TLP:W\" , \"depth\" : 0 , \"heuristic\" : { \"attack_ids\" : [], \"frequency\" : 1 , \"heur_id\" : 1 , \"score\" : 100 , \"score_map\" : {}, \"signatures\" : {} }, \"tags\" : { \"network\" : { \"static\" : { \"domain\" : [ \"cyber.gc.ca\" ] } } }, \"title_text\" : \"Example of a default section\" , \"zeroize_on_tag_safe\" : false } ] }, \"sha256\" : \"dab595d88c22eba68e831e072471b02206d12cb29173c708c606416ecf50b942\" , \"temp_submission_data\" : {} } Live mode \u00b6 The following technique is how to hook in a service to an Assemblyline development instance so you can perform live debugging and you can send files to your service using the Assemblyline UI. The way to run a service in debug mode will differ depending on if you've been using VSCode or PyCharm as your IDE. Follow the appropriate documentation for your current setup: Run a service LIVE in VSCode Run a service LIVE in PyCharm Important You will need to adjust the documentation according to: The correct name for your service ( Sample ) The correct service python module for your service ( sample.Sample ) The correct working directory for your service ( ~/git/services/assemblyline-service-sample ) Run from a shell \u00b6 If you don't plan on doing any debugging and you just want to run the service live in your development environment, you can just spin up two shells and run Task Handler in one and your Sample service in the other. Task Handler \u00b6 # Load your service virtual environment source ~/git/services/venv/bin/activate # Run task handler python -m assemblyline_service_client.task_handler Sample service \u00b6 # Load your service virtual environment source ~/git/services/venv/bin/activate # Go to your service directory cd ~/git/services/assemblyline-service-sample # Run your service SERVICE_PATH = sample.Sample python -m assemblyline_v4_service.run_service Production container mode \u00b6 When you are confident your service is stable enough, it is time to test it in its final form: A Docker container. Build the container \u00b6 Change working directory to root of the service: cd ~/git/services/assemblyline-service-sample Run the docker build command and tag the container with the same name that the container name has in your service manifest docker build -t testing/assemblyline-service-sample . Run the container LIVE \u00b6 The way to run a container LIVE in your development environment differs depending on if you've been using VSCode or PyCharm as your IDE. Follow the appropriate documentation for your current setup: Run a single container in VSCode Run a single container in PyCharm Important You will need to adjust the documentation according to: The correct name for your service ( Sample ) The correct container name ( testing/assemblyline-service-sample ) Run container from a shell \u00b6 If you don't want to use the IDE to test your production container, you can always run it straight from a shell. Use the following command to run it: docker run --env SERVICE_API_HOST = http:// ` ip addr show docker0 | grep \"inet \" | awk '{print $2}' | cut -f1 -d \"/\" ` :5003 --network = host --name SampleService testing/assemblyline-service-sample Add the container to your deployment \u00b6 Note For the scaler and updater to be able to use your service container, they must be able to get it from a docker registry. You can either use DockerHub, a work central registry, or a local registry. Push the container to your local registry \u00b6 Tip A local docker registry should have already been installed during the development environment setup . If not, you can start one by simply running this command: sudo docker run -dp 32000 :5000 --restart = always --name registry registry Use the following to push your sample service to the local registry: docker tag testing/assemblyline-service-sample localhost:32000/testing/assemblyline-service-sample docker push --all-tags localhost:32000/testing/assemblyline-service-sample Add the service in the management interface \u00b6 Using your web browser, go to the service management page: https://localhost/admin/services (Replace localhost by your VM's IP) Click the Add service button Paste the entire content of the service_manifest.yml file from your service directory in the text box If you are using the local registry from this documentation, change the ${REGISTRY} from the content of service_manifest.yml to 172.17.0.1:32000/ Click the Add button Your service information has been added to the system. The scaler component should automatically start a container of your newly created service.","title":"Run your service"},{"location":"fr/developer_manual/services/run_your_service/#run-your-service","text":"This section of the service documentation will show you how to run your service in 3 different ways: Standalone mode directly on a file Live code on an Assemblyline system Production container on an Assemblyline system Important This documentation assumes the following: You have read through the setup environment documentation and created the appropriate development environment to perform service development. You have completed the developing an Assemblyline service documentation and your service is in the ~/git/services/assemblyline-service-sample directory on the machine where your IDE runs. Whether you are using VSCode or PyCharm as your IDE, you have a virtual environment dedicated to running services either located at ~/git/services/venv or ~/venv/services on the VM where the code runs","title":"Run your service"},{"location":"fr/developer_manual/services/run_your_service/#standalone-mode","text":"To test an Assemblyline service in standalone mode, the run_service_once module from the assemblyline-v4-service package can be used to run a single task through the service for testing.","title":"Standalone mode"},{"location":"fr/developer_manual/services/run_your_service/#running-the-sample-service","text":"Load the virtual environment source ~/git/services/venv/bin/activate Ensure the current working directory is the root of the service directory of the service to be run. cd ~/git/services/assemblyline-service-sample From a terminal, run the run_service_once module, specifying the service path for the service to run and the path to the file to scan. For this example, we will have the service scan itself. python -m assemblyline_v4_service.dev.run_service_once sample.Sample sample.py The run_service_once module creates a directory at the same spot where the file is found with the service name that scanned the file appended to it. In the previous example, the output of the service should be located at ~/git/services/assemblyline-service-sample/sample.py_sample . The directory will contain a result.json file containing the result from the service. You can view the result.json file using the following command: cat ~/git/services/assemblyline-service-sample/sample.py_sample/result.json | json_pp It will look something like this: ~/git/services/assemblyline-service-sample/sample.py_sample/result.json pretty printed { \"classification\" : \"TLP:W\" , \"drop_file\" : false , \"response\" : { \"extracted\" : [], \"milestones\" : { \"service_completed\" : \"2021-09-15T15:52:47.024909Z\" , \"service_started\" : \"2021-09-15T15:52:47.024761Z\" }, \"service_context\" : null , \"service_debug_info\" : null , \"service_name\" : \"sample\" , \"service_tool_version\" : null , \"service_version\" : \"1\" , \"supplementary\" : [] }, \"result\" : { \"score\" : 100 , \"sections\" : [ { \"auto_collapse\" : false , \"body\" : \"This is a line displayed in the body of the section\" , \"body_format\" : \"TEXT\" , \"classification\" : \"TLP:W\" , \"depth\" : 0 , \"heuristic\" : { \"attack_ids\" : [], \"frequency\" : 1 , \"heur_id\" : 1 , \"score\" : 100 , \"score_map\" : {}, \"signatures\" : {} }, \"tags\" : { \"network\" : { \"static\" : { \"domain\" : [ \"cyber.gc.ca\" ] } } }, \"title_text\" : \"Example of a default section\" , \"zeroize_on_tag_safe\" : false } ] }, \"sha256\" : \"dab595d88c22eba68e831e072471b02206d12cb29173c708c606416ecf50b942\" , \"temp_submission_data\" : {} }","title":"Running the Sample service"},{"location":"fr/developer_manual/services/run_your_service/#live-mode","text":"The following technique is how to hook in a service to an Assemblyline development instance so you can perform live debugging and you can send files to your service using the Assemblyline UI. The way to run a service in debug mode will differ depending on if you've been using VSCode or PyCharm as your IDE. Follow the appropriate documentation for your current setup: Run a service LIVE in VSCode Run a service LIVE in PyCharm Important You will need to adjust the documentation according to: The correct name for your service ( Sample ) The correct service python module for your service ( sample.Sample ) The correct working directory for your service ( ~/git/services/assemblyline-service-sample )","title":"Live mode"},{"location":"fr/developer_manual/services/run_your_service/#run-from-a-shell","text":"If you don't plan on doing any debugging and you just want to run the service live in your development environment, you can just spin up two shells and run Task Handler in one and your Sample service in the other.","title":"Run from a shell"},{"location":"fr/developer_manual/services/run_your_service/#task-handler","text":"# Load your service virtual environment source ~/git/services/venv/bin/activate # Run task handler python -m assemblyline_service_client.task_handler","title":"Task Handler"},{"location":"fr/developer_manual/services/run_your_service/#sample-service","text":"# Load your service virtual environment source ~/git/services/venv/bin/activate # Go to your service directory cd ~/git/services/assemblyline-service-sample # Run your service SERVICE_PATH = sample.Sample python -m assemblyline_v4_service.run_service","title":"Sample service"},{"location":"fr/developer_manual/services/run_your_service/#production-container-mode","text":"When you are confident your service is stable enough, it is time to test it in its final form: A Docker container.","title":"Production container mode"},{"location":"fr/developer_manual/services/run_your_service/#build-the-container","text":"Change working directory to root of the service: cd ~/git/services/assemblyline-service-sample Run the docker build command and tag the container with the same name that the container name has in your service manifest docker build -t testing/assemblyline-service-sample .","title":"Build the container"},{"location":"fr/developer_manual/services/run_your_service/#run-the-container-live","text":"The way to run a container LIVE in your development environment differs depending on if you've been using VSCode or PyCharm as your IDE. Follow the appropriate documentation for your current setup: Run a single container in VSCode Run a single container in PyCharm Important You will need to adjust the documentation according to: The correct name for your service ( Sample ) The correct container name ( testing/assemblyline-service-sample )","title":"Run the container LIVE"},{"location":"fr/developer_manual/services/run_your_service/#run-container-from-a-shell","text":"If you don't want to use the IDE to test your production container, you can always run it straight from a shell. Use the following command to run it: docker run --env SERVICE_API_HOST = http:// ` ip addr show docker0 | grep \"inet \" | awk '{print $2}' | cut -f1 -d \"/\" ` :5003 --network = host --name SampleService testing/assemblyline-service-sample","title":"Run container from a shell"},{"location":"fr/developer_manual/services/run_your_service/#add-the-container-to-your-deployment","text":"Note For the scaler and updater to be able to use your service container, they must be able to get it from a docker registry. You can either use DockerHub, a work central registry, or a local registry.","title":"Add the container to your deployment"},{"location":"fr/developer_manual/services/run_your_service/#push-the-container-to-your-local-registry","text":"Tip A local docker registry should have already been installed during the development environment setup . If not, you can start one by simply running this command: sudo docker run -dp 32000 :5000 --restart = always --name registry registry Use the following to push your sample service to the local registry: docker tag testing/assemblyline-service-sample localhost:32000/testing/assemblyline-service-sample docker push --all-tags localhost:32000/testing/assemblyline-service-sample","title":"Push the container to your local registry"},{"location":"fr/developer_manual/services/run_your_service/#add-the-service-in-the-management-interface","text":"Using your web browser, go to the service management page: https://localhost/admin/services (Replace localhost by your VM's IP) Click the Add service button Paste the entire content of the service_manifest.yml file from your service directory in the text box If you are using the local registry from this documentation, change the ${REGISTRY} from the content of service_manifest.yml to 172.17.0.1:32000/ Click the Add button Your service information has been added to the system. The scaler component should automatically start a container of your newly created service.","title":"Add the service in the management interface"},{"location":"fr/developer_manual/services/advanced/request/","text":"Request class \u00b6 The Request object is the parameter received by the service execute function. It holds information about the task to be processed by the service. You can view the source for the class here: Request class source Class variables \u00b6 The following table describes all of the Request object variables which the service can use. Variable name Description deep_scan Returns whether the file should be deep-scanned or not. Deep-scanning usually takes more time and is better suited for files that are sent manually. file_contents Returns the raw byte contents of the file to be scanned. file_name Returns the name of the file (as submitted by the user) to be scanned. file_path Returns the path to the file to be scanned. The service can use this path directly to access the file. file_type Returns the Assemblyline-style file type of the file to be scanned. max_extracted Returns the maximum number of files that are allowed to be extracted by a service. By default this is set to 500. md5 Returns the MD5 hash of the file to be scanned. result Used to get and set the current result. sha1 Returns the SHA1 hash of the file to be scanned. sha256 Returns the SHA256 hash of the file to be scanned. sid ID of the submission being scanned. task The original task object used to create this request. You can find more information there about the request (metadata submitted, files already extracted by other services, tags already generated by other services and more...) temp_submission_data Can be used to get and set temporary submission data which is passed onto subsequent tasks resulting from adding extracted files. Class functions \u00b6 The following table describes the Request object functions which the service can use. add_extracted() \u00b6 This function adds a file extracted by the service to the result. The extracted file will also be scanned through a set of services, as if it had been originally submitted. This function can take the following parameters: path : Complete path to the file name : Display name of the file description : Descriptive text about the file classification : Optional classification of the file Example Excerpt from Assemblyline ResultSample service: result_sample.py ... # ================================================================== # Re-Submitting files to the system # Adding extracted files will have them resubmitted to the system for analysis ... fd , temp_path = tempfile . mkstemp ( dir = self . working_directory ) with os . fdopen ( fd , \"wb\" ) as myfile : myfile . write ( b \"CLASSIFIED!!!__\" + data . encode ()) request . add_extracted ( temp_path , \"classified.doc\" , \"Classified file ... don't look\" , classification = cl_engine . RESTRICTED ) ... add_supplementary() \u00b6 This function adds a supplementary file generated by the service to the result. The supplementary file is uploaded for the user's informational use only and is not scanned. This function can take the following parameters: path : Complete path to the file name : Display name of the file description : Descriptive text about the file classification : Optional classification of the file Example Excerpt from Assemblyline ResultSample service: result_sample.py ... # ================================================================== # Supplementary files # Adding supplementary files will save them on the datastore for future # reference but won't reprocess those files. fd , temp_path = tempfile . mkstemp ( dir = self . working_directory ) with os . fdopen ( fd , \"w\" ) as myfile : myfile . write ( json . dumps ( urls )) request . add_supplementary ( temp_path , \"urls.json\" , \"These are urls as a JSON file\" ) ... drop() \u00b6 When called, the task will be dropped and will not be processed further by other remaining service(s). Example Excerpt from Assemblyline Safelist service: safelist.py ... # Stop processing, the file is safe request . drop () ... get_param() \u00b6 Retrieve a submission parameter for the task. This function can take the following parameter: name : name of the submission parameter to retrieve Example Excerpt from Assemblyline Extract service: extract.py ... def execute ( self , request : ServiceRequest ): ... continue_after_extract = request . get_param ( 'continue_after_extract' ) ... set_service_context() \u00b6 Set the context of the service which ran the file. For example, if the service ran an AntiVirus engine on the file, then the AntiVirus definition version would be the service context. This function can take the following parameters: context : Service context as string Example Excerpt from Assemblyline Metadefender service: metadefender.py ... def execute ( self , request : ServiceRequest ): ... request . set_service_context ( f \"Definition Time Range: { self . nodes [ self . current_node ][ 'oldest_dat' ] } - \" f \" { self . nodes [ self . current_node ][ 'newest_dat' ] } \" ) ...","title":"Request Class"},{"location":"fr/developer_manual/services/advanced/request/#request-class","text":"The Request object is the parameter received by the service execute function. It holds information about the task to be processed by the service. You can view the source for the class here: Request class source","title":"Request class"},{"location":"fr/developer_manual/services/advanced/request/#class-variables","text":"The following table describes all of the Request object variables which the service can use. Variable name Description deep_scan Returns whether the file should be deep-scanned or not. Deep-scanning usually takes more time and is better suited for files that are sent manually. file_contents Returns the raw byte contents of the file to be scanned. file_name Returns the name of the file (as submitted by the user) to be scanned. file_path Returns the path to the file to be scanned. The service can use this path directly to access the file. file_type Returns the Assemblyline-style file type of the file to be scanned. max_extracted Returns the maximum number of files that are allowed to be extracted by a service. By default this is set to 500. md5 Returns the MD5 hash of the file to be scanned. result Used to get and set the current result. sha1 Returns the SHA1 hash of the file to be scanned. sha256 Returns the SHA256 hash of the file to be scanned. sid ID of the submission being scanned. task The original task object used to create this request. You can find more information there about the request (metadata submitted, files already extracted by other services, tags already generated by other services and more...) temp_submission_data Can be used to get and set temporary submission data which is passed onto subsequent tasks resulting from adding extracted files.","title":"Class variables"},{"location":"fr/developer_manual/services/advanced/request/#class-functions","text":"The following table describes the Request object functions which the service can use.","title":"Class functions"},{"location":"fr/developer_manual/services/advanced/request/#add_extracted","text":"This function adds a file extracted by the service to the result. The extracted file will also be scanned through a set of services, as if it had been originally submitted. This function can take the following parameters: path : Complete path to the file name : Display name of the file description : Descriptive text about the file classification : Optional classification of the file Example Excerpt from Assemblyline ResultSample service: result_sample.py ... # ================================================================== # Re-Submitting files to the system # Adding extracted files will have them resubmitted to the system for analysis ... fd , temp_path = tempfile . mkstemp ( dir = self . working_directory ) with os . fdopen ( fd , \"wb\" ) as myfile : myfile . write ( b \"CLASSIFIED!!!__\" + data . encode ()) request . add_extracted ( temp_path , \"classified.doc\" , \"Classified file ... don't look\" , classification = cl_engine . RESTRICTED ) ...","title":"add_extracted()"},{"location":"fr/developer_manual/services/advanced/request/#add_supplementary","text":"This function adds a supplementary file generated by the service to the result. The supplementary file is uploaded for the user's informational use only and is not scanned. This function can take the following parameters: path : Complete path to the file name : Display name of the file description : Descriptive text about the file classification : Optional classification of the file Example Excerpt from Assemblyline ResultSample service: result_sample.py ... # ================================================================== # Supplementary files # Adding supplementary files will save them on the datastore for future # reference but won't reprocess those files. fd , temp_path = tempfile . mkstemp ( dir = self . working_directory ) with os . fdopen ( fd , \"w\" ) as myfile : myfile . write ( json . dumps ( urls )) request . add_supplementary ( temp_path , \"urls.json\" , \"These are urls as a JSON file\" ) ...","title":"add_supplementary()"},{"location":"fr/developer_manual/services/advanced/request/#drop","text":"When called, the task will be dropped and will not be processed further by other remaining service(s). Example Excerpt from Assemblyline Safelist service: safelist.py ... # Stop processing, the file is safe request . drop () ...","title":"drop()"},{"location":"fr/developer_manual/services/advanced/request/#get_param","text":"Retrieve a submission parameter for the task. This function can take the following parameter: name : name of the submission parameter to retrieve Example Excerpt from Assemblyline Extract service: extract.py ... def execute ( self , request : ServiceRequest ): ... continue_after_extract = request . get_param ( 'continue_after_extract' ) ...","title":"get_param()"},{"location":"fr/developer_manual/services/advanced/request/#set_service_context","text":"Set the context of the service which ran the file. For example, if the service ran an AntiVirus engine on the file, then the AntiVirus definition version would be the service context. This function can take the following parameters: context : Service context as string Example Excerpt from Assemblyline Metadefender service: metadefender.py ... def execute ( self , request : ServiceRequest ): ... request . set_service_context ( f \"Definition Time Range: { self . nodes [ self . current_node ][ 'oldest_dat' ] } - \" f \" { self . nodes [ self . current_node ][ 'newest_dat' ] } \" ) ...","title":"set_service_context()"},{"location":"fr/developer_manual/services/advanced/result/","text":"Result class \u00b6 All services in Assemblyline must create a Result object containing the different ResultSections which encapsulate their findings. This Result object must then be set as the Request's result parameter which will then be saved in the database to show to the user. You can view the source for the class here: Result class source Class variables \u00b6 Even though the Result class is critical for the service, it does not have many variables that should be used by the service. Variable Name Description sections List of all the ResultSection objects that have been added to the Result object so far. Class functions \u00b6 There is only one function that the service writer should ever use in a Result object. add_section() \u00b6 This function allows the service to add a ResultSection object to the current Result object. It can take the following parameters: section : The ResultSection object to add to the list on_top : (Optional) Boolean value that indicates if the section should be on top of the other sections or not Example Excerpt from Assemblyline ResultSample service: result_sample.py ... kv_body = { \"a_str\" : \"Some string\" , \"a_bool\" : False , \"an_int\" : 102 , } kv_section = ResultSection ( 'Example of a KEY_VALUE section' , body_format = BODY_FORMAT . KEY_VALUE , body = json . dumps ( kv_body )) result . add_section ( kv_section ) ... Tips on writing good results \u00b6 Here's a few tips on writing services: DO's Make sure your results are easily readable for the user Include only necessary information Collapse sections that are less important by default If your service has information that can be tagged, don't forget to add those tags Choose a ResultSection type that will represent your data well DON'T's If your service has nothing to say, do not create ResultSection objects that contain that the service found nothing. There are a lot of shortcuts that the system takes with empty results and this will circumvent them. Don't overwhelm the users with information or they will stop reading it and just skim through Don't resubmit too many embedded files when you are not sure if those files are worth analyzing, otherwise the results will be hard to read and will take a very long time to execute.","title":"Result Class"},{"location":"fr/developer_manual/services/advanced/result/#result-class","text":"All services in Assemblyline must create a Result object containing the different ResultSections which encapsulate their findings. This Result object must then be set as the Request's result parameter which will then be saved in the database to show to the user. You can view the source for the class here: Result class source","title":"Result class"},{"location":"fr/developer_manual/services/advanced/result/#class-variables","text":"Even though the Result class is critical for the service, it does not have many variables that should be used by the service. Variable Name Description sections List of all the ResultSection objects that have been added to the Result object so far.","title":"Class variables"},{"location":"fr/developer_manual/services/advanced/result/#class-functions","text":"There is only one function that the service writer should ever use in a Result object.","title":"Class functions"},{"location":"fr/developer_manual/services/advanced/result/#add_section","text":"This function allows the service to add a ResultSection object to the current Result object. It can take the following parameters: section : The ResultSection object to add to the list on_top : (Optional) Boolean value that indicates if the section should be on top of the other sections or not Example Excerpt from Assemblyline ResultSample service: result_sample.py ... kv_body = { \"a_str\" : \"Some string\" , \"a_bool\" : False , \"an_int\" : 102 , } kv_section = ResultSection ( 'Example of a KEY_VALUE section' , body_format = BODY_FORMAT . KEY_VALUE , body = json . dumps ( kv_body )) result . add_section ( kv_section ) ...","title":"add_section()"},{"location":"fr/developer_manual/services/advanced/result/#tips-on-writing-good-results","text":"Here's a few tips on writing services: DO's Make sure your results are easily readable for the user Include only necessary information Collapse sections that are less important by default If your service has information that can be tagged, don't forget to add those tags Choose a ResultSection type that will represent your data well DON'T's If your service has nothing to say, do not create ResultSection objects that contain that the service found nothing. There are a lot of shortcuts that the system takes with empty results and this will circumvent them. Don't overwhelm the users with information or they will stop reading it and just skim through Don't resubmit too many embedded files when you are not sure if those files are worth analyzing, otherwise the results will be hard to read and will take a very long time to execute.","title":"Tips on writing good results"},{"location":"fr/developer_manual/services/advanced/result_section/","text":"ResultSection class \u00b6 A ResultSection is bascally part of a service result that encapsulates a certain type of information that your service needs to convey to the user. For example, if you have a service that extracts networking indicators as well as process lists, you should put network indicators in their own section and then the process list in another. Result sections have the following properties: They have different types that will display information in different manners They can attach a heuristic which will add a maliciousness score to the result section They can tag important pieces of information about a file They can contain subsections which are just sections inside of another section They can have a classification which allows the API to redact partial results from a service depending on the user You can view the source for the class here: ResultSection class source Class variables \u00b6 The ResultSection class includes many instance variables which can be used to shape the way the section will be shown to the user. The following table describes all of the variables of the ResultSection class. Variable Name Description parent Parent ResultSection object (Only if the section is a child of another) subsections List of children ResultSection objects body Body of the section. Can take multiple forms depending on the section type : List of strings, string, JSON blob... classification The classification level of the current section body_format The types of body of the current section (Default: TEXT) tags Dictionary containing the different tags that have been added to the section heuristic Current heuristic assigned to the section zeroize_on_tag_safe Should the section be forced to a score of 0 if all tags found in it are marked as Safelisted? (Default: False) auto_collapse Should the section be displayed in collapsed mode when first rendered in the UI? (Default: False) zeroize_on_sig_safe Should the section be forced to a score of 0 if all heuristic signatures found in it are marked as Safelisted? (Default: True) Class functions \u00b6 __init__() \u00b6 The constructor of the ResultSection object allows you to set all variables from the start. Parameters: title_text : (Required) Title of the section body : Body of the section classification : Classification of the section body_format : Type of body heuristic : Heuristic assigned to the section tags : Dictionary of tags assigned to the section parent : Parent of the section (either another section or the Result object) zeroize_on_tag_safe : Should the section be forced to a score of 0 if all tags found in it are marked as Safelisted? auto_collapse : Should the section be displayed in collapsed mode when first rendered in the UI? zeroize_on_sig_safe : Should the section be forced to a score of 0 if all heuristic signatures found in it are marked as Safelisted? Example Excerpt from the Assemblyline ResultSample service: result_sample.py ... # The classification of a section can be set to any valid classification for your system section_color_map = ResultSection ( \"Example of colormap result section\" , body_format = BODY_FORMAT . GRAPH_DATA , body = json . dumps ( color_map_data ), classification = cl_engine . RESTRICTED ) result . add_section ( section_color_map ) ... add_line() \u00b6 This function allows the service to add a line to the body of a ResultSection object. Parameters: text : A string containing the line to add to the body Example Excerpt from the Assemblyline ResultSample service: result_sample.py ... text_section = ResultSection ( 'Example of a default section' ) # You can add lines to your section one at a time # Here we will generate a random line text_section . add_line ( get_random_phrase ()) ... add_lines() \u00b6 This function allows the service to add multiple lines to the body of a ResultSection object. Parameters: lines : List of string to add as multiple lines in the body Example Excerpt from the Assemblyline ResultSample service: result_sample.py ... text_section = ResultSection ( 'Example of a default section' ) ... # Or your can add them from a list # Here we will generate random amount of random lines text_section . add_lines ([ get_random_phrase () for _ in range ( random . randint ( 1 , 5 ))]) ... add_subsection() \u00b6 This function allows the service to add a subsection to the current ResultSection object. Parameters: subsection : The ResultSection object to add as a subsection on_top : (Optional) Boolean value that indicates if the section should be on top of the other sections or not Example Excerpt from the Assemblyline ResultSample service: result_sample.py ... url_sub_section = ResultSection ( 'Example of a url sub-section with multiple links' , body = json . dumps ( urls ), body_format = BODY_FORMAT . URL , heuristic = url_heuristic , classification = cl_engine . RESTRICTED ) ... url_sub_sub_section = ResultSection ( 'example of a two level deep sub-section' , body = json . dumps ( ips ), body_format = BODY_FORMAT . URL ) ... # Since url_sub_sub_section is a sub-section of url_sub_section # we will add it as a sub-section of url_sub_section not to the main result itself url_sub_section . add_subsection ( url_sub_sub_section ) ... add_tag() \u00b6 This function allows the service writer to add a tag to the ResultSection Parameters: type : Type of tag value : Value of the tag Example Excerpt from the Assemblyline ResultSample service: result_sample.py ... # You can tag data to a section. Tagging is used to quickly find defining information about a file text_section . add_tag ( \"attribution.implant\" , \"ResultSample\" ) ... set_body() \u00b6 Set the body and the body format of a section Parameters: body : New body value body_format : (Optional) Type of body - Default: TEXT set_heuristic() \u00b6 Set a heuristic for a current section/subsection. A heuristic is required to assign a score to a result section/subsection. Parameters: heur_id : Heuristic ID as set in the service manifest attack_id : (optional) Attack ID related to the heuristic signature : (optional) Signature name that triggered the heuristic Example Excerpt from the Assemblyline ResultSample service: result_sample.py ... # If the section needs to affect the score of the file you need to set a heuristic # Here we will pick one at random # In addition to adding a heuristic, we will associate a signature to the heuristic. # We're doing this by adding the signature name to the heuristic. (Here we use a random name) text_section . set_heuristic ( 3 , signature = \"sig_one\" ) ... Section types \u00b6 These are all result section types that Assemblyline supports. You can see a screenshot of each section as well as the code that was used to generate the actual section. TEXT \u00b6 Code used to generate the TEXT section Excerpt from the Assemblyline ResultSample service: result_sample.py ... # ================================================================== # Standard text section: BODY_FORMAT.TEXT - DEFAULT # Text sections basically just dump the text to the screen... # The scores of all sections will be SUMed in the service result # The Result classification will be the highest classification found in the sections text_section = ResultSection ( 'Example of a default section' ) # You can add lines to your section one at a time # Here we will generate a random line text_section . add_line ( get_random_phrase ()) # Or your can add them from a list # Here we will generate a random amount of random lines text_section . add_lines ([ get_random_phrase () for _ in range ( random . randint ( 1 , 5 ))]) # You can tag data to a section. Tagging is used to quickly find defining information about a file text_section . add_tag ( \"attribution.implant\" , \"ResultSample\" ) # If the section needs to affect the score of the file you need to set a heuristics # Here we will pick one at random # In addition to adding a heuristic, we will associate a signature to the heuristic. # We're doing this by adding the signature name to the heuristic. (Here we use a random name) text_section . set_heuristic ( 3 , signature = \"sig_one\" ) # You can attach ATT&CK IDs to heuristics after they where defined text_section . heuristic . add_attack_id ( random . choice ( list ( software_map . keys ()))) text_section . heuristic . add_attack_id ( random . choice ( list ( attack_map . keys ()))) text_section . heuristic . add_attack_id ( random . choice ( list ( group_map . keys ()))) text_section . heuristic . add_attack_id ( random . choice ( list ( revoke_map . keys ()))) # Same thing for the signatures, they can be added to a heuristic after the fact and you can even say how # many time the signature fired by setting its frequency. If you call add_signature_id twice with the # same signature, this will also increase the frequency of the signature. text_section . heuristic . add_signature_id ( \"sig_two\" , score = 20 , frequency = 2 ) text_section . heuristic . add_signature_id ( \"sig_two\" , score = 20 , frequency = 3 ) text_section . heuristic . add_signature_id ( \"sig_three\" ) text_section . heuristic . add_signature_id ( \"sig_three\" ) text_section . heuristic . add_signature_id ( \"sig_four\" , score = 0 ) # The heuristic for text_section should have the following properties: # 1. 1 ATT&CK ID: T1066 # 2. 4 signatures: sig_one, sig_two, sig_three and sig_four # 3. Signature frequencies are cumulative, therefore they will be as follows: # - sig_one = 1 # - sig_two = 5 # - sig_three = 2 # - sig_four = 1 # 4. The score used by each heuristic is driven by the following rules: signature_score_map is the highest # priority, then score value for the add_signature_id is in second place and finally the default # heuristic score is used. The score used to calculate the total score for the text_section is # as follows: # - sig_one: 10 -> heuristic default score # - sig_two: 20 -> score provided by the function add_signature_id # - sig_three: 30 -> score provided by the heuristic map # - sig_four: 40 -> score provided by the heuristic map because it's higher priority than the # function score # 5. Total section score is then: 1x10 + 5x20 + 2x30 + 1x40 = 210 # Make sure you add your section to the result result . add_section ( text_section ) ... MEMORY_DUMP \u00b6 Code used to generate the MEMORY_DUMP section Excerpt from the Assemblyline ResultSample service: result_sample.py ... # ================================================================== # Memory dump section: BODY_FORMAT.MEMORY_DUMP # Dump whatever string content you have into a <pre/> html tag so you can do your own formatting data = hexdump ( b \"This is some random text that we will format as an hexdump and you'll see \" b \"that the hexdump formatting will be preserved by the memory dump section!\" ) memdump_section = ResultSection ( 'Example of a memory dump section' , body_format = BODY_FORMAT . MEMORY_DUMP , body = data ) memdump_section . set_heuristic ( random . randint ( 1 , 4 )) result . add_section ( memdump_section ) ... GRAPH_DATA \u00b6 Code used to generate the GRAPH_DATA section Excerpt from the Assemblyline ResultSample service: result_sample.py ... # ================================================================== # Color map Section: BODY_FORMAT.GRAPH_DATA # Creates a color map bar using a minimum and maximum domain # e.g. We are using this section to display the entropy distribution in some services cmap_min = 0 cmap_max = 20 color_map_data = { 'type' : 'colormap' , 'data' : { 'domain' : [ cmap_min , cmap_max ], 'values' : [ random . random () * cmap_max for _ in range ( 50 )] } } # The classification of a section can be set to any valid classification for your system section_color_map = ResultSection ( \"Example of colormap result section\" , body_format = BODY_FORMAT . GRAPH_DATA , body = json . dumps ( color_map_data ), classification = cl_engine . RESTRICTED ) result . add_section ( section_color_map ) ... URL \u00b6 Code used to generate the URL section Excerpt from the Assemblyline ResultSample service: result_sample.py ... # ================================================================== # URL section: BODY_FORMAT.URL # Generate a list of clickable URLs using a JSON encoded format # As you can see here, the body of the section can be set directly instead of line by line random_host = get_random_host () url_section = ResultSection ( 'Example of a simple url section' , body_format = BODY_FORMAT . URL , body = json . dumps ({ \"name\" : \"Random url!\" , \"url\" : f \"https:// { random_host } /\" })) # Since URLs are very important features, we can tag those features in the system so that they are easy to find # Tags are defined by a type and a value url_section . add_tag ( \"network.static.domain\" , random_host ) # You may also want to provide a list of URLs! # Also, no need to provide a name, the URL link will be displayed host1 = get_random_host () host2 = get_random_host () urls = [ { \"url\" : f \"https:// { host1 } /\" }, { \"url\" : f \"https:// { host2 } /\" }] # A heuristic can fire more then once without being associated to a signature url_heuristic = Heuristic ( 4 , frequency = len ( urls )) url_sub_section = ResultSection ( 'Example of a URL sub-section with multiple links' , body = json . dumps ( urls ), body_format = BODY_FORMAT . URL , heuristic = url_heuristic , classification = cl_engine . RESTRICTED ) url_sub_section . add_tag ( \"network.static.domain\" , host1 ) url_sub_section . add_tag ( \"network.dynamic.domain\" , host2 ) # Since url_sub_section is a subsection of url_section # we will add it as a subsection of url_section, not to the main result itself url_section . add_subsection ( url_sub_section ) result . add_section ( url_section ) ... JSON \u00b6 Code used to generate the JSON section Excerpt from the Assemblyline ResultSample service: result_sample.py ... # ================================================================== # JSON section: # Re-use the JSON editor we use for administration (https://github.com/josdejong/jsoneditor) # to display a tree view of JSON results. # NB: Use this sparingly! As a service developer you should do your best to include important # results as their own result sections. # The body argument must be a json dump of a python dictionary json_body = { \"a_str\" : \"Some string\" , \"a_list\" : [ \"a\" , \"b\" , \"c\" ], \"a_bool\" : False , \"an_int\" : 102 , \"a_dict\" : { \"list_of_dict\" : [ { \"d1_key\" : \"val\" , \"d1_key2\" : \"val2\" }, { \"d2_key\" : \"val\" , \"d2_key2\" : \"val2\" } ], \"bool\" : True } } json_section = ResultSection ( 'Example of a JSON section' , body_format = BODY_FORMAT . JSON , body = json . dumps ( json_body )) result . add_section ( json_section ) ... KEY_VALUE \u00b6 Code used to generate the KEY_VALUE section Excerpt from Assemblyline Result sample service: result_sample.py ... # ================================================================== # KEY_VALUE section: # This section allows the service writer to list a bunch of key/value pairs to be displayed in the UI # while also providing easy to parse data for auto mated tools. # NB: You should definitely use this over a JSON body type since this one will be displayed correctly # in the UI for the user # The body argument must be a json dumps of a dictionary (only str, int, and booleans are allowed) kv_body = { \"a_str\" : \"Some string\" , \"a_bool\" : False , \"an_int\" : 102 , } kv_section = ResultSection ( 'Example of a KEY_VALUE section' , body_format = BODY_FORMAT . KEY_VALUE , body = json . dumps ( kv_body )) result . add_section ( kv_section ) ... PROCESS_TREE \u00b6 Code used to generate the section Excerpt from Assemblyline Result sample service: result_sample.py ... # ================================================================== # PROCESS_TREE section: # This section allows the service writer to list a bunch of dictionary objects that have nested lists # of dictionaries to be displayed in the UI. Each dictionary object represents a process, and therefore # each dictionary must have be of the following format: # { # \"process_pid\": int, # \"process_name\": str, # \"command_line\": str, # \"children\": [] NB: This list either is empty or contains more dictionaries that have the same # structure # } nc_body = [ { \"process_pid\" : 123 , \"process_name\" : \"evil.exe\" , \"command_line\" : \"C: \\\\ evil.exe\" , \"signatures\" : {}, \"children\" : [ { \"process_pid\" : 321 , \"process_name\" : \"takeovercomputer.exe\" , \"command_line\" : \"C: \\\\ Temp \\\\ takeovercomputer.exe -f do_bad_stuff\" , \"signatures\" : { \"one\" : 250 }, \"children\" : [ { \"process_pid\" : 456 , \"process_name\" : \"evenworsethanbefore.exe\" , \"command_line\" : \"C: \\\\ Temp \\\\ evenworsethanbefore.exe -f change_reg_key_cuz_im_bad\" , \"signatures\" : { \"one\" : 10 , \"two\" : 10 , \"three\" : 10 }, \"children\" : [] }, { \"process_pid\" : 234 , \"process_name\" : \"badfile.exe\" , \"command_line\" : \"C: \\\\ badfile.exe -k nothing_to_see_here\" , \"signatures\" : { \"one\" : 1000 , \"two\" : 10 , \"three\" : 10 , \"four\" : 10 , \"five\" : 10 }, \"children\" : [] } ] }, { \"process_pid\" : 345 , \"process_name\" : \"benignexe.exe\" , \"command_line\" : \"C: \\\\ benignexe.exe -f \\\" just kidding, i'm evil \\\" \" , \"signatures\" : { \"one\" : 2000 }, \"children\" : [] } ] }, { \"process_pid\" : 987 , \"process_name\" : \"runzeroday.exe\" , \"command_line\" : \"C: \\\\ runzeroday.exe -f insert_bad_spelling\" , \"signatures\" : {}, \"children\" : [] } ] nc_section = ResultSection ( 'Example of a PROCESS_TREE section' , body_format = BODY_FORMAT . PROCESS_TREE , body = json . dumps ( nc_body )) result . add_section ( nc_section ) ... TABLE \u00b6 Code used to generate the TABLE section Excerpt from Assemblyline Result sample service: result_sample.py ... # ================================================================== # TABLE section: # This section allows the service writer to have their content displayed in a table format in the UI # The body argument must be a list [] of dict {} objects. A dict object can have a key value pair # where the value is a flat nested dictionary, and this nested dictionary will be displayed as a nested # table within a cell. table_body = [ { \"a_str\" : \"Some string1\" , \"extra_column_here\" : \"confirmed\" , \"a_bool\" : False , \"an_int\" : 101 , }, { \"a_str\" : \"Some string2\" , \"a_bool\" : True , \"an_int\" : 102 , }, { \"a_str\" : \"Some string3\" , \"a_bool\" : False , \"an_int\" : 103 , }, { \"a_str\" : \"Some string4\" , \"a_bool\" : None , \"an_int\" : - 1000000000000000000 , \"extra_column_there\" : \"confirmed\" , \"nested_table\" : { \"a_str\" : \"Some string3\" , \"a_bool\" : False , \"nested_table_thats_too_deep\" : { \"a_str\" : \"Some string3\" , \"a_bool\" : False , \"an_int\" : 103 , }, }, }, ] table_section = ResultSection ( 'Example of a TABLE section' , body_format = BODY_FORMAT . TABLE , body = json . dumps ( table_body )) result . add_section ( table_section ) ...","title":"ResultSection Class"},{"location":"fr/developer_manual/services/advanced/result_section/#resultsection-class","text":"A ResultSection is bascally part of a service result that encapsulates a certain type of information that your service needs to convey to the user. For example, if you have a service that extracts networking indicators as well as process lists, you should put network indicators in their own section and then the process list in another. Result sections have the following properties: They have different types that will display information in different manners They can attach a heuristic which will add a maliciousness score to the result section They can tag important pieces of information about a file They can contain subsections which are just sections inside of another section They can have a classification which allows the API to redact partial results from a service depending on the user You can view the source for the class here: ResultSection class source","title":"ResultSection class"},{"location":"fr/developer_manual/services/advanced/result_section/#class-variables","text":"The ResultSection class includes many instance variables which can be used to shape the way the section will be shown to the user. The following table describes all of the variables of the ResultSection class. Variable Name Description parent Parent ResultSection object (Only if the section is a child of another) subsections List of children ResultSection objects body Body of the section. Can take multiple forms depending on the section type : List of strings, string, JSON blob... classification The classification level of the current section body_format The types of body of the current section (Default: TEXT) tags Dictionary containing the different tags that have been added to the section heuristic Current heuristic assigned to the section zeroize_on_tag_safe Should the section be forced to a score of 0 if all tags found in it are marked as Safelisted? (Default: False) auto_collapse Should the section be displayed in collapsed mode when first rendered in the UI? (Default: False) zeroize_on_sig_safe Should the section be forced to a score of 0 if all heuristic signatures found in it are marked as Safelisted? (Default: True)","title":"Class variables"},{"location":"fr/developer_manual/services/advanced/result_section/#class-functions","text":"","title":"Class functions"},{"location":"fr/developer_manual/services/advanced/result_section/#__init__","text":"The constructor of the ResultSection object allows you to set all variables from the start. Parameters: title_text : (Required) Title of the section body : Body of the section classification : Classification of the section body_format : Type of body heuristic : Heuristic assigned to the section tags : Dictionary of tags assigned to the section parent : Parent of the section (either another section or the Result object) zeroize_on_tag_safe : Should the section be forced to a score of 0 if all tags found in it are marked as Safelisted? auto_collapse : Should the section be displayed in collapsed mode when first rendered in the UI? zeroize_on_sig_safe : Should the section be forced to a score of 0 if all heuristic signatures found in it are marked as Safelisted? Example Excerpt from the Assemblyline ResultSample service: result_sample.py ... # The classification of a section can be set to any valid classification for your system section_color_map = ResultSection ( \"Example of colormap result section\" , body_format = BODY_FORMAT . GRAPH_DATA , body = json . dumps ( color_map_data ), classification = cl_engine . RESTRICTED ) result . add_section ( section_color_map ) ...","title":"__init__()"},{"location":"fr/developer_manual/services/advanced/result_section/#add_line","text":"This function allows the service to add a line to the body of a ResultSection object. Parameters: text : A string containing the line to add to the body Example Excerpt from the Assemblyline ResultSample service: result_sample.py ... text_section = ResultSection ( 'Example of a default section' ) # You can add lines to your section one at a time # Here we will generate a random line text_section . add_line ( get_random_phrase ()) ...","title":"add_line()"},{"location":"fr/developer_manual/services/advanced/result_section/#add_lines","text":"This function allows the service to add multiple lines to the body of a ResultSection object. Parameters: lines : List of string to add as multiple lines in the body Example Excerpt from the Assemblyline ResultSample service: result_sample.py ... text_section = ResultSection ( 'Example of a default section' ) ... # Or your can add them from a list # Here we will generate random amount of random lines text_section . add_lines ([ get_random_phrase () for _ in range ( random . randint ( 1 , 5 ))]) ...","title":"add_lines()"},{"location":"fr/developer_manual/services/advanced/result_section/#add_subsection","text":"This function allows the service to add a subsection to the current ResultSection object. Parameters: subsection : The ResultSection object to add as a subsection on_top : (Optional) Boolean value that indicates if the section should be on top of the other sections or not Example Excerpt from the Assemblyline ResultSample service: result_sample.py ... url_sub_section = ResultSection ( 'Example of a url sub-section with multiple links' , body = json . dumps ( urls ), body_format = BODY_FORMAT . URL , heuristic = url_heuristic , classification = cl_engine . RESTRICTED ) ... url_sub_sub_section = ResultSection ( 'example of a two level deep sub-section' , body = json . dumps ( ips ), body_format = BODY_FORMAT . URL ) ... # Since url_sub_sub_section is a sub-section of url_sub_section # we will add it as a sub-section of url_sub_section not to the main result itself url_sub_section . add_subsection ( url_sub_sub_section ) ...","title":"add_subsection()"},{"location":"fr/developer_manual/services/advanced/result_section/#add_tag","text":"This function allows the service writer to add a tag to the ResultSection Parameters: type : Type of tag value : Value of the tag Example Excerpt from the Assemblyline ResultSample service: result_sample.py ... # You can tag data to a section. Tagging is used to quickly find defining information about a file text_section . add_tag ( \"attribution.implant\" , \"ResultSample\" ) ...","title":"add_tag()"},{"location":"fr/developer_manual/services/advanced/result_section/#set_body","text":"Set the body and the body format of a section Parameters: body : New body value body_format : (Optional) Type of body - Default: TEXT","title":"set_body()"},{"location":"fr/developer_manual/services/advanced/result_section/#set_heuristic","text":"Set a heuristic for a current section/subsection. A heuristic is required to assign a score to a result section/subsection. Parameters: heur_id : Heuristic ID as set in the service manifest attack_id : (optional) Attack ID related to the heuristic signature : (optional) Signature name that triggered the heuristic Example Excerpt from the Assemblyline ResultSample service: result_sample.py ... # If the section needs to affect the score of the file you need to set a heuristic # Here we will pick one at random # In addition to adding a heuristic, we will associate a signature to the heuristic. # We're doing this by adding the signature name to the heuristic. (Here we use a random name) text_section . set_heuristic ( 3 , signature = \"sig_one\" ) ...","title":"set_heuristic()"},{"location":"fr/developer_manual/services/advanced/result_section/#section-types","text":"These are all result section types that Assemblyline supports. You can see a screenshot of each section as well as the code that was used to generate the actual section.","title":"Section types"},{"location":"fr/developer_manual/services/advanced/result_section/#text","text":"Code used to generate the TEXT section Excerpt from the Assemblyline ResultSample service: result_sample.py ... # ================================================================== # Standard text section: BODY_FORMAT.TEXT - DEFAULT # Text sections basically just dump the text to the screen... # The scores of all sections will be SUMed in the service result # The Result classification will be the highest classification found in the sections text_section = ResultSection ( 'Example of a default section' ) # You can add lines to your section one at a time # Here we will generate a random line text_section . add_line ( get_random_phrase ()) # Or your can add them from a list # Here we will generate a random amount of random lines text_section . add_lines ([ get_random_phrase () for _ in range ( random . randint ( 1 , 5 ))]) # You can tag data to a section. Tagging is used to quickly find defining information about a file text_section . add_tag ( \"attribution.implant\" , \"ResultSample\" ) # If the section needs to affect the score of the file you need to set a heuristics # Here we will pick one at random # In addition to adding a heuristic, we will associate a signature to the heuristic. # We're doing this by adding the signature name to the heuristic. (Here we use a random name) text_section . set_heuristic ( 3 , signature = \"sig_one\" ) # You can attach ATT&CK IDs to heuristics after they where defined text_section . heuristic . add_attack_id ( random . choice ( list ( software_map . keys ()))) text_section . heuristic . add_attack_id ( random . choice ( list ( attack_map . keys ()))) text_section . heuristic . add_attack_id ( random . choice ( list ( group_map . keys ()))) text_section . heuristic . add_attack_id ( random . choice ( list ( revoke_map . keys ()))) # Same thing for the signatures, they can be added to a heuristic after the fact and you can even say how # many time the signature fired by setting its frequency. If you call add_signature_id twice with the # same signature, this will also increase the frequency of the signature. text_section . heuristic . add_signature_id ( \"sig_two\" , score = 20 , frequency = 2 ) text_section . heuristic . add_signature_id ( \"sig_two\" , score = 20 , frequency = 3 ) text_section . heuristic . add_signature_id ( \"sig_three\" ) text_section . heuristic . add_signature_id ( \"sig_three\" ) text_section . heuristic . add_signature_id ( \"sig_four\" , score = 0 ) # The heuristic for text_section should have the following properties: # 1. 1 ATT&CK ID: T1066 # 2. 4 signatures: sig_one, sig_two, sig_three and sig_four # 3. Signature frequencies are cumulative, therefore they will be as follows: # - sig_one = 1 # - sig_two = 5 # - sig_three = 2 # - sig_four = 1 # 4. The score used by each heuristic is driven by the following rules: signature_score_map is the highest # priority, then score value for the add_signature_id is in second place and finally the default # heuristic score is used. The score used to calculate the total score for the text_section is # as follows: # - sig_one: 10 -> heuristic default score # - sig_two: 20 -> score provided by the function add_signature_id # - sig_three: 30 -> score provided by the heuristic map # - sig_four: 40 -> score provided by the heuristic map because it's higher priority than the # function score # 5. Total section score is then: 1x10 + 5x20 + 2x30 + 1x40 = 210 # Make sure you add your section to the result result . add_section ( text_section ) ...","title":"TEXT"},{"location":"fr/developer_manual/services/advanced/result_section/#memory_dump","text":"Code used to generate the MEMORY_DUMP section Excerpt from the Assemblyline ResultSample service: result_sample.py ... # ================================================================== # Memory dump section: BODY_FORMAT.MEMORY_DUMP # Dump whatever string content you have into a <pre/> html tag so you can do your own formatting data = hexdump ( b \"This is some random text that we will format as an hexdump and you'll see \" b \"that the hexdump formatting will be preserved by the memory dump section!\" ) memdump_section = ResultSection ( 'Example of a memory dump section' , body_format = BODY_FORMAT . MEMORY_DUMP , body = data ) memdump_section . set_heuristic ( random . randint ( 1 , 4 )) result . add_section ( memdump_section ) ...","title":"MEMORY_DUMP"},{"location":"fr/developer_manual/services/advanced/result_section/#graph_data","text":"Code used to generate the GRAPH_DATA section Excerpt from the Assemblyline ResultSample service: result_sample.py ... # ================================================================== # Color map Section: BODY_FORMAT.GRAPH_DATA # Creates a color map bar using a minimum and maximum domain # e.g. We are using this section to display the entropy distribution in some services cmap_min = 0 cmap_max = 20 color_map_data = { 'type' : 'colormap' , 'data' : { 'domain' : [ cmap_min , cmap_max ], 'values' : [ random . random () * cmap_max for _ in range ( 50 )] } } # The classification of a section can be set to any valid classification for your system section_color_map = ResultSection ( \"Example of colormap result section\" , body_format = BODY_FORMAT . GRAPH_DATA , body = json . dumps ( color_map_data ), classification = cl_engine . RESTRICTED ) result . add_section ( section_color_map ) ...","title":"GRAPH_DATA"},{"location":"fr/developer_manual/services/advanced/result_section/#url","text":"Code used to generate the URL section Excerpt from the Assemblyline ResultSample service: result_sample.py ... # ================================================================== # URL section: BODY_FORMAT.URL # Generate a list of clickable URLs using a JSON encoded format # As you can see here, the body of the section can be set directly instead of line by line random_host = get_random_host () url_section = ResultSection ( 'Example of a simple url section' , body_format = BODY_FORMAT . URL , body = json . dumps ({ \"name\" : \"Random url!\" , \"url\" : f \"https:// { random_host } /\" })) # Since URLs are very important features, we can tag those features in the system so that they are easy to find # Tags are defined by a type and a value url_section . add_tag ( \"network.static.domain\" , random_host ) # You may also want to provide a list of URLs! # Also, no need to provide a name, the URL link will be displayed host1 = get_random_host () host2 = get_random_host () urls = [ { \"url\" : f \"https:// { host1 } /\" }, { \"url\" : f \"https:// { host2 } /\" }] # A heuristic can fire more then once without being associated to a signature url_heuristic = Heuristic ( 4 , frequency = len ( urls )) url_sub_section = ResultSection ( 'Example of a URL sub-section with multiple links' , body = json . dumps ( urls ), body_format = BODY_FORMAT . URL , heuristic = url_heuristic , classification = cl_engine . RESTRICTED ) url_sub_section . add_tag ( \"network.static.domain\" , host1 ) url_sub_section . add_tag ( \"network.dynamic.domain\" , host2 ) # Since url_sub_section is a subsection of url_section # we will add it as a subsection of url_section, not to the main result itself url_section . add_subsection ( url_sub_section ) result . add_section ( url_section ) ...","title":"URL"},{"location":"fr/developer_manual/services/advanced/result_section/#json","text":"Code used to generate the JSON section Excerpt from the Assemblyline ResultSample service: result_sample.py ... # ================================================================== # JSON section: # Re-use the JSON editor we use for administration (https://github.com/josdejong/jsoneditor) # to display a tree view of JSON results. # NB: Use this sparingly! As a service developer you should do your best to include important # results as their own result sections. # The body argument must be a json dump of a python dictionary json_body = { \"a_str\" : \"Some string\" , \"a_list\" : [ \"a\" , \"b\" , \"c\" ], \"a_bool\" : False , \"an_int\" : 102 , \"a_dict\" : { \"list_of_dict\" : [ { \"d1_key\" : \"val\" , \"d1_key2\" : \"val2\" }, { \"d2_key\" : \"val\" , \"d2_key2\" : \"val2\" } ], \"bool\" : True } } json_section = ResultSection ( 'Example of a JSON section' , body_format = BODY_FORMAT . JSON , body = json . dumps ( json_body )) result . add_section ( json_section ) ...","title":"JSON"},{"location":"fr/developer_manual/services/advanced/result_section/#key_value","text":"Code used to generate the KEY_VALUE section Excerpt from Assemblyline Result sample service: result_sample.py ... # ================================================================== # KEY_VALUE section: # This section allows the service writer to list a bunch of key/value pairs to be displayed in the UI # while also providing easy to parse data for auto mated tools. # NB: You should definitely use this over a JSON body type since this one will be displayed correctly # in the UI for the user # The body argument must be a json dumps of a dictionary (only str, int, and booleans are allowed) kv_body = { \"a_str\" : \"Some string\" , \"a_bool\" : False , \"an_int\" : 102 , } kv_section = ResultSection ( 'Example of a KEY_VALUE section' , body_format = BODY_FORMAT . KEY_VALUE , body = json . dumps ( kv_body )) result . add_section ( kv_section ) ...","title":"KEY_VALUE"},{"location":"fr/developer_manual/services/advanced/result_section/#process_tree","text":"Code used to generate the section Excerpt from Assemblyline Result sample service: result_sample.py ... # ================================================================== # PROCESS_TREE section: # This section allows the service writer to list a bunch of dictionary objects that have nested lists # of dictionaries to be displayed in the UI. Each dictionary object represents a process, and therefore # each dictionary must have be of the following format: # { # \"process_pid\": int, # \"process_name\": str, # \"command_line\": str, # \"children\": [] NB: This list either is empty or contains more dictionaries that have the same # structure # } nc_body = [ { \"process_pid\" : 123 , \"process_name\" : \"evil.exe\" , \"command_line\" : \"C: \\\\ evil.exe\" , \"signatures\" : {}, \"children\" : [ { \"process_pid\" : 321 , \"process_name\" : \"takeovercomputer.exe\" , \"command_line\" : \"C: \\\\ Temp \\\\ takeovercomputer.exe -f do_bad_stuff\" , \"signatures\" : { \"one\" : 250 }, \"children\" : [ { \"process_pid\" : 456 , \"process_name\" : \"evenworsethanbefore.exe\" , \"command_line\" : \"C: \\\\ Temp \\\\ evenworsethanbefore.exe -f change_reg_key_cuz_im_bad\" , \"signatures\" : { \"one\" : 10 , \"two\" : 10 , \"three\" : 10 }, \"children\" : [] }, { \"process_pid\" : 234 , \"process_name\" : \"badfile.exe\" , \"command_line\" : \"C: \\\\ badfile.exe -k nothing_to_see_here\" , \"signatures\" : { \"one\" : 1000 , \"two\" : 10 , \"three\" : 10 , \"four\" : 10 , \"five\" : 10 }, \"children\" : [] } ] }, { \"process_pid\" : 345 , \"process_name\" : \"benignexe.exe\" , \"command_line\" : \"C: \\\\ benignexe.exe -f \\\" just kidding, i'm evil \\\" \" , \"signatures\" : { \"one\" : 2000 }, \"children\" : [] } ] }, { \"process_pid\" : 987 , \"process_name\" : \"runzeroday.exe\" , \"command_line\" : \"C: \\\\ runzeroday.exe -f insert_bad_spelling\" , \"signatures\" : {}, \"children\" : [] } ] nc_section = ResultSection ( 'Example of a PROCESS_TREE section' , body_format = BODY_FORMAT . PROCESS_TREE , body = json . dumps ( nc_body )) result . add_section ( nc_section ) ...","title":"PROCESS_TREE"},{"location":"fr/developer_manual/services/advanced/result_section/#table","text":"Code used to generate the TABLE section Excerpt from Assemblyline Result sample service: result_sample.py ... # ================================================================== # TABLE section: # This section allows the service writer to have their content displayed in a table format in the UI # The body argument must be a list [] of dict {} objects. A dict object can have a key value pair # where the value is a flat nested dictionary, and this nested dictionary will be displayed as a nested # table within a cell. table_body = [ { \"a_str\" : \"Some string1\" , \"extra_column_here\" : \"confirmed\" , \"a_bool\" : False , \"an_int\" : 101 , }, { \"a_str\" : \"Some string2\" , \"a_bool\" : True , \"an_int\" : 102 , }, { \"a_str\" : \"Some string3\" , \"a_bool\" : False , \"an_int\" : 103 , }, { \"a_str\" : \"Some string4\" , \"a_bool\" : None , \"an_int\" : - 1000000000000000000 , \"extra_column_there\" : \"confirmed\" , \"nested_table\" : { \"a_str\" : \"Some string3\" , \"a_bool\" : False , \"nested_table_thats_too_deep\" : { \"a_str\" : \"Some string3\" , \"a_bool\" : False , \"an_int\" : 103 , }, }, }, ] table_section = ResultSection ( 'Example of a TABLE section' , body_format = BODY_FORMAT . TABLE , body = json . dumps ( table_body )) result . add_section ( table_section ) ...","title":"TABLE"},{"location":"fr/developer_manual/services/advanced/service_base/","text":"ServiceBase class \u00b6 All service created for Assemblyline must inherit from the ServiceBase class which can be imported from assemblyline_v4_service.common.base . In this section we will go through the different methods and variables available to you in the ServiceBase class. You can view the source for the class here: ServiceBase class source Class variables \u00b6 The ServiceBase base class includes many instance variables which can be used to access service related information. The following tables describes all of the variables of the ServiceBase class. Variable Name Description config Reference to the service parameters containing values updated by the user for service configuration. dependencies A dictionary containing connection details for service dependencies log Reference to the logger. rules_directory Returns the directory path which contains the current location of your rules rules_hash A hash of the files in the rules_list. Used to invalidate caching if rules change. rules_list Returns a list of directory paths which point to rule files derived from rules_directory service_attributes Service attributes from the service manifest . update_time An integer representing the epoch of when the last update occurred working_directory Returns the directory path which the service can use to temporarily store files during each task execution. Class functions \u00b6 This is the list of all the functions that you can override in your service. They are explained in order of importance and the likelihood at which you will override them. execute() \u00b6 This is where your service processing code lives! The execute function is called every time the service receives a new file to scan. To this function, a single Request object is provided as an input which provides the service all the information available about the file which was requested for scanning. It is inside this function that you will create a Result objects to send your scan results back to the system for storage and display in the user interface. This has to be done before the end of the function execution. Make sure you go through the tips on writing good results before starting to built your service. get_tool_version() \u00b6 The purpose of the get_tool_version function is to the return a string indicating the version of the tools used by the service or a hash of the signatures it uses. The tool version should be updated to reflect changes in the service tools or signatures, so that Assemblyline can rescan files on the new service version if they are submitted again. start() \u00b6 The start function is called when the Assemblyline service is initiated and should be used to prepare your service for task execution. get_api_interface() \u00b6 The purpose of the get_api_interface function is to give the service direct access to the service server component to perform API request to find out if a file or a tag is meant to be safelisted. Warning By using this function, your service will not work using the run_service_once command and will be completely tied to the service server API. stop() \u00b6 The stop function is called when the Assemblyline service is stopped and should be used to cleanup your service. The following functions are used if and only if you're using a dependency that's a service updater named 'updates'. For this reason, we reserve the dependency name 'updates' to be used for service updaters. _load_rules() \u00b6 The _load_rules function is called to process the rules_list in a specific way defined by the service. _clear_rules() \u00b6 The _clear_rules function is optionally called to remove the current ruleset from memory. Requires implementation by the service writer for use. _download_rules() \u00b6 The _download_rules function is called after each _cleanup call to check if there is new updates to be processed. If so, it will attempt to download and use the new ruleset otherwise it will revert to the old ruleset. It will call on _load_rules and _clear_rules during this attempt process.","title":"ServiceBase Class"},{"location":"fr/developer_manual/services/advanced/service_base/#servicebase-class","text":"All service created for Assemblyline must inherit from the ServiceBase class which can be imported from assemblyline_v4_service.common.base . In this section we will go through the different methods and variables available to you in the ServiceBase class. You can view the source for the class here: ServiceBase class source","title":"ServiceBase class"},{"location":"fr/developer_manual/services/advanced/service_base/#class-variables","text":"The ServiceBase base class includes many instance variables which can be used to access service related information. The following tables describes all of the variables of the ServiceBase class. Variable Name Description config Reference to the service parameters containing values updated by the user for service configuration. dependencies A dictionary containing connection details for service dependencies log Reference to the logger. rules_directory Returns the directory path which contains the current location of your rules rules_hash A hash of the files in the rules_list. Used to invalidate caching if rules change. rules_list Returns a list of directory paths which point to rule files derived from rules_directory service_attributes Service attributes from the service manifest . update_time An integer representing the epoch of when the last update occurred working_directory Returns the directory path which the service can use to temporarily store files during each task execution.","title":"Class variables"},{"location":"fr/developer_manual/services/advanced/service_base/#class-functions","text":"This is the list of all the functions that you can override in your service. They are explained in order of importance and the likelihood at which you will override them.","title":"Class functions"},{"location":"fr/developer_manual/services/advanced/service_base/#execute","text":"This is where your service processing code lives! The execute function is called every time the service receives a new file to scan. To this function, a single Request object is provided as an input which provides the service all the information available about the file which was requested for scanning. It is inside this function that you will create a Result objects to send your scan results back to the system for storage and display in the user interface. This has to be done before the end of the function execution. Make sure you go through the tips on writing good results before starting to built your service.","title":"execute()"},{"location":"fr/developer_manual/services/advanced/service_base/#get_tool_version","text":"The purpose of the get_tool_version function is to the return a string indicating the version of the tools used by the service or a hash of the signatures it uses. The tool version should be updated to reflect changes in the service tools or signatures, so that Assemblyline can rescan files on the new service version if they are submitted again.","title":"get_tool_version()"},{"location":"fr/developer_manual/services/advanced/service_base/#start","text":"The start function is called when the Assemblyline service is initiated and should be used to prepare your service for task execution.","title":"start()"},{"location":"fr/developer_manual/services/advanced/service_base/#get_api_interface","text":"The purpose of the get_api_interface function is to give the service direct access to the service server component to perform API request to find out if a file or a tag is meant to be safelisted. Warning By using this function, your service will not work using the run_service_once command and will be completely tied to the service server API.","title":"get_api_interface()"},{"location":"fr/developer_manual/services/advanced/service_base/#stop","text":"The stop function is called when the Assemblyline service is stopped and should be used to cleanup your service. The following functions are used if and only if you're using a dependency that's a service updater named 'updates'. For this reason, we reserve the dependency name 'updates' to be used for service updaters.","title":"stop()"},{"location":"fr/developer_manual/services/advanced/service_base/#_load_rules","text":"The _load_rules function is called to process the rules_list in a specific way defined by the service.","title":"_load_rules()"},{"location":"fr/developer_manual/services/advanced/service_base/#_clear_rules","text":"The _clear_rules function is optionally called to remove the current ruleset from memory. Requires implementation by the service writer for use.","title":"_clear_rules()"},{"location":"fr/developer_manual/services/advanced/service_base/#_download_rules","text":"The _download_rules function is called after each _cleanup call to check if there is new updates to be processed. If so, it will attempt to download and use the new ruleset otherwise it will revert to the old ruleset. It will call on _load_rules and _clear_rules during this attempt process.","title":"_download_rules()"},{"location":"fr/developer_manual/services/advanced/service_manifest/","text":"Service manifest \u00b6 Every service must have a service_manifest.yml file in its root directory. The manifest file presents essential information about the service to the Assemblyline core system, information the Assemblyline core system must have before it can run the service. The table below shows all the elements that the manifest file can contain, including a brief description of each. Field name Value type Required? Description accepts Keyword No Default: .* Regexes applied to Assemblyline style file type string. For example, .* will allow the service to accept all types of files. category Keyword No Default: Static Analysis Which category is the service part of? Must be one of Antivirus , Dynamic Analysis , External , Extraction , Filtering , Networking , or Static Analysis . config Mapping of Any No Dictionary of service configuration variables. The key names can be any Keyword and the value can be of Any type. default_result_classification Classification string No Default: UNRESTRICTED The default classification for the results generated by the service. If no classification is provided for a result section, this default classification is used. dependencies Mapping of Dependency Config No Refer to the dependency config section. description Text No Default: NA Detailed description of the service and its features. disable_cache Boolean No Default: false Should the result cache be disabled for this service? Only disable caching for services that will always provide different results each run. docker_config Docker Config Yes Refer to the Docker Config section. enabled Boolean No Default: false Should the service be enabled by default? file_required Boolean Does the service require access to the file to perform its task? If set to false , the service will only have access to the file metadata (e.g. hashes, size, type, etc.). heuristics List of Heuristic No List of heuristic(s) used in the service for scoring. Refer to the heuristic section. is_external Boolean No Default: false Does the service make API calls to other products not part of the Assemblyline infrastructure (e.g. VirusTotal, ...)? licence_count Integer No Default: 0 Number of concurrent services allowed to run at the same time. name Keyword Yes Name of the service. rejects Keyword No Default: empty|metadata/.* Regexes applied to Assemblyline style file type string. For example, empty|metadata/.* will reject all empty and metadata files. stage Keyword No Default: CORE At which stage should the service run. Must be one of: (1) FILTER , (2) EXTRACT , (3) CORE , (4) SECONDARY , (5) POST . Note that stages are executed in the numbered order shown. submission_params List of Submission Params No List of submission param(s) that define parameters that the user can change about the service for each of its scans. Refer to the submission_params section. timeout Integer No Default: 60 Maximum execution time the service has before the task is timed out. update_config Update Config No Refer to the update config section. version 1 Keyword Yes Version of the service. 1 the version in the manifest must be the same as the image tag in order to successfully pass registration on service update/load. Dependency config \u00b6 Field name Value type Required? Description container Docker Config Yes Refer to Docker Config section. volumes Mapping of Persistent Volume No Refer to the persistent volume section. Docker config \u00b6 Field name Value type Required? Description allow_internet_access Boolean No Default: false Should the container be allowed to access the internet? command List[Keyword] No Command that should be run when the container launches. cpu_cores Float No Default: 1.0 Amount of CPU that should be allocated to the container. environment List of Environment Variable No Refer to the environment variable section. image Keyword Yes Image name always prepended by ${REGISTRY} or ${PRIVATE_REGISTRY} if image not on DockerHub. Append the rest of the image path. Do not put a / between the rest of the image path and registry var. Image name always ends in :$SERVICE_TAG ports List[Keyword] No List of ports to bind from the container. ram_mb Integer No Default: 1024 Amount of RAM in MB that should be allocated to the container. Environment variable \u00b6 Field name Value type Required? Description name Keyword Yes Name of the variable. value Keyword Yes Value of the variable. Heuristic \u00b6 Field name Value type Required? Description attack_id Enum No Mitre's Att&ck matrix ID. classification Classification No Default: UNRESTRICTED description Text Yes Detailed description of the heuristic which addresses the technique used to score. filetype Keyword Yes Regex of the filetype which applies to this heuristic. heur_id Keyword Yes Unique ID for identifying the heuristic. max_score Integer No The maximum score the heuristic can have. name Keyword Yes Short name for the heuristic. score Integer Yes Score that should be applied when this heuristic is set. Persistent volume \u00b6 Field name Value type Required? Description mount_path Keyword Yes Path into the container to mount volume. capacity Keyword Yes Storage capacity required in bytes. storage_class Keyword Yes Submission params \u00b6 Field name Value type Required? Description default Any Yes Default value of the parameter. name Keyword Yes Variable name of the parameter. type Enum Yes Type of variable. Must be one of: bool , int , list , or str . value Any Yes Value of the variable as configured by the user or the default if not configured. Update config \u00b6 Field name Value type Required? Description generates_signatures Boolean No Default: false Should the downloaded files be used to create signatures in the system? sources List of Update Source No List of source(s) from which updates can be downloaded. Refer to the update source section. update_interval_seconds Integer Yes Interval in seconds at which the updater runs. wait_for_update Boolean False Should the service wait for its updater dependency to be running? signature_delimiter Enum Must be of: new_line , double_new_line , pipe , comma , space , none , file , custom Type of delimiter used for signaure downloads. custom_delimiter Keyword Optional Custom signature delimiter to use when signature_delimiter: custom Update source \u00b6 Field name Value type Required? Description headers List of Environment Variable No Refer to the environment variable section. name Keyword Yes Unique name of the source. password Keyword No The password required to access the file. pattern Keyword No Regex pattern to match against the file names of all downloaded files from this source. This is useful when you want to filter out some files from a repo which contains many files. private_key Keyword No Key for accessing file or Git repo. uri Keyword Yes URL of the update file. Some example URL formats are: git@github.com:sample/sample-repo.git , https://file-examples.com/wp-content/uploads/2017/02/zip_2MB.zip . username Keyword No The username required to access the file.","title":"Service manifest"},{"location":"fr/developer_manual/services/advanced/service_manifest/#service-manifest","text":"Every service must have a service_manifest.yml file in its root directory. The manifest file presents essential information about the service to the Assemblyline core system, information the Assemblyline core system must have before it can run the service. The table below shows all the elements that the manifest file can contain, including a brief description of each. Field name Value type Required? Description accepts Keyword No Default: .* Regexes applied to Assemblyline style file type string. For example, .* will allow the service to accept all types of files. category Keyword No Default: Static Analysis Which category is the service part of? Must be one of Antivirus , Dynamic Analysis , External , Extraction , Filtering , Networking , or Static Analysis . config Mapping of Any No Dictionary of service configuration variables. The key names can be any Keyword and the value can be of Any type. default_result_classification Classification string No Default: UNRESTRICTED The default classification for the results generated by the service. If no classification is provided for a result section, this default classification is used. dependencies Mapping of Dependency Config No Refer to the dependency config section. description Text No Default: NA Detailed description of the service and its features. disable_cache Boolean No Default: false Should the result cache be disabled for this service? Only disable caching for services that will always provide different results each run. docker_config Docker Config Yes Refer to the Docker Config section. enabled Boolean No Default: false Should the service be enabled by default? file_required Boolean Does the service require access to the file to perform its task? If set to false , the service will only have access to the file metadata (e.g. hashes, size, type, etc.). heuristics List of Heuristic No List of heuristic(s) used in the service for scoring. Refer to the heuristic section. is_external Boolean No Default: false Does the service make API calls to other products not part of the Assemblyline infrastructure (e.g. VirusTotal, ...)? licence_count Integer No Default: 0 Number of concurrent services allowed to run at the same time. name Keyword Yes Name of the service. rejects Keyword No Default: empty|metadata/.* Regexes applied to Assemblyline style file type string. For example, empty|metadata/.* will reject all empty and metadata files. stage Keyword No Default: CORE At which stage should the service run. Must be one of: (1) FILTER , (2) EXTRACT , (3) CORE , (4) SECONDARY , (5) POST . Note that stages are executed in the numbered order shown. submission_params List of Submission Params No List of submission param(s) that define parameters that the user can change about the service for each of its scans. Refer to the submission_params section. timeout Integer No Default: 60 Maximum execution time the service has before the task is timed out. update_config Update Config No Refer to the update config section. version 1 Keyword Yes Version of the service. 1 the version in the manifest must be the same as the image tag in order to successfully pass registration on service update/load.","title":"Service manifest"},{"location":"fr/developer_manual/services/advanced/service_manifest/#dependency-config","text":"Field name Value type Required? Description container Docker Config Yes Refer to Docker Config section. volumes Mapping of Persistent Volume No Refer to the persistent volume section.","title":"Dependency config"},{"location":"fr/developer_manual/services/advanced/service_manifest/#docker-config","text":"Field name Value type Required? Description allow_internet_access Boolean No Default: false Should the container be allowed to access the internet? command List[Keyword] No Command that should be run when the container launches. cpu_cores Float No Default: 1.0 Amount of CPU that should be allocated to the container. environment List of Environment Variable No Refer to the environment variable section. image Keyword Yes Image name always prepended by ${REGISTRY} or ${PRIVATE_REGISTRY} if image not on DockerHub. Append the rest of the image path. Do not put a / between the rest of the image path and registry var. Image name always ends in :$SERVICE_TAG ports List[Keyword] No List of ports to bind from the container. ram_mb Integer No Default: 1024 Amount of RAM in MB that should be allocated to the container.","title":"Docker config"},{"location":"fr/developer_manual/services/advanced/service_manifest/#environment-variable","text":"Field name Value type Required? Description name Keyword Yes Name of the variable. value Keyword Yes Value of the variable.","title":"Environment variable"},{"location":"fr/developer_manual/services/advanced/service_manifest/#heuristic","text":"Field name Value type Required? Description attack_id Enum No Mitre's Att&ck matrix ID. classification Classification No Default: UNRESTRICTED description Text Yes Detailed description of the heuristic which addresses the technique used to score. filetype Keyword Yes Regex of the filetype which applies to this heuristic. heur_id Keyword Yes Unique ID for identifying the heuristic. max_score Integer No The maximum score the heuristic can have. name Keyword Yes Short name for the heuristic. score Integer Yes Score that should be applied when this heuristic is set.","title":"Heuristic"},{"location":"fr/developer_manual/services/advanced/service_manifest/#persistent-volume","text":"Field name Value type Required? Description mount_path Keyword Yes Path into the container to mount volume. capacity Keyword Yes Storage capacity required in bytes. storage_class Keyword Yes","title":"Persistent volume"},{"location":"fr/developer_manual/services/advanced/service_manifest/#submission-params","text":"Field name Value type Required? Description default Any Yes Default value of the parameter. name Keyword Yes Variable name of the parameter. type Enum Yes Type of variable. Must be one of: bool , int , list , or str . value Any Yes Value of the variable as configured by the user or the default if not configured.","title":"Submission params"},{"location":"fr/developer_manual/services/advanced/service_manifest/#update-config","text":"Field name Value type Required? Description generates_signatures Boolean No Default: false Should the downloaded files be used to create signatures in the system? sources List of Update Source No List of source(s) from which updates can be downloaded. Refer to the update source section. update_interval_seconds Integer Yes Interval in seconds at which the updater runs. wait_for_update Boolean False Should the service wait for its updater dependency to be running? signature_delimiter Enum Must be of: new_line , double_new_line , pipe , comma , space , none , file , custom Type of delimiter used for signaure downloads. custom_delimiter Keyword Optional Custom signature delimiter to use when signature_delimiter: custom","title":"Update config"},{"location":"fr/developer_manual/services/advanced/service_manifest/#update-source","text":"Field name Value type Required? Description headers List of Environment Variable No Refer to the environment variable section. name Keyword Yes Unique name of the source. password Keyword No The password required to access the file. pattern Keyword No Regex pattern to match against the file names of all downloaded files from this source. This is useful when you want to filter out some files from a repo which contains many files. private_key Keyword No Key for accessing file or Git repo. uri Keyword Yes URL of the update file. Some example URL formats are: git@github.com:sample/sample-repo.git , https://file-examples.com/wp-content/uploads/2017/02/zip_2MB.zip . username Keyword No The username required to access the file.","title":"Update source"},{"location":"fr/developer_manual/services/advanced/service_updater_base/","text":"ServiceUpdater class \u00b6 Some services created for Assemblyline will require signatures/rules as part of it's analysis process. ServiceUpdater class which can be imported from assemblyline_v4_service.updater.updater . In this section we will go through the different methods and variables available to you in the ServiceUpdater class. You can view the source for the class here: ServiceUpdater class source Class variables \u00b6 The ServiceUpdater base class includes many instance variables which can be used to access service related information. The following tables describes all of the variables of the ServiceUpdater class. Variable Name Description updater_type The type of updater, typically the service's name lower-cased taken from the SERVICE_PATH environment variable default_pattern The default regex pattern used if a source doesn't provide one (Default: .*) Class functions \u00b6 This is the list of all the functions that you can override in your updater. They are explained in order of importance and the likelihood at which you will override them. Note: the updater are yours to define however you would like for your service, we have implemented the basics that work with our existing services but this does not mean you have to follow what's already defined. Override it! ie. [Safelist] (https://github.com/CybercentreCanada/assemblyline-service-safelist) import_update() [Implementation Required] \u00b6 The import_update function is called to import a set of files into Assemblyline. This involves creating a list of Signature objects and importing them via the Signature API by using the Assemblyline client. do_source_update() [Override Optional] \u00b6 The do_source_update function is called on a separate thread that checks external signature sources for changes. This will then fetch the new ruleset on modification and update the signature set in Assemblyline to make it available to both users and the do_local_update thread. is_valid() [Override Optional] \u00b6 The is_valid function is called to determine if a file from a source is a valid file. The definition of its validity can vary between services but it should be able to answer the question 'Can the service use this?'. do_local_update() [Override Optional] \u00b6 The do_local_update function is called on a separate thread that checks Assemblyline for local changes to signatures such as change in status or additions/removals to the ruleset. This will then fetch the new ruleset on modification and make it available to be served to service instances. Warning Failure to implement import_update() in your service's subclass of ServiceUpdater will render the updater unusable. For an example, see Adding a Service Updater","title":"ServiceUpdater Class"},{"location":"fr/developer_manual/services/advanced/service_updater_base/#serviceupdater-class","text":"Some services created for Assemblyline will require signatures/rules as part of it's analysis process. ServiceUpdater class which can be imported from assemblyline_v4_service.updater.updater . In this section we will go through the different methods and variables available to you in the ServiceUpdater class. You can view the source for the class here: ServiceUpdater class source","title":"ServiceUpdater class"},{"location":"fr/developer_manual/services/advanced/service_updater_base/#class-variables","text":"The ServiceUpdater base class includes many instance variables which can be used to access service related information. The following tables describes all of the variables of the ServiceUpdater class. Variable Name Description updater_type The type of updater, typically the service's name lower-cased taken from the SERVICE_PATH environment variable default_pattern The default regex pattern used if a source doesn't provide one (Default: .*)","title":"Class variables"},{"location":"fr/developer_manual/services/advanced/service_updater_base/#class-functions","text":"This is the list of all the functions that you can override in your updater. They are explained in order of importance and the likelihood at which you will override them. Note: the updater are yours to define however you would like for your service, we have implemented the basics that work with our existing services but this does not mean you have to follow what's already defined. Override it! ie. [Safelist] (https://github.com/CybercentreCanada/assemblyline-service-safelist)","title":"Class functions"},{"location":"fr/developer_manual/services/advanced/service_updater_base/#import_update-implementation-required","text":"The import_update function is called to import a set of files into Assemblyline. This involves creating a list of Signature objects and importing them via the Signature API by using the Assemblyline client.","title":"import_update() [Implementation Required]"},{"location":"fr/developer_manual/services/advanced/service_updater_base/#do_source_update-override-optional","text":"The do_source_update function is called on a separate thread that checks external signature sources for changes. This will then fetch the new ruleset on modification and update the signature set in Assemblyline to make it available to both users and the do_local_update thread.","title":"do_source_update() [Override Optional]"},{"location":"fr/developer_manual/services/advanced/service_updater_base/#is_valid-override-optional","text":"The is_valid function is called to determine if a file from a source is a valid file. The definition of its validity can vary between services but it should be able to answer the question 'Can the service use this?'.","title":"is_valid() [Override Optional]"},{"location":"fr/developer_manual/services/advanced/service_updater_base/#do_local_update-override-optional","text":"The do_local_update function is called on a separate thread that checks Assemblyline for local changes to signatures such as change in status or additions/removals to the ruleset. This will then fetch the new ruleset on modification and make it available to be served to service instances. Warning Failure to implement import_update() in your service's subclass of ServiceUpdater will render the updater unusable. For an example, see Adding a Service Updater","title":"do_local_update() [Override Optional]"},{"location":"fr/developer_manual/services/advanced/service_updater_upgrade/","text":"How-to: Upgrade Service Updater for Assemblyline 4.1+ \u00b6 For this tutorial, we'll take an existing official service that was ported to 4.1 as an example Sigma 4.0.1.stable10 . Where do I start? \u00b6 As a starting point, you would create an instance of the ServiceUpdater class. We recommend setting a default_pattern (default: *) as this is a fallback if a source didn't provide a pattern when searching for acceptable signature files. In Sigma's case, its signatures are typically in .yml format. from assemblyline_v4_service.updater.updater import ServiceUpdater class SigmaUpdateServer ( ServiceUpdater ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) def import_update ( self , files_sha256 , client , source , default_classification ): -> None pass def is_valid ( self , file_path ) -> bool : return super () . is_valid ( file_path ) #Returns true always if __name__ == '__main__' : with SigmaUpdateServer ( default_pattern = \"*.yml\" ) as server : server . serve_forever () What do I need to keep from my old updater? \u00b6 Importing into Assemblyine \u00b6 In most cases, the means of importing your signatures into Assemblyline's Signature API will still be needed. As with the following within the if block, you'll be able to copy-paste that section of code into a import_update() (with obvious changes to variables names to match the definition). Failure to implement a import_update() will raise a NotImplementedError indicating this is a must for an updater. Starting at line 259 of sigma_updater.py: if files_sha256 : LOGGER . info ( \"Found new Sigma rule files to process!\" ) sigma_importer = SigmaImporter ( al_client , logger = LOGGER ) for source , source_val in files_sha256 . items (): total_imported = 0 default_classification = source_default_classification [ source ] for file in source_val . keys (): try : total_imported += sigma_importer . import_file ( file , source , default_classification = default_classification ) except ValueError : LOGGER . warning ( f \" { file } failed to import due to a Sigma error\" ) except ComposerError : LOGGER . warning ( f \" { file } failed to import due to a YAML-parsing error\" ) LOGGER . info ( f \" { total_imported } signatures were imported for source { source } \" ) else : LOGGER . info ( 'No new Sigma rule files to process' ) In the new updater: from assemblyline.common import forge from assemblyline_v4_service.updater.updater import ServiceUpdater from sigma_.sigma_importer import SigmaImporter from pysigma.exceptions import UnsupportedFeature from yaml.composer import ComposerError classification = forge . get_classification () class SigmaUpdateServer ( ServiceUpdater ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) def import_update ( self , files_sha256 , client , source , default_classification = classification . UNRESTRICTED ): -> None sigma_importer = SigmaImporter ( client , logger = self . log ) total_imported = 0 for file , _ in files_sha256 : try : total_imported += sigma_importer . import_file ( file , source , default_classification ) except ValueError : self . log . warning ( f \" { file } failed to import due to a Sigma error\" ) except ComposerError : self . log . warning ( f \" { file } failed to import due to a YAML-parsing error\" ) except UnsupportedFeature as e : self . log . warning ( f ' { file } | { e } ' ) self . log . info ( f \" { total_imported } signatures were imported for source { source } \" ) def is_valid ( self , file_path ) -> bool : return super () . is_valid ( file_path ) #Returns true always if __name__ == '__main__' : with SigmaUpdateServer ( default_pattern = \"*.yml\" ) as server : server . serve_forever () Validating signature file (Optional) \u00b6 Some updaters might've had a means of validating signature files before adding it as part of the signature collection before import. In Sigma's case, it called val_file , after downloading its signature files by either a URL download or a Git clone. If deemed a valid file for the service, then it would add it as part of the signature collection, otherwise it was ignored. Starting at line 236 of sigma_updater.py: if uri . endswith ( '.git' ): files = git_clone_repo ( source , previous_update = previous_update ) for file , sha256 in files : files_sha256 . setdefault ( source_name , {}) if previous_hash . get ( source_name , {}) . get ( file , None ) != sha256 : try : if val_file ( file ): files_sha256 [ source_name ][ file ] = sha256 else : LOGGER . warning ( f \" { file } was not imported due to failed validation\" ) except UnsupportedFeature as e : LOGGER . warning ( f \" { file } | { e } \" ) else : files = url_download ( source , previous_update = previous_update ) for file , sha256 in files : files_sha256 . setdefault ( source_name , {}) if previous_hash . get ( source_name , {}) . get ( file , None ) != sha256 : if val_file ( file ): files_sha256 [ source_name ][ file ] = sha256 else : LOGGER . warning ( f \" { file } was not imported due to failed validation\" ) In the new updater, you can override the class' is_valid() call to suit your validation needs: from yaml.composer import ComposerError from assemblyline.common import forge from assemblyline_v4_service.updater.updater import ServiceUpdater from sigma_.sigma_importer import SigmaImporter from pysigma.pysigma import val_file from pysigma.exceptions import UnsupportedFeature classification = forge . get_classification () class SigmaUpdateServer ( ServiceUpdater ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) def import_update ( self , files_sha256 , client , source , default_classification = classification . UNRESTRICTED ): sigma_importer = SigmaImporter ( client , logger = self . log ) total_imported = 0 for file , _ in files_sha256 : try : total_imported += sigma_importer . import_file ( file , source , default_classification ) except ValueError : self . log . warning ( f \" { file } failed to import due to a Sigma error\" ) except ComposerError : self . log . warning ( f \" { file } failed to import due to a YAML-parsing error\" ) except UnsupportedFeature as e : self . log . warning ( f ' { file } | { e } ' ) self . log . info ( f \" { total_imported } signatures were imported for source { source } \" ) def is_valid ( self , file_path ) -> bool : try : return val_file ( file_path ) except UnsupportedFeature : return False if __name__ == '__main__' : with SigmaUpdateServer ( default_pattern = \"*.yml\" ) as server : server . serve_forever () Is there anything I need to change for the actual service code? \u00b6 There would be a need to implement a load_rules() since services can use their rules in different ways so standardizing on a method isn't feasible at the moment. In the case of Sigma: def _load_rules ( self ) -> None : temp_list = [] # Patch source_name into signature and import for rule in self . rules_list : with open ( rule , 'r' ) as yaml_fh : file = yaml_fh . read () source_name = rule . split ( '/' )[ - 2 ] patched_rule = f ' { file } \\n signature_source: { source_name } ' temp_list . append ( patched_rule ) self . log . info ( f \"Number of rules to be loaded: { len ( temp_list ) } \" ) for rule in temp_list : try : self . sigma_parser . add_signature ( rule ) except Exception as e : self . log . warning ( f \" { e } | { rule } \" ) self . log . info ( f \"Number of rules successfully loaded: { len ( self . sigma_parser . rules ) } \" ) Note: After each submission, during a _cleanup() , there is a call made to _download_rules() which will ask your updater if there's new signatures to retrieve. If there's nothing new, then you will proceed to use the same signature set. Otherwise, the service wil retrieve the new set and attempt to load it into the service. For this reason, there can be the need to implement a _clear_rules() to ensure service stability if there was a failure while loading the new set that could have a negative impact on analysis. What don't I need to implement and why? \u00b6 do_source_update() \u00b6 Considering most of our services have the same pattern of: 1. Establishing a client for Assemblyline using the service_update_account 2. Iterating over our source list and using either a URL download or Git clone to retrieve signatures 3. (Optional) If there are rules to be updated, modify the rules before import to add extra metadata or validation 4. Importing the rules into Assemblyline We decided to streamline that process for most services while giving service writers the ability to override certain aspects as needed. The service base contains a set of helper functions for retrieving files during the download process. So you'll notice most services don't need to implement their own do_source_update() , they just need to implement an import_update() . Safelist is an example of a service that is an exception to this rule where it does implement its own do_source_update() . do_local_update() \u00b6 This particular function doesn't need to be overridden (but you're free to do so) because its primary function is retrieve the signature set from Assemblyline and make it accessible to service instances when they ask for signatures. This is separate from do_source_update() as that function is meant to retrieve / check for changes from sources outside Assemblyline whereas do_local_update() is checking for internal changes to the signature set (ie. a change in status from 'DEPLOYED' to 'DISABLED'). We invite you to look at the ServiceUpdater code to have a better understanding of how the new service updaters work and how they use the functions you implement ( is_valid , import_update ). Do I have to change the service manifest for 4.1? \u00b6 Yes, very much so. We've gone away from considering the service updater as a special dependency that needs its own config section. We now consider it to just be part of the service's list of dependencies, although we reserve the container name 'updates' to indicate this dependency is an updater to the service. Service Manifest for 4.0: update_config : generates_signatures : true method : run run_options : allow_internet_access : true command : [ \"python\" , \"-m\" , \"sigma_updater\" ] image : ${REGISTRY}cccs/assemblyline-service-sigma:$SERVICE_TAG sources : - name : sigma pattern : .*windows\\/.*\\.yml uri : https://github.com/SigmaHQ/sigma.git update_interval_seconds : 21600 # Quarter-day (every 6 hours) will now become in 4.1: dependencies : updates : container : allow_internet_access : true command : [ \"python\" , \"-m\" , \"sigma_.update_server\" ] image : ${REGISTRY}cccs/assemblyline-service-sigma:$SERVICE_TAG ports : [ \"5003\" ] run_as_core : True update_config : generates_signatures : true sources : - name : sigma pattern : .*windows\\/.*\\.yml uri : https://github.com/SigmaHQ/sigma.git update_interval_seconds : 21600 # Quarter-day (every 6 hours) signature_delimiter : \"file\" Notes: - Source management will still be part of the update_config, but container configuration will be moved to a list of dependencies. - signature_delimiter indicates how signatures should be separated when downloaded from Assemblyline's Signature API (default: double newline). - In Sigma's case, it gets its list of files in the following form: /updates/<random_tempdir>/sigma/<source_name>/<signature_name> where signature_name only contains the signature associated to the name - Under the default delimiter, it would get: /updates/<random_tempdir>/sigma/<source_name> where source_name is a compiled list of all signatures from the source separated by double newlines in a single file (which might be fine for some services like YARA and Suricata) In order for updaters to work, they need to be able to communicate with other core components. So you'll need to enable the dependency to be able to run_as_core , otherwise this could lead to issues where the updater isn't able to resolve to other components like Redis and/or Elasticsearch by name. Setting port(s) helps facilitate communication between the service and its dependency over certain ports and so, as a result, Scaler will create the appropriate NetworkPolicy to ensure communication only between a service and its dependencies.","title":"How-to: Upgrade Service Updater for Assemblyline 4.1+"},{"location":"fr/developer_manual/services/advanced/service_updater_upgrade/#how-to-upgrade-service-updater-for-assemblyline-41","text":"For this tutorial, we'll take an existing official service that was ported to 4.1 as an example Sigma 4.0.1.stable10 .","title":"How-to: Upgrade Service Updater for Assemblyline 4.1+"},{"location":"fr/developer_manual/services/advanced/service_updater_upgrade/#where-do-i-start","text":"As a starting point, you would create an instance of the ServiceUpdater class. We recommend setting a default_pattern (default: *) as this is a fallback if a source didn't provide a pattern when searching for acceptable signature files. In Sigma's case, its signatures are typically in .yml format. from assemblyline_v4_service.updater.updater import ServiceUpdater class SigmaUpdateServer ( ServiceUpdater ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) def import_update ( self , files_sha256 , client , source , default_classification ): -> None pass def is_valid ( self , file_path ) -> bool : return super () . is_valid ( file_path ) #Returns true always if __name__ == '__main__' : with SigmaUpdateServer ( default_pattern = \"*.yml\" ) as server : server . serve_forever ()","title":"Where do I start?"},{"location":"fr/developer_manual/services/advanced/service_updater_upgrade/#what-do-i-need-to-keep-from-my-old-updater","text":"","title":"What do I need to keep from my old updater?"},{"location":"fr/developer_manual/services/advanced/service_updater_upgrade/#importing-into-assemblyine","text":"In most cases, the means of importing your signatures into Assemblyline's Signature API will still be needed. As with the following within the if block, you'll be able to copy-paste that section of code into a import_update() (with obvious changes to variables names to match the definition). Failure to implement a import_update() will raise a NotImplementedError indicating this is a must for an updater. Starting at line 259 of sigma_updater.py: if files_sha256 : LOGGER . info ( \"Found new Sigma rule files to process!\" ) sigma_importer = SigmaImporter ( al_client , logger = LOGGER ) for source , source_val in files_sha256 . items (): total_imported = 0 default_classification = source_default_classification [ source ] for file in source_val . keys (): try : total_imported += sigma_importer . import_file ( file , source , default_classification = default_classification ) except ValueError : LOGGER . warning ( f \" { file } failed to import due to a Sigma error\" ) except ComposerError : LOGGER . warning ( f \" { file } failed to import due to a YAML-parsing error\" ) LOGGER . info ( f \" { total_imported } signatures were imported for source { source } \" ) else : LOGGER . info ( 'No new Sigma rule files to process' ) In the new updater: from assemblyline.common import forge from assemblyline_v4_service.updater.updater import ServiceUpdater from sigma_.sigma_importer import SigmaImporter from pysigma.exceptions import UnsupportedFeature from yaml.composer import ComposerError classification = forge . get_classification () class SigmaUpdateServer ( ServiceUpdater ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) def import_update ( self , files_sha256 , client , source , default_classification = classification . UNRESTRICTED ): -> None sigma_importer = SigmaImporter ( client , logger = self . log ) total_imported = 0 for file , _ in files_sha256 : try : total_imported += sigma_importer . import_file ( file , source , default_classification ) except ValueError : self . log . warning ( f \" { file } failed to import due to a Sigma error\" ) except ComposerError : self . log . warning ( f \" { file } failed to import due to a YAML-parsing error\" ) except UnsupportedFeature as e : self . log . warning ( f ' { file } | { e } ' ) self . log . info ( f \" { total_imported } signatures were imported for source { source } \" ) def is_valid ( self , file_path ) -> bool : return super () . is_valid ( file_path ) #Returns true always if __name__ == '__main__' : with SigmaUpdateServer ( default_pattern = \"*.yml\" ) as server : server . serve_forever ()","title":"Importing into Assemblyine"},{"location":"fr/developer_manual/services/advanced/service_updater_upgrade/#validating-signature-file-optional","text":"Some updaters might've had a means of validating signature files before adding it as part of the signature collection before import. In Sigma's case, it called val_file , after downloading its signature files by either a URL download or a Git clone. If deemed a valid file for the service, then it would add it as part of the signature collection, otherwise it was ignored. Starting at line 236 of sigma_updater.py: if uri . endswith ( '.git' ): files = git_clone_repo ( source , previous_update = previous_update ) for file , sha256 in files : files_sha256 . setdefault ( source_name , {}) if previous_hash . get ( source_name , {}) . get ( file , None ) != sha256 : try : if val_file ( file ): files_sha256 [ source_name ][ file ] = sha256 else : LOGGER . warning ( f \" { file } was not imported due to failed validation\" ) except UnsupportedFeature as e : LOGGER . warning ( f \" { file } | { e } \" ) else : files = url_download ( source , previous_update = previous_update ) for file , sha256 in files : files_sha256 . setdefault ( source_name , {}) if previous_hash . get ( source_name , {}) . get ( file , None ) != sha256 : if val_file ( file ): files_sha256 [ source_name ][ file ] = sha256 else : LOGGER . warning ( f \" { file } was not imported due to failed validation\" ) In the new updater, you can override the class' is_valid() call to suit your validation needs: from yaml.composer import ComposerError from assemblyline.common import forge from assemblyline_v4_service.updater.updater import ServiceUpdater from sigma_.sigma_importer import SigmaImporter from pysigma.pysigma import val_file from pysigma.exceptions import UnsupportedFeature classification = forge . get_classification () class SigmaUpdateServer ( ServiceUpdater ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) def import_update ( self , files_sha256 , client , source , default_classification = classification . UNRESTRICTED ): sigma_importer = SigmaImporter ( client , logger = self . log ) total_imported = 0 for file , _ in files_sha256 : try : total_imported += sigma_importer . import_file ( file , source , default_classification ) except ValueError : self . log . warning ( f \" { file } failed to import due to a Sigma error\" ) except ComposerError : self . log . warning ( f \" { file } failed to import due to a YAML-parsing error\" ) except UnsupportedFeature as e : self . log . warning ( f ' { file } | { e } ' ) self . log . info ( f \" { total_imported } signatures were imported for source { source } \" ) def is_valid ( self , file_path ) -> bool : try : return val_file ( file_path ) except UnsupportedFeature : return False if __name__ == '__main__' : with SigmaUpdateServer ( default_pattern = \"*.yml\" ) as server : server . serve_forever ()","title":"Validating signature file (Optional)"},{"location":"fr/developer_manual/services/advanced/service_updater_upgrade/#is-there-anything-i-need-to-change-for-the-actual-service-code","text":"There would be a need to implement a load_rules() since services can use their rules in different ways so standardizing on a method isn't feasible at the moment. In the case of Sigma: def _load_rules ( self ) -> None : temp_list = [] # Patch source_name into signature and import for rule in self . rules_list : with open ( rule , 'r' ) as yaml_fh : file = yaml_fh . read () source_name = rule . split ( '/' )[ - 2 ] patched_rule = f ' { file } \\n signature_source: { source_name } ' temp_list . append ( patched_rule ) self . log . info ( f \"Number of rules to be loaded: { len ( temp_list ) } \" ) for rule in temp_list : try : self . sigma_parser . add_signature ( rule ) except Exception as e : self . log . warning ( f \" { e } | { rule } \" ) self . log . info ( f \"Number of rules successfully loaded: { len ( self . sigma_parser . rules ) } \" ) Note: After each submission, during a _cleanup() , there is a call made to _download_rules() which will ask your updater if there's new signatures to retrieve. If there's nothing new, then you will proceed to use the same signature set. Otherwise, the service wil retrieve the new set and attempt to load it into the service. For this reason, there can be the need to implement a _clear_rules() to ensure service stability if there was a failure while loading the new set that could have a negative impact on analysis.","title":"Is there anything I need to change for the actual service code?"},{"location":"fr/developer_manual/services/advanced/service_updater_upgrade/#what-dont-i-need-to-implement-and-why","text":"","title":"What don't I need to implement and why?"},{"location":"fr/developer_manual/services/advanced/service_updater_upgrade/#do_source_update","text":"Considering most of our services have the same pattern of: 1. Establishing a client for Assemblyline using the service_update_account 2. Iterating over our source list and using either a URL download or Git clone to retrieve signatures 3. (Optional) If there are rules to be updated, modify the rules before import to add extra metadata or validation 4. Importing the rules into Assemblyline We decided to streamline that process for most services while giving service writers the ability to override certain aspects as needed. The service base contains a set of helper functions for retrieving files during the download process. So you'll notice most services don't need to implement their own do_source_update() , they just need to implement an import_update() . Safelist is an example of a service that is an exception to this rule where it does implement its own do_source_update() .","title":"do_source_update()"},{"location":"fr/developer_manual/services/advanced/service_updater_upgrade/#do_local_update","text":"This particular function doesn't need to be overridden (but you're free to do so) because its primary function is retrieve the signature set from Assemblyline and make it accessible to service instances when they ask for signatures. This is separate from do_source_update() as that function is meant to retrieve / check for changes from sources outside Assemblyline whereas do_local_update() is checking for internal changes to the signature set (ie. a change in status from 'DEPLOYED' to 'DISABLED'). We invite you to look at the ServiceUpdater code to have a better understanding of how the new service updaters work and how they use the functions you implement ( is_valid , import_update ).","title":"do_local_update()"},{"location":"fr/developer_manual/services/advanced/service_updater_upgrade/#do-i-have-to-change-the-service-manifest-for-41","text":"Yes, very much so. We've gone away from considering the service updater as a special dependency that needs its own config section. We now consider it to just be part of the service's list of dependencies, although we reserve the container name 'updates' to indicate this dependency is an updater to the service. Service Manifest for 4.0: update_config : generates_signatures : true method : run run_options : allow_internet_access : true command : [ \"python\" , \"-m\" , \"sigma_updater\" ] image : ${REGISTRY}cccs/assemblyline-service-sigma:$SERVICE_TAG sources : - name : sigma pattern : .*windows\\/.*\\.yml uri : https://github.com/SigmaHQ/sigma.git update_interval_seconds : 21600 # Quarter-day (every 6 hours) will now become in 4.1: dependencies : updates : container : allow_internet_access : true command : [ \"python\" , \"-m\" , \"sigma_.update_server\" ] image : ${REGISTRY}cccs/assemblyline-service-sigma:$SERVICE_TAG ports : [ \"5003\" ] run_as_core : True update_config : generates_signatures : true sources : - name : sigma pattern : .*windows\\/.*\\.yml uri : https://github.com/SigmaHQ/sigma.git update_interval_seconds : 21600 # Quarter-day (every 6 hours) signature_delimiter : \"file\" Notes: - Source management will still be part of the update_config, but container configuration will be moved to a list of dependencies. - signature_delimiter indicates how signatures should be separated when downloaded from Assemblyline's Signature API (default: double newline). - In Sigma's case, it gets its list of files in the following form: /updates/<random_tempdir>/sigma/<source_name>/<signature_name> where signature_name only contains the signature associated to the name - Under the default delimiter, it would get: /updates/<random_tempdir>/sigma/<source_name> where source_name is a compiled list of all signatures from the source separated by double newlines in a single file (which might be fine for some services like YARA and Suricata) In order for updaters to work, they need to be able to communicate with other core components. So you'll need to enable the dependency to be able to run_as_core , otherwise this could lead to issues where the updater isn't able to resolve to other components like Redis and/or Elasticsearch by name. Setting port(s) helps facilitate communication between the service and its dependency over certain ports and so, as a result, Scaler will create the appropriate NetworkPolicy to ensure communication only between a service and its dependencies.","title":"Do I have to change the service manifest for 4.1?"},{"location":"fr/installation/appliance/","text":"Appliance installation (MicroK8s) \u00b6 This is the documentation for an appliance instance of the Assemblyline platform suited for smaller-scale deployments. Since we've used microk8s as the backend for this, the appliance setup can later be scaled to multiple nodes. Setup requirements \u00b6 Caveat The documentation provided here assumes that you are installing your appliance on an Ubuntu-based system and was only tested on Ubuntu 20.04. You might have to change the commands a bit if you use other Linux distributions. The recommended minimum system requirement for this appliance is 6 CPU and 12 GB of Ram. Install pre-requisites: \u00b6 Online Install microk8s: sudo snap install microk8s --classic Install microk8s addons: sudo microk8s enable dns ha-cluster storage metrics-server Install Helm and set it up to use with microk8s: sudo snap install helm --classic sudo mkdir /var/snap/microk8s/current/bin sudo ln -s /snap/bin/helm /var/snap/microk8s/current/bin/helm Install git: sudo apt install git Install ingress controller: sudo microk8s kubectl create ns ingress sudo microk8s helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx sudo microk8s helm repo update sudo microk8s helm install ingress-nginx ingress-nginx/ingress-nginx --set controller.hostPort.enabled = true -n ingress Offline Download offline packages (On an internet-connected system): Create a directory to house everything to be transferred mkdir al_deps cd al_deps Download offline snap packages for snap_pkg in \"microk8s\" \"helm\" \"kubectl\" do sudo snap download $snap_pkg done Fetch helm charts # Clone the Assemblyline helm chart repo git clone https://github.com/CybercentreCanada/assemblyline-helm-chart # Download dependency helm charts helm dependency update assemblyline-helm-chart/assemblyline/ wget https://github.com/kubernetes/ingress-nginx/releases/download/helm-chart-4.0.12/ingress-nginx-4.0.12.tgz Fetch container images # Calico CNI for container_image in \"cni\" \"pod2daemon-flexvol\" \"node\" do docker pull calico/ $container_image :v3.19.1 && docker save calico/ $container_image :v3.19.1 >> calico_ $container_image .tar done docker pull calico/kube-controllers:v3.17.3 && docker save calico/kube-controllers:v3.17.3 >> calico_kube-controllers.tar # NGINX-Ingress, refer to values.yaml in `charts` directory for container_image in \"controller:v1.1.0\" \"kube-webhook-certgen:v1.1.1\" do docker pull k8s.gcr.io/ingress-nginx/ $container_image && docker save k8s.gcr.io/ingress-nginx/ $container_image >> $container_image .tar done # microk8s add-ons, refer to images used in corresponding .yaml files (https://github.com/ubuntu/microk8s/tree/master/microk8s-resources/actions) export ARCH = amd64 # DNS, Storage, coreDNS, metrics-server container images for container_image in \"k8s-dns-kube-dns\" \"k8s-dns-dnsmasq-nanny\" \"k8s-dns-sidecar\" do docker pull gcr.io/google_containers/ $container_image - $ARCH :1.14.7 && docker save gcr.io/google_containers/ $container_image - $ARCH :1.14.7 >> $container_image .tar done docker pull k8s.gcr.io/pause:3.1 && docker save k8s.gcr.io/pause:3.1 >> pause.tar docker pull coredns/coredns:1.8.0 && docker save coredns/coredns:1.8.0 >> coredns.tar docker pull cdkbot/hostpath-provisioner- $ARCH :1.0.0 && docker save cdkbot/hostpath-provisioner- $ARCH :1.0.0 >> storage.tar docker pull k8s.gcr.io/metrics-server/metrics-server:v0.5.2 && docker save k8s.gcr.io/metrics-server/metrics-server:v0.5.2 >> metrics.tar # Assemblyline Core (release: 4.2.stable) for al_image in \"core\" \"ui\" \"ui-frontend\" \"service-server\" \"socketio\" do docker pull cccs/assemblyline- $al_image :4.2.stable && docker save cccs/assemblyline- $al_image :4.2.stable >> al_ $al_image .tar done # Elastic ES_REG = docker.elastic.co ES_VER = 7 .15.0 for beat in \"filebeat\" \"metricbeat\" do docker pull $ES_REG /beats/ $beat : $ES_VER && docker save $ES_REG /beats/ $beat : $ES_VER >> es_ $beat .tar done for es in \"logstash\" \"kibana\" \"elasticsearch\" do docker pull $ES_REG / $es / $es : $ES_VER && docker save $ES_REG / $es / $es : $ES_VER >> es_ $es .tar done # Filestore image (MinIO) for minio_image in \"minio:RELEASE.2021-02-14T04-01-33Z\" \"mc:RELEASE.2021-02-14T04-28-06Z\" do docker pull minio/ $minio_image && docker save minio/ $minio_image >> minio_ $minio_image .tar done # Redis image docker pull redis && docker save redis >> redis.tar (Optional) Container Registry Image docker pull registry && docker save registry >> registry.tar (Optional) Pull official service images for svc_image in apkaye beaver characterize configextractor cuckoo deobfuscripter emlparser espresso extract floss frankenstrings iparse metadefender metapeek oletools pdfid peepdf pefile pixaxe safelist sigma suricata swiffer tagcheck torrentslicer unpacme unpacker vipermonkey virustotal-dynamic virustotal-static xlmmacrodeobfuscator yara do docker pull cccs/assemblyline-service- $svc_image :stable && docker save cccs/assemblyline-service- $svc_image :stable >> $svc_image .tar done Load images into a container registry (Optional) Setup local container registry # Assumes Docker is installed on hosting system for container_image in registry.tar ) : do sudo docker load -i $container_image done # Start up registry container sudo docker run -dp 32000 :5000 --restart = always --name registry registry Load images from disk and push to registry REGISTRY = localhost:32000 # Re-tag images and push to local registry for image in $( docker image ls --format {{ .Repository }} : {{ .Tag }} ) do image_tag = $image if [ $( grep -o '/' <<< $image | wc -l ) -eq 2 ] then image_tag = $( cut -d '/' -f 2 - <<< $image ) fi sudo docker tag $image $REGISTRY / $image_tag && docker push $REGISTRY / $image_tag sudo docker image rm $image sudo docker image rm $REGISTRY / $image_tag done Setup computing host(s): Install microk8s sudo snap ack microk8s_*.assert sudo snap install microk8s_*.snap --classic Modify Container Registry Endpoints sudo vim /var/snap/microk8s/current/args/containerd-template.toml Replace REGISTRY with the domain/port of your container registry (ie. localhost:32000) [plugins.\"io.containerd.grpc.v1.cri\".registry.mirrors] [plugins.\"io.containerd.grpc.v1.cri\".registry.mirrors.\"docker.io\"] endpoint = [ \"https://registry-1.docker.io\" , \"http://<REGISTRY>\" , ] [plugins.\"io.containerd.grpc.v1.cri\".registry.mirrors.\"*\"] endpoint = [ \"http://<REGISTRY>\" ] Restart containerd to acknowledge the changes sudo systemctl restart containerd Enable microk8s add-ons sudo microk8s reset && sudo microk8s enable dns ha-cluster storage metrics-server Fetch kubeconfig for administration cp /var/snap/microk8s/current/credentials/client.config <remote_destination> Install admin tools (separate or same host(s) for computing): sudo snap ack helm_*.assert sudo snap install helm_*.snap --classic sudo snap ack kubectl_*.assert sudo snap install kubectl_*.snap --classic # Copy kubeconfig from cluster and make it accessible for kubectl/helm export KUBECONFIG = $HOME /.kube/config # If installing on computing hosts # sudo mkdir /var/snap/microk8s/current/bin # sudo ln -s /snap/bin/helm /var/snap/microk8s/current/bin/helm # sudo ln -s /snap/bin/kubectl /var/snap/microk8s/current/bin/kubectl # (Optional) Install additional Kubernetes monitoring tools like k9s or Lens Install ingress: # These steps assume you know the digest of the re-tagged images required for the ingress-nginx helm chart kubectl create ns ingress helm install ingress-nginx ./ingress-nginx-*.tgz --set controller.hostPort.enabled = true -n ingress Adding more nodes (optional) Note: This can be done before or after the system is live. Install required addon: sudo microk8s enable ha-cluster From the master node, run: sudo microk8s add-node This will generate a command with a token to be executed on a standby node. On your standby node, ensure the microk8s ha-cluster addon is enabled before running the command from the master to join the cluster. To verify the nodes are connected, run (on any node): sudo microk8s kubectl get nodes Repeat this process for any additional standby nodes that are to be added. For more details, see: Clustering with MicroK8s Get the Assemblyline chart to your administration computer \u00b6 Get the Assemblyline helm charts \u00b6 mkdir ~/git && cd ~/git git clone https://github.com/CybercentreCanada/assemblyline-helm-chart.git Create your personal deployment \u00b6 mkdir ~/git/deployment cp ~/git/assemblyline-helm-chart/appliance/*.yaml ~/git/deployment Setup the charts and secrets \u00b6 The values.yaml file in your deployment directory ~/git/deployment is already pre-configured for use with microk8s as a basic one node minimal appliance. Make sure you go through the file to adjust disk sizes and to turn on/off features to your liking. The secret.yaml file in your deployment directory is preconfigured with default passwords, you should change them. Tip The secrets are used to set up during bootstrap so make sure you change them before deploying the al chart. Warning Be sure to update any values as deemed necessary ie. FQDN Deploy Assemblyline via Helm: \u00b6 Create a namespace for your Assemblyline install \u00b6 For this documentation, we will use al as the namespace. sudo microk8s kubectl create namespace al Deploy the secret to the namespace \u00b6 sudo microk8s kubectl apply -f ~/git/deployment/secrets.yaml --namespace = al From this point on, you don't need the secrets.yaml file anymore. You should delete it so there is no file on disk containing your passwords. rm ~/git/deployment/secrets.yaml Finally, let's deploy Assemblyline's chart: \u00b6 For documentation, we will use assemblyline as the deployment name. sudo microk8s helm install assemblyline ~/git/assemblyline-helm-chart/assemblyline -f ~/git/deployment/values.yaml -n al Warning After you've ran the helm install command, the system has a lot of setting up to do (Creating database indexes, loading service, setting up default accounts, loading signatures ...). Don't expect it to be fully operational for at least the next 15 minutes. Updating the current deployment \u00b6 Once you have your Assemblyline chart deployed through helm, you can change any values in the values.yaml file and upgrade your deployment with the following command: sudo microk8s helm upgrade assemblyline ~/git/assemblyline-helm-chart/assemblyline -f ~/git/deployment/values.yaml -n al (Optional) Get the latest assemblyline-helm-chart Before doing your helm upgrade command, you can get the latest changes that we did to the chart by pulling them. (This could conflict with the changes you've made so be careful while doing this.) cd ~/git/assemblyline-helm-chart && git pull Quality of life improvements \u00b6 Lens IDE \u00b6 If the computer on which your microk8s deployment is installed has a desktop interface, I strongly suggest that you use K8s Lens IDE to manage the system Install Lens \u00b6 sudo snap install kontena-lens --classic Configure Lens \u00b6 After you run Lens for the first time, click the \"Add cluster\" menu/button, select the paste as text tab and paste the output of the following command: sudo microk8s kubectl config view --raw Sudoless access to MicroK8s \u00b6 MicroK8s require you to add sudo in front of every command, you can add your user to the microk8s group so you don't have to. sudo usermod -a -G microk8s $USER sudo chown -f -R $USER ~/.kube You will need to reboot for these changes to take effect Temporarily, you can add the group to your current shell by running the following: newgrp microk8s Alias to Kubectl \u00b6 Since all is running inside microk8s you can create an alias to the kubectl command to make your life easier sudo snap alias microk8s.kubectl kubectl kubectl config set-context --current --namespace=al Alternative Installations \u00b6 We will officially only support microk8s installations for appliances, but you can technically install it on any local Kubernetes frameworks (k3s, minikube, kind...). That said there will be no documentation for these setups, and you will have to modify the values.yaml storage classes to fit with your desired framework.","title":"Appliance installation (MicroK8s)"},{"location":"fr/installation/appliance/#appliance-installation-microk8s","text":"This is the documentation for an appliance instance of the Assemblyline platform suited for smaller-scale deployments. Since we've used microk8s as the backend for this, the appliance setup can later be scaled to multiple nodes.","title":"Appliance installation (MicroK8s)"},{"location":"fr/installation/appliance/#setup-requirements","text":"Caveat The documentation provided here assumes that you are installing your appliance on an Ubuntu-based system and was only tested on Ubuntu 20.04. You might have to change the commands a bit if you use other Linux distributions. The recommended minimum system requirement for this appliance is 6 CPU and 12 GB of Ram.","title":"Setup requirements"},{"location":"fr/installation/appliance/#install-pre-requisites","text":"Online Install microk8s: sudo snap install microk8s --classic Install microk8s addons: sudo microk8s enable dns ha-cluster storage metrics-server Install Helm and set it up to use with microk8s: sudo snap install helm --classic sudo mkdir /var/snap/microk8s/current/bin sudo ln -s /snap/bin/helm /var/snap/microk8s/current/bin/helm Install git: sudo apt install git Install ingress controller: sudo microk8s kubectl create ns ingress sudo microk8s helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx sudo microk8s helm repo update sudo microk8s helm install ingress-nginx ingress-nginx/ingress-nginx --set controller.hostPort.enabled = true -n ingress Offline Download offline packages (On an internet-connected system): Create a directory to house everything to be transferred mkdir al_deps cd al_deps Download offline snap packages for snap_pkg in \"microk8s\" \"helm\" \"kubectl\" do sudo snap download $snap_pkg done Fetch helm charts # Clone the Assemblyline helm chart repo git clone https://github.com/CybercentreCanada/assemblyline-helm-chart # Download dependency helm charts helm dependency update assemblyline-helm-chart/assemblyline/ wget https://github.com/kubernetes/ingress-nginx/releases/download/helm-chart-4.0.12/ingress-nginx-4.0.12.tgz Fetch container images # Calico CNI for container_image in \"cni\" \"pod2daemon-flexvol\" \"node\" do docker pull calico/ $container_image :v3.19.1 && docker save calico/ $container_image :v3.19.1 >> calico_ $container_image .tar done docker pull calico/kube-controllers:v3.17.3 && docker save calico/kube-controllers:v3.17.3 >> calico_kube-controllers.tar # NGINX-Ingress, refer to values.yaml in `charts` directory for container_image in \"controller:v1.1.0\" \"kube-webhook-certgen:v1.1.1\" do docker pull k8s.gcr.io/ingress-nginx/ $container_image && docker save k8s.gcr.io/ingress-nginx/ $container_image >> $container_image .tar done # microk8s add-ons, refer to images used in corresponding .yaml files (https://github.com/ubuntu/microk8s/tree/master/microk8s-resources/actions) export ARCH = amd64 # DNS, Storage, coreDNS, metrics-server container images for container_image in \"k8s-dns-kube-dns\" \"k8s-dns-dnsmasq-nanny\" \"k8s-dns-sidecar\" do docker pull gcr.io/google_containers/ $container_image - $ARCH :1.14.7 && docker save gcr.io/google_containers/ $container_image - $ARCH :1.14.7 >> $container_image .tar done docker pull k8s.gcr.io/pause:3.1 && docker save k8s.gcr.io/pause:3.1 >> pause.tar docker pull coredns/coredns:1.8.0 && docker save coredns/coredns:1.8.0 >> coredns.tar docker pull cdkbot/hostpath-provisioner- $ARCH :1.0.0 && docker save cdkbot/hostpath-provisioner- $ARCH :1.0.0 >> storage.tar docker pull k8s.gcr.io/metrics-server/metrics-server:v0.5.2 && docker save k8s.gcr.io/metrics-server/metrics-server:v0.5.2 >> metrics.tar # Assemblyline Core (release: 4.2.stable) for al_image in \"core\" \"ui\" \"ui-frontend\" \"service-server\" \"socketio\" do docker pull cccs/assemblyline- $al_image :4.2.stable && docker save cccs/assemblyline- $al_image :4.2.stable >> al_ $al_image .tar done # Elastic ES_REG = docker.elastic.co ES_VER = 7 .15.0 for beat in \"filebeat\" \"metricbeat\" do docker pull $ES_REG /beats/ $beat : $ES_VER && docker save $ES_REG /beats/ $beat : $ES_VER >> es_ $beat .tar done for es in \"logstash\" \"kibana\" \"elasticsearch\" do docker pull $ES_REG / $es / $es : $ES_VER && docker save $ES_REG / $es / $es : $ES_VER >> es_ $es .tar done # Filestore image (MinIO) for minio_image in \"minio:RELEASE.2021-02-14T04-01-33Z\" \"mc:RELEASE.2021-02-14T04-28-06Z\" do docker pull minio/ $minio_image && docker save minio/ $minio_image >> minio_ $minio_image .tar done # Redis image docker pull redis && docker save redis >> redis.tar (Optional) Container Registry Image docker pull registry && docker save registry >> registry.tar (Optional) Pull official service images for svc_image in apkaye beaver characterize configextractor cuckoo deobfuscripter emlparser espresso extract floss frankenstrings iparse metadefender metapeek oletools pdfid peepdf pefile pixaxe safelist sigma suricata swiffer tagcheck torrentslicer unpacme unpacker vipermonkey virustotal-dynamic virustotal-static xlmmacrodeobfuscator yara do docker pull cccs/assemblyline-service- $svc_image :stable && docker save cccs/assemblyline-service- $svc_image :stable >> $svc_image .tar done Load images into a container registry (Optional) Setup local container registry # Assumes Docker is installed on hosting system for container_image in registry.tar ) : do sudo docker load -i $container_image done # Start up registry container sudo docker run -dp 32000 :5000 --restart = always --name registry registry Load images from disk and push to registry REGISTRY = localhost:32000 # Re-tag images and push to local registry for image in $( docker image ls --format {{ .Repository }} : {{ .Tag }} ) do image_tag = $image if [ $( grep -o '/' <<< $image | wc -l ) -eq 2 ] then image_tag = $( cut -d '/' -f 2 - <<< $image ) fi sudo docker tag $image $REGISTRY / $image_tag && docker push $REGISTRY / $image_tag sudo docker image rm $image sudo docker image rm $REGISTRY / $image_tag done Setup computing host(s): Install microk8s sudo snap ack microk8s_*.assert sudo snap install microk8s_*.snap --classic Modify Container Registry Endpoints sudo vim /var/snap/microk8s/current/args/containerd-template.toml Replace REGISTRY with the domain/port of your container registry (ie. localhost:32000) [plugins.\"io.containerd.grpc.v1.cri\".registry.mirrors] [plugins.\"io.containerd.grpc.v1.cri\".registry.mirrors.\"docker.io\"] endpoint = [ \"https://registry-1.docker.io\" , \"http://<REGISTRY>\" , ] [plugins.\"io.containerd.grpc.v1.cri\".registry.mirrors.\"*\"] endpoint = [ \"http://<REGISTRY>\" ] Restart containerd to acknowledge the changes sudo systemctl restart containerd Enable microk8s add-ons sudo microk8s reset && sudo microk8s enable dns ha-cluster storage metrics-server Fetch kubeconfig for administration cp /var/snap/microk8s/current/credentials/client.config <remote_destination> Install admin tools (separate or same host(s) for computing): sudo snap ack helm_*.assert sudo snap install helm_*.snap --classic sudo snap ack kubectl_*.assert sudo snap install kubectl_*.snap --classic # Copy kubeconfig from cluster and make it accessible for kubectl/helm export KUBECONFIG = $HOME /.kube/config # If installing on computing hosts # sudo mkdir /var/snap/microk8s/current/bin # sudo ln -s /snap/bin/helm /var/snap/microk8s/current/bin/helm # sudo ln -s /snap/bin/kubectl /var/snap/microk8s/current/bin/kubectl # (Optional) Install additional Kubernetes monitoring tools like k9s or Lens Install ingress: # These steps assume you know the digest of the re-tagged images required for the ingress-nginx helm chart kubectl create ns ingress helm install ingress-nginx ./ingress-nginx-*.tgz --set controller.hostPort.enabled = true -n ingress Adding more nodes (optional) Note: This can be done before or after the system is live. Install required addon: sudo microk8s enable ha-cluster From the master node, run: sudo microk8s add-node This will generate a command with a token to be executed on a standby node. On your standby node, ensure the microk8s ha-cluster addon is enabled before running the command from the master to join the cluster. To verify the nodes are connected, run (on any node): sudo microk8s kubectl get nodes Repeat this process for any additional standby nodes that are to be added. For more details, see: Clustering with MicroK8s","title":"Install pre-requisites:"},{"location":"fr/installation/appliance/#get-the-assemblyline-chart-to-your-administration-computer","text":"","title":"Get the Assemblyline chart to your administration computer"},{"location":"fr/installation/appliance/#get-the-assemblyline-helm-charts","text":"mkdir ~/git && cd ~/git git clone https://github.com/CybercentreCanada/assemblyline-helm-chart.git","title":"Get the Assemblyline helm charts"},{"location":"fr/installation/appliance/#create-your-personal-deployment","text":"mkdir ~/git/deployment cp ~/git/assemblyline-helm-chart/appliance/*.yaml ~/git/deployment","title":"Create your personal deployment"},{"location":"fr/installation/appliance/#setup-the-charts-and-secrets","text":"The values.yaml file in your deployment directory ~/git/deployment is already pre-configured for use with microk8s as a basic one node minimal appliance. Make sure you go through the file to adjust disk sizes and to turn on/off features to your liking. The secret.yaml file in your deployment directory is preconfigured with default passwords, you should change them. Tip The secrets are used to set up during bootstrap so make sure you change them before deploying the al chart. Warning Be sure to update any values as deemed necessary ie. FQDN","title":"Setup the charts and secrets"},{"location":"fr/installation/appliance/#deploy-assemblyline-via-helm","text":"","title":"Deploy Assemblyline via Helm:"},{"location":"fr/installation/appliance/#create-a-namespace-for-your-assemblyline-install","text":"For this documentation, we will use al as the namespace. sudo microk8s kubectl create namespace al","title":"Create a namespace for your Assemblyline install"},{"location":"fr/installation/appliance/#deploy-the-secret-to-the-namespace","text":"sudo microk8s kubectl apply -f ~/git/deployment/secrets.yaml --namespace = al From this point on, you don't need the secrets.yaml file anymore. You should delete it so there is no file on disk containing your passwords. rm ~/git/deployment/secrets.yaml","title":"Deploy the secret to the namespace"},{"location":"fr/installation/appliance/#finally-lets-deploy-assemblylines-chart","text":"For documentation, we will use assemblyline as the deployment name. sudo microk8s helm install assemblyline ~/git/assemblyline-helm-chart/assemblyline -f ~/git/deployment/values.yaml -n al Warning After you've ran the helm install command, the system has a lot of setting up to do (Creating database indexes, loading service, setting up default accounts, loading signatures ...). Don't expect it to be fully operational for at least the next 15 minutes.","title":"Finally, let's deploy Assemblyline's chart:"},{"location":"fr/installation/appliance/#updating-the-current-deployment","text":"Once you have your Assemblyline chart deployed through helm, you can change any values in the values.yaml file and upgrade your deployment with the following command: sudo microk8s helm upgrade assemblyline ~/git/assemblyline-helm-chart/assemblyline -f ~/git/deployment/values.yaml -n al (Optional) Get the latest assemblyline-helm-chart Before doing your helm upgrade command, you can get the latest changes that we did to the chart by pulling them. (This could conflict with the changes you've made so be careful while doing this.) cd ~/git/assemblyline-helm-chart && git pull","title":"Updating the current deployment"},{"location":"fr/installation/appliance/#quality-of-life-improvements","text":"","title":"Quality of life improvements"},{"location":"fr/installation/appliance/#lens-ide","text":"If the computer on which your microk8s deployment is installed has a desktop interface, I strongly suggest that you use K8s Lens IDE to manage the system","title":"Lens IDE"},{"location":"fr/installation/appliance/#install-lens","text":"sudo snap install kontena-lens --classic","title":"Install Lens"},{"location":"fr/installation/appliance/#configure-lens","text":"After you run Lens for the first time, click the \"Add cluster\" menu/button, select the paste as text tab and paste the output of the following command: sudo microk8s kubectl config view --raw","title":"Configure Lens"},{"location":"fr/installation/appliance/#sudoless-access-to-microk8s","text":"MicroK8s require you to add sudo in front of every command, you can add your user to the microk8s group so you don't have to. sudo usermod -a -G microk8s $USER sudo chown -f -R $USER ~/.kube You will need to reboot for these changes to take effect Temporarily, you can add the group to your current shell by running the following: newgrp microk8s","title":"Sudoless access to MicroK8s"},{"location":"fr/installation/appliance/#alias-to-kubectl","text":"Since all is running inside microk8s you can create an alias to the kubectl command to make your life easier sudo snap alias microk8s.kubectl kubectl kubectl config set-context --current --namespace=al","title":"Alias to Kubectl"},{"location":"fr/installation/appliance/#alternative-installations","text":"We will officially only support microk8s installations for appliances, but you can technically install it on any local Kubernetes frameworks (k3s, minikube, kind...). That said there will be no documentation for these setups, and you will have to modify the values.yaml storage classes to fit with your desired framework.","title":"Alternative Installations"},{"location":"fr/installation/appliance_docker/","text":"Appliance installation (Docker) \u00b6 This is the documentation for an appliance instance of the Assemblyline platform suited for very small single machine deployment. Setup requirements \u00b6 Caveat The documentation provided here assumes that you are installing your appliance on one of the following systems: Debian: Ubuntu 20.04 RHEL: RHEL 8.5 You might have to change the commands a bit if you use other Linux distributions. The recommended minimum system requirement for this appliance is 4 CPUs and 8 GB of RAM. Warning If you have more then 16 CPUs and 64 GB of ram, you should consider using the Microk8s appliance instead. Microk8s will be able to auto-scale core components based on load but this docker appliance can only scale services. Install pre-requisites \u00b6 Online Ubuntu Install Docker: sudo apt-get update sudo apt-get install -y apt-transport-https ca-certificates curl gnupg-agent software-properties-common curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - sudo apt-key fingerprint 0EBFCD88 sudo add-apt-repository \"deb [arch=amd64] https://download.docker.com/linux/ubuntu $( lsb_release -cs ) stable\" sudo apt-get install -y docker-ce docker-ce-cli containerd.io Install Docker compose: sudo curl -s -L \"https://github.com/docker/compose/releases/download/1.29.2/docker-compose- $( uname -s ) - $( uname -m ) \" -o /usr/local/bin/docker-compose sudo chmod +x /usr/local/bin/docker-compose sudo curl -s -L https://raw.githubusercontent.com/docker/compose/1.29.2/contrib/completion/bash/docker-compose -o /etc/bash_completion.d/docker-compose RHEL 8.5 Warning Many of the instructions below have been set to force yes and allowerasing for quick implementation. It is recommended that these flags be removed for production environments to avoid impacting production environments by missing key messages and warnings. Step 4 contains a firewall configuration, we strongly advise firewall settings should be managed and reviewed by your organization before deployment. Install Docker: yum update -y --allowerasing yum install -y yum-utils yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo yum install -y docker-ce docker-ce-cli containerd.io --allowerasing systemctl start docker systemctl enable docker Install Docker compose: curl -s -L \"https://github.com/docker/compose/releases/download/1.29.2/docker-compose- $( uname -s ) - $( uname -m ) \" -o /usr/bin/docker-compose chmod +x /usr/bin/docker-compose curl -s -L https://raw.githubusercontent.com/docker/compose/1.29.2/contrib/completion/bash/docker-compose -o /etc/bash_completion.d/docker-compose Upgrade Python3.9: dnf install -y python39 alternatives --set python3 /usr/bin/python3.9 python3 --version Configure firewalld for Docker: sed -i 's/FirewallBackend=nftables/FirewallBackend=iptables/' /etc/firewalld/firewalld.conf firewall-cmd --reload reboot Setup your Assemblyline appliance \u00b6 Download the Assemblyline docker-compose files \u00b6 Online mkdir ~/git cd ~/git git clone https://github.com/CybercentreCanada/assemblyline-docker-compose.git Choose your deployment type \u00b6 Important After this step, we will assume that the commands that you run are from your deployment directory: ~/deployments/assemblyline/ Assemblyline only mkdir ~/deployments cp -R ~/git/assemblyline-docker-compose/minimal_appliance ~/deployments/assemblyline cd ~/deployments/assemblyline Assemblyline with ELK monitoring stack mkdir ~/deployments cp -R ~/git/assemblyline-docker-compose/full_appliance ~/deployments/assemblyline cd ~/deployments/assemblyline Setup your appliance \u00b6 The config/config.yaml file in your deployment directory is already pre-configured for use with docker-compose as a single node appliance. You can review the settings already configured but you should not have anything to change there. The .env file in your deployment directory is preconfigured with default passwords, you should definitely change them. Deploy Assemblyline \u00b6 Create your https certs \u00b6 openssl req -nodes -x509 -newkey rsa:4096 -keyout ~/deployments/assemblyline/config/nginx.key -out ~/deployments/assemblyline/config/nginx.crt -days 365 -subj \"/C=CA/ST=Ontario/L=Ottawa/O=CCCS/CN=assemblyline.local\" Pull necessary docker containers \u00b6 cd ~/deployments/assemblyline sudo docker-compose pull sudo docker-compose build sudo docker-compose -f bootstrap-compose.yaml pull Finally deploy your appliance \u00b6 cd ~/deployments/assemblyline sudo docker-compose up -d sudo docker-compose -f bootstrap-compose.yaml up Info Once the docker-compose command on the bootstrap file complete, your cluster will be ready to use and you can login with the default admin user/password that you've set in your .env file Docker Compose cheat sheet \u00b6 Updating your appliance \u00b6 cd ~/deployments/assemblyline sudo docker-compose pull sudo docker-compose build sudo docker-compose up -d Changing Assemblyline configuration file \u00b6 Edit the cd ~/deployments/assemblyline/config/config.yml then: cd ~/deployments/assemblyline sudo docker-compose restart Check core services logs \u00b6 For core components: cd ~/deployments/assemblyline sudo docker-compose logs Or for a specific component: cd ~/deployments/assemblyline sudo docker-compose logs ui Take down your appliance \u00b6 Tip This will remove all containers related to your appliance but will not remove the volumes so you can bring it back up safely. cd ~/deployments/assemblyline sudo docker-compose stop sudo docker rm --force $( sudo docker ps -a --filter label = app = assemblyline -q ) sudo docker-compose down --remove-orphans Bring your appliance back online \u00b6 cd ~/deployments/assemblyline sudo docker-compose up -d","title":"Appliance installation (Docker)"},{"location":"fr/installation/appliance_docker/#appliance-installation-docker","text":"This is the documentation for an appliance instance of the Assemblyline platform suited for very small single machine deployment.","title":"Appliance installation (Docker)"},{"location":"fr/installation/appliance_docker/#setup-requirements","text":"Caveat The documentation provided here assumes that you are installing your appliance on one of the following systems: Debian: Ubuntu 20.04 RHEL: RHEL 8.5 You might have to change the commands a bit if you use other Linux distributions. The recommended minimum system requirement for this appliance is 4 CPUs and 8 GB of RAM. Warning If you have more then 16 CPUs and 64 GB of ram, you should consider using the Microk8s appliance instead. Microk8s will be able to auto-scale core components based on load but this docker appliance can only scale services.","title":"Setup requirements"},{"location":"fr/installation/appliance_docker/#install-pre-requisites","text":"Online Ubuntu Install Docker: sudo apt-get update sudo apt-get install -y apt-transport-https ca-certificates curl gnupg-agent software-properties-common curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - sudo apt-key fingerprint 0EBFCD88 sudo add-apt-repository \"deb [arch=amd64] https://download.docker.com/linux/ubuntu $( lsb_release -cs ) stable\" sudo apt-get install -y docker-ce docker-ce-cli containerd.io Install Docker compose: sudo curl -s -L \"https://github.com/docker/compose/releases/download/1.29.2/docker-compose- $( uname -s ) - $( uname -m ) \" -o /usr/local/bin/docker-compose sudo chmod +x /usr/local/bin/docker-compose sudo curl -s -L https://raw.githubusercontent.com/docker/compose/1.29.2/contrib/completion/bash/docker-compose -o /etc/bash_completion.d/docker-compose RHEL 8.5 Warning Many of the instructions below have been set to force yes and allowerasing for quick implementation. It is recommended that these flags be removed for production environments to avoid impacting production environments by missing key messages and warnings. Step 4 contains a firewall configuration, we strongly advise firewall settings should be managed and reviewed by your organization before deployment. Install Docker: yum update -y --allowerasing yum install -y yum-utils yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo yum install -y docker-ce docker-ce-cli containerd.io --allowerasing systemctl start docker systemctl enable docker Install Docker compose: curl -s -L \"https://github.com/docker/compose/releases/download/1.29.2/docker-compose- $( uname -s ) - $( uname -m ) \" -o /usr/bin/docker-compose chmod +x /usr/bin/docker-compose curl -s -L https://raw.githubusercontent.com/docker/compose/1.29.2/contrib/completion/bash/docker-compose -o /etc/bash_completion.d/docker-compose Upgrade Python3.9: dnf install -y python39 alternatives --set python3 /usr/bin/python3.9 python3 --version Configure firewalld for Docker: sed -i 's/FirewallBackend=nftables/FirewallBackend=iptables/' /etc/firewalld/firewalld.conf firewall-cmd --reload reboot","title":"Install pre-requisites"},{"location":"fr/installation/appliance_docker/#setup-your-assemblyline-appliance","text":"","title":"Setup your Assemblyline appliance"},{"location":"fr/installation/appliance_docker/#download-the-assemblyline-docker-compose-files","text":"Online mkdir ~/git cd ~/git git clone https://github.com/CybercentreCanada/assemblyline-docker-compose.git","title":"Download the Assemblyline docker-compose files"},{"location":"fr/installation/appliance_docker/#choose-your-deployment-type","text":"Important After this step, we will assume that the commands that you run are from your deployment directory: ~/deployments/assemblyline/ Assemblyline only mkdir ~/deployments cp -R ~/git/assemblyline-docker-compose/minimal_appliance ~/deployments/assemblyline cd ~/deployments/assemblyline Assemblyline with ELK monitoring stack mkdir ~/deployments cp -R ~/git/assemblyline-docker-compose/full_appliance ~/deployments/assemblyline cd ~/deployments/assemblyline","title":"Choose your deployment type"},{"location":"fr/installation/appliance_docker/#setup-your-appliance","text":"The config/config.yaml file in your deployment directory is already pre-configured for use with docker-compose as a single node appliance. You can review the settings already configured but you should not have anything to change there. The .env file in your deployment directory is preconfigured with default passwords, you should definitely change them.","title":"Setup your appliance"},{"location":"fr/installation/appliance_docker/#deploy-assemblyline","text":"","title":"Deploy Assemblyline"},{"location":"fr/installation/appliance_docker/#create-your-https-certs","text":"openssl req -nodes -x509 -newkey rsa:4096 -keyout ~/deployments/assemblyline/config/nginx.key -out ~/deployments/assemblyline/config/nginx.crt -days 365 -subj \"/C=CA/ST=Ontario/L=Ottawa/O=CCCS/CN=assemblyline.local\"","title":"Create your https certs"},{"location":"fr/installation/appliance_docker/#pull-necessary-docker-containers","text":"cd ~/deployments/assemblyline sudo docker-compose pull sudo docker-compose build sudo docker-compose -f bootstrap-compose.yaml pull","title":"Pull necessary docker containers"},{"location":"fr/installation/appliance_docker/#finally-deploy-your-appliance","text":"cd ~/deployments/assemblyline sudo docker-compose up -d sudo docker-compose -f bootstrap-compose.yaml up Info Once the docker-compose command on the bootstrap file complete, your cluster will be ready to use and you can login with the default admin user/password that you've set in your .env file","title":"Finally deploy your appliance"},{"location":"fr/installation/appliance_docker/#docker-compose-cheat-sheet","text":"","title":"Docker Compose cheat sheet"},{"location":"fr/installation/appliance_docker/#updating-your-appliance","text":"cd ~/deployments/assemblyline sudo docker-compose pull sudo docker-compose build sudo docker-compose up -d","title":"Updating your appliance"},{"location":"fr/installation/appliance_docker/#changing-assemblyline-configuration-file","text":"Edit the cd ~/deployments/assemblyline/config/config.yml then: cd ~/deployments/assemblyline sudo docker-compose restart","title":"Changing Assemblyline configuration file"},{"location":"fr/installation/appliance_docker/#check-core-services-logs","text":"For core components: cd ~/deployments/assemblyline sudo docker-compose logs Or for a specific component: cd ~/deployments/assemblyline sudo docker-compose logs ui","title":"Check core services logs"},{"location":"fr/installation/appliance_docker/#take-down-your-appliance","text":"Tip This will remove all containers related to your appliance but will not remove the volumes so you can bring it back up safely. cd ~/deployments/assemblyline sudo docker-compose stop sudo docker rm --force $( sudo docker ps -a --filter label = app = assemblyline -q ) sudo docker-compose down --remove-orphans","title":"Take down your appliance"},{"location":"fr/installation/appliance_docker/#bring-your-appliance-back-online","text":"cd ~/deployments/assemblyline sudo docker-compose up -d","title":"Bring your appliance back online"},{"location":"fr/installation/classification_engine/","text":"Classification engine \u00b6 Assemblyline can do record-based access control for submission, files, results, and even up to individual sections and tags of said results. It was built to support the Government of Canada levels of security and the Traffic Light Protocol but can also be modified at will. Once turned on, the classification engine will do the following changes to the system: Users will have to be assigned a maximum classification level that they can see as well as the groups they are members of Each submission to the system will have to have a classification level The User Interface will: Show the effective classification of each submission, file, result, and result sections Have a dedicated help section that will explain how classification conflicts are resolved Let you pick a classification while submitting a file Automatically hide portions of the result for a user that does not have enough privileges to see them Configuration \u00b6 The classification engine has many parameters that can be customized so you can get record-based access controls that fit your organization. Here is an exhaustive configuration file of the classification engine that explains every single parameter: Exhaustive classification configuration # Turn on/off classification enforcement. When this flag is off, this # completely disables the classification engine, any documents added while # the classification engine is off getting the default unrestricted value enforce : false # When this flag is on, the classification engine will automatically create # groups based on the domain part of a user's email address # EX: # For user with email: test@local.host # The group \"local.host\" will be valid in the system dynamic_groups : false # List of Classification Levels: # This is a graded list; a smaller number is less restricted than a higher number # A user must be allowed a classification level >= to be able to view the data levels : # List of alternate names for the current marking. If a user submits a file with # those markings, the classification will automatically rename it to the value # specified in the name - aliases : - UNRESTRICTED - UNCLASSIFIED - U # Stylesheet applied in the UI for the current classification level css : # Name of the color scheme used for display # Possible values: default, primary, secondary, success, info, warning, error color : default # Deprecated parameters (Use color instead) # These were used in the old UI but are still valid in the new UI because if # the new UI cannot find the color param, it will use the label param and # strips \"label-\" part. banner : alert-default label : label-default text : text-muted # Description of the classification level description : Subject to standard copyright rules, TLP:WHITE information may be distributed without restriction. # Integer value of the Classification level (higher is more classified) lvl : 100 # Long name of the classification level name : TLP:WHITE # Short name of the classification level short_name : TLP:W - aliases : [] css : color : success description : Recipients may share TLP:GREEN information with peers and partner organizations within their sector or community, but not via publicly accessible channels. Information in this category can be circulated widely within a particular community. TLP:GREEN information may not be released outside of the community. lvl : 110 name : TLP:GREEN short_name : TLP:G - aliases : - RESTRICTED css : color : warning description : Recipients may only share TLP:AMBER information with members of their own organization and with clients or customers who need to know the information to protect themselves or prevent further harm. lvl : 120 name : TLP:AMBER short_name : TLP:A # List of required tokens: # A user requesting access to an item must have all the # required tokens the item has, to gain access to it required : # List of alternate names for the token - aliases : [] # Description of the token description : Produced using a commercial tool with limited distribution # Long name for the token name : COMMERCIAL # Short name for the token short_name : CMR # (optional) The minimum classification level an item must have # for this token to be valid. So, because this token has a value # of 120, once it's selected, the classification level automatically # jumps to TPL:A which was set to 120 in the previous section. require_lvl : 120 # List of groups: # A user requesting access to an item must be part of a least # of one the group the item is part of to gain access groups : # List of aliases for the group - aliases : - ANY # (optional) This is a special flag that when set to true if any other groups # are selected in a classification, this group will automatically be selected # as well. auto_select : true # Description of the group description : Employees of CSE # Long name for the group name : CSE # Short name for the group short_name : CSE # (optional) Assuming that this group is the only group selected, this is the # display name that will be used in the classification # NOTE: values must be in the aliases of this group and only this group solitary_display_name : ANY # List of sub-groups: # A user requesting access to an item must be part of a least # of one the sub-group the item is part of to gain access subgroups : # List of aliases for the subgroups - aliases : [] # Description of the sub-group description : Member of Incident Response team # Long name of the sub-group name : IR TEAM # Short name of the sub-group short_name : IR - aliases : [] description : Member of the Canadian Centre for Cyber Security # (optional) This is a special flag that auto select the corresponding # group when this sub-group is selected require_group : CSE name : CCCS short_name : CCCS # (optional) This is a special flag that makes sure that none other then # the corresponding group is selected when this sub-group is selected limited_to_group : CSE # Default restricted classification restricted : TLP:A//CMR # Default unrestricted classification: # When no classification is provided or the classification engine is # disabled, this is the classification value each item will get unrestricted : TLP:W Enabling it in your system \u00b6 By default, the classification engine is disabled in the system, but it can easily be enabled by creating a new config map in Kubernetes. Info For this documentation we will enable a classification engine that supports only the traffic light protocol. In your deployment directory, create a file named objects.yaml with the following content: objects.yaml apiVersion : v1 kind : ConfigMap metadata : name : assemblyline-extra-config data : classification : | enforce: true levels: - aliases: - UNRESTRICTED - U css: color: default description: Subject to standard copyright rules, TLP:WHITE information may be distributed without restriction. lvl: 100 name: TLP:WHITE short_name: TLP:W - aliases: [] css: color: success description: Recipients may share TLP:GREEN information with peers and partner organizations within their sector or community, but not via publicly accessible channels. Information in this category can be circulated widely within a particular community. TLP:GREEN information may not be released outside of the community. lvl: 110 name: TLP:GREEN short_name: TLP:G - aliases: - RESTRICTED - R css: color: warning description: Recipients may only share TLP:AMBER information with members of their own organization and with clients or customers who need to know the information to protect themselves or prevent further harm. lvl: 120 name: TLP:AMBER short_name: TLP:A required: [] groups: [] subgroups: [] restricted: TLP:A unrestricted: TLP:W Use kubectl to apply the objects.yaml file to your system: Appliance sudo microk8s kubectl apply -f ~/git/deployment/objects.yaml --namespace = al Cluster kubectl apply -f <deployment_directory>/objects.yaml --namespace = al Then you must tell your pods to use the classification engine from the newly created config map. Add the following block to the values.yaml or your deployment: Partial values.yaml to load the custom classification.yml file ... coreEnv : - name : CLASSIFICATION_CONFIGMAP value : assemblyline-extra-config - name : CLASSIFICATION_CONFIGMAP_KEY value : classification coreMounts : - name : al-extra-config mountPath : /etc/assemblyline/classification.yml subPath : classification coreVolumes : - name : al-extra-config configMap : name : assemblyline-extra-config ... Finally update your deployment using helm upgrade command : Appliance sudo microk8s helm upgrade assemblyline ~/git/assemblyline-helm-chart/assemblyline -f ~/git/deployment/values.yaml -n al Cluster helm upgrade assemblyline <assemblyline-helm-chart>/assemblyline -f <deployment_directory>/values.yaml -n al Important It takes a while for all the containers to be restarted so be patient and it will eventually show up in the UI.","title":"Classification engine"},{"location":"fr/installation/classification_engine/#classification-engine","text":"Assemblyline can do record-based access control for submission, files, results, and even up to individual sections and tags of said results. It was built to support the Government of Canada levels of security and the Traffic Light Protocol but can also be modified at will. Once turned on, the classification engine will do the following changes to the system: Users will have to be assigned a maximum classification level that they can see as well as the groups they are members of Each submission to the system will have to have a classification level The User Interface will: Show the effective classification of each submission, file, result, and result sections Have a dedicated help section that will explain how classification conflicts are resolved Let you pick a classification while submitting a file Automatically hide portions of the result for a user that does not have enough privileges to see them","title":"Classification engine"},{"location":"fr/installation/classification_engine/#configuration","text":"The classification engine has many parameters that can be customized so you can get record-based access controls that fit your organization. Here is an exhaustive configuration file of the classification engine that explains every single parameter: Exhaustive classification configuration # Turn on/off classification enforcement. When this flag is off, this # completely disables the classification engine, any documents added while # the classification engine is off getting the default unrestricted value enforce : false # When this flag is on, the classification engine will automatically create # groups based on the domain part of a user's email address # EX: # For user with email: test@local.host # The group \"local.host\" will be valid in the system dynamic_groups : false # List of Classification Levels: # This is a graded list; a smaller number is less restricted than a higher number # A user must be allowed a classification level >= to be able to view the data levels : # List of alternate names for the current marking. If a user submits a file with # those markings, the classification will automatically rename it to the value # specified in the name - aliases : - UNRESTRICTED - UNCLASSIFIED - U # Stylesheet applied in the UI for the current classification level css : # Name of the color scheme used for display # Possible values: default, primary, secondary, success, info, warning, error color : default # Deprecated parameters (Use color instead) # These were used in the old UI but are still valid in the new UI because if # the new UI cannot find the color param, it will use the label param and # strips \"label-\" part. banner : alert-default label : label-default text : text-muted # Description of the classification level description : Subject to standard copyright rules, TLP:WHITE information may be distributed without restriction. # Integer value of the Classification level (higher is more classified) lvl : 100 # Long name of the classification level name : TLP:WHITE # Short name of the classification level short_name : TLP:W - aliases : [] css : color : success description : Recipients may share TLP:GREEN information with peers and partner organizations within their sector or community, but not via publicly accessible channels. Information in this category can be circulated widely within a particular community. TLP:GREEN information may not be released outside of the community. lvl : 110 name : TLP:GREEN short_name : TLP:G - aliases : - RESTRICTED css : color : warning description : Recipients may only share TLP:AMBER information with members of their own organization and with clients or customers who need to know the information to protect themselves or prevent further harm. lvl : 120 name : TLP:AMBER short_name : TLP:A # List of required tokens: # A user requesting access to an item must have all the # required tokens the item has, to gain access to it required : # List of alternate names for the token - aliases : [] # Description of the token description : Produced using a commercial tool with limited distribution # Long name for the token name : COMMERCIAL # Short name for the token short_name : CMR # (optional) The minimum classification level an item must have # for this token to be valid. So, because this token has a value # of 120, once it's selected, the classification level automatically # jumps to TPL:A which was set to 120 in the previous section. require_lvl : 120 # List of groups: # A user requesting access to an item must be part of a least # of one the group the item is part of to gain access groups : # List of aliases for the group - aliases : - ANY # (optional) This is a special flag that when set to true if any other groups # are selected in a classification, this group will automatically be selected # as well. auto_select : true # Description of the group description : Employees of CSE # Long name for the group name : CSE # Short name for the group short_name : CSE # (optional) Assuming that this group is the only group selected, this is the # display name that will be used in the classification # NOTE: values must be in the aliases of this group and only this group solitary_display_name : ANY # List of sub-groups: # A user requesting access to an item must be part of a least # of one the sub-group the item is part of to gain access subgroups : # List of aliases for the subgroups - aliases : [] # Description of the sub-group description : Member of Incident Response team # Long name of the sub-group name : IR TEAM # Short name of the sub-group short_name : IR - aliases : [] description : Member of the Canadian Centre for Cyber Security # (optional) This is a special flag that auto select the corresponding # group when this sub-group is selected require_group : CSE name : CCCS short_name : CCCS # (optional) This is a special flag that makes sure that none other then # the corresponding group is selected when this sub-group is selected limited_to_group : CSE # Default restricted classification restricted : TLP:A//CMR # Default unrestricted classification: # When no classification is provided or the classification engine is # disabled, this is the classification value each item will get unrestricted : TLP:W","title":"Configuration"},{"location":"fr/installation/classification_engine/#enabling-it-in-your-system","text":"By default, the classification engine is disabled in the system, but it can easily be enabled by creating a new config map in Kubernetes. Info For this documentation we will enable a classification engine that supports only the traffic light protocol. In your deployment directory, create a file named objects.yaml with the following content: objects.yaml apiVersion : v1 kind : ConfigMap metadata : name : assemblyline-extra-config data : classification : | enforce: true levels: - aliases: - UNRESTRICTED - U css: color: default description: Subject to standard copyright rules, TLP:WHITE information may be distributed without restriction. lvl: 100 name: TLP:WHITE short_name: TLP:W - aliases: [] css: color: success description: Recipients may share TLP:GREEN information with peers and partner organizations within their sector or community, but not via publicly accessible channels. Information in this category can be circulated widely within a particular community. TLP:GREEN information may not be released outside of the community. lvl: 110 name: TLP:GREEN short_name: TLP:G - aliases: - RESTRICTED - R css: color: warning description: Recipients may only share TLP:AMBER information with members of their own organization and with clients or customers who need to know the information to protect themselves or prevent further harm. lvl: 120 name: TLP:AMBER short_name: TLP:A required: [] groups: [] subgroups: [] restricted: TLP:A unrestricted: TLP:W Use kubectl to apply the objects.yaml file to your system: Appliance sudo microk8s kubectl apply -f ~/git/deployment/objects.yaml --namespace = al Cluster kubectl apply -f <deployment_directory>/objects.yaml --namespace = al Then you must tell your pods to use the classification engine from the newly created config map. Add the following block to the values.yaml or your deployment: Partial values.yaml to load the custom classification.yml file ... coreEnv : - name : CLASSIFICATION_CONFIGMAP value : assemblyline-extra-config - name : CLASSIFICATION_CONFIGMAP_KEY value : classification coreMounts : - name : al-extra-config mountPath : /etc/assemblyline/classification.yml subPath : classification coreVolumes : - name : al-extra-config configMap : name : assemblyline-extra-config ... Finally update your deployment using helm upgrade command : Appliance sudo microk8s helm upgrade assemblyline ~/git/assemblyline-helm-chart/assemblyline -f ~/git/deployment/values.yaml -n al Cluster helm upgrade assemblyline <assemblyline-helm-chart>/assemblyline -f <deployment_directory>/values.yaml -n al Important It takes a while for all the containers to be restarted so be patient and it will eventually show up in the UI.","title":"Enabling it in your system"},{"location":"fr/installation/cluster/","text":"Cluster installation \u00b6 Pre-requisites \u00b6 A Kubernetes 1.18+ cluster that has an ingress controller and storage class with read/write many (RWX) support. Assemblyline is known to work with the following Kubernetes providers: Rancher AKS (Azure) EKS (Amazon) GKE (Google) kubectl already configured for your cluster on your machine helm already configured for your cluster on your machine Installation \u00b6 1. Get Assemblyline Helm chart ready \u00b6 Download the latest Assemblyline helm chart Unzip it into a directory of your choice which we will refer to as assemblyline-helm-chart Create a new directory of your choice which will hold your personal deployment configuration. We will refer to it as deployment_directory 2. Create the assemblyline namespace \u00b6 When deploying an Assemblyline instance using our chart, it must be in its own namespace. For this documentation, we will use the al namespace. kubectl create namespace al 3. Setup secrets \u00b6 In the deployment_directory you've just created, create a secrets.yaml file which will contain the different passwords used by Assemblyline. The secrets.yaml file should have the following format apiVersion : v1 kind : Secret metadata : name : assemblyline-system-passwords type : Opaque stringData : datastore-password : logging-password : # If this is the password for backends like azure blob storage, the password may need to be URL-encoded # if it includes non-alphanumeric characters filestore-password : initial-admin-password : Tip Here is an example of secrets.yaml file used for appliance deployments. When you're done setting the different passwords in your secrets.yaml file, upload it to your namespace: kubectl apply -f <deployment_directory>/secrets.yaml --namespace = al Warning From this point on, you will not need the secret.yaml file anymore. You should delete it. 4. Configure your deployment \u00b6 In your deployment_directory , create a values.yaml file which will contain the configuration specific to your deployment. Tip For an exhaustive view of all the possible parameters you can change the values.yaml you've created, refer to the assemblyline-helm-chart/assemblyline/values.yaml file. These are the strict minimum configuration changes you will need to do: Setup the ingress controller by changing the values of: ingressAnnotations.cert-manager.io/issuer: (Name of the issuer in K8s. This is for cert validation) tlsSecretName (Name of the TLS cert in k8s. This is for cert validation) configuration.ui.fqdn (Domain name for your al instance). Setup the storage classes according to your Kubernetes cluster : redisStorageClass (Use SSD backed managed disks) log-storage.volumeClaimTemplate.storageClassName (Use SSD backed managed disks) datastore.volumeClaimTemplate.storageClassName (Use SSD backed managed disks) updateStorageClass (Use standard file sharing disks) persistentStorageClass (Use standard file sharing disks) sharedStorageClass (Use standard file sharing disks) Decide where you want files stored, set the appropriate URI in the configuration.filestore.* fields. You should try to avoid using the internal filestore and use something like Azure blob store, Amazon S3... Enable/disable/configure logging features, (disabled by default). This is an example values.yaml file to get you started # 1. Setup the ingress controller ingressAnnotations : kubernetes.io/ingress.class : \"nginx\" nginx.ingress.kubernetes.io/proxy-body-size : 100M cert-manager.io/issuer : <CHANGE_ME> tlsSecretName : <CHANGE_ME> # 2. Setup the storage classes according to your Kubernetes cluster redisStorageClass : <CHANGE_ME> datastore : volumeClaimTemplate : storageClassName : <CHANGE_ME> log-storage : volumeClaimTemplate : storageClassName : <CHANGE_ME> updateStorageClass : <CHANGE_ME> persistantStorageClass : <CHANGE_ME> sharedStorageClass : <CHANGE_ME> # 3. Decide where you want files stored internalFilestore : false # Un-comment and setup if internal filestore used #filestore: # persistence: # size: 500Gi # StorageClass: <CHANGE_ME> # 4. Enable/disable/configure logging features enableLogging : false enableMetrics : false enableAPM : false internalELKStack : false seperateInternalELKStack : false loggingUsername : elastic loggingTLSVerify : \"none\" # Internal configuration for assemblyline components. See the assemblyline # administration documentation for more details. # https://cybercentrecanada.github.io/assemblyline4_docs/configuration/config_file/ configuration : # 1. Setup the ingress controller submission : max_file_size : 104857600 ui : fqdn : \"localhost\" # 3. Decide where you want files stored filestore : cache : [ \"s3://${INTERNAL_FILESTORE_ACCESS}:${INTERNAL_FILESTORE_KEY}@filestore:9000?s3_bucket=al-cache&use_ssl=False\" ] storage : [ \"s3://${INTERNAL_FILESTORE_ACCESS}:${INTERNAL_FILESTORE_KEY}@filestore:9000?s3_bucket=al-storage&use_ssl=False\" ] # 4. Enable/disable/configure logging features logging : log_level : WARNING 5. Deploy your current configuration \u00b6 Now that you've fully configured your values.yaml file, you can simply deploy it via helm by referencing the default assemblyline helm chart. helm install assemblyline <assemblyline-helm-chart>/assemblyline -f <deployment_directory>/values.yaml -n al Warning After you've ran the helm install command, the system has a lot of setting up to do (Creating database indexes, loading service, setting up default accounts, loading signatures ...). Don't expect it to be fully operational for at least the next 15 minutes. Update your deployment \u00b6 Once you have your Assemblyline chart deployed through helm, you can change any values in the values.yaml file and upgrade your deployment with the following command: helm upgrade assemblyline <assemblyline-helm-chart>/assemblyline -f <deployment_directory>/values.yaml -n al","title":"Cluster installation"},{"location":"fr/installation/cluster/#cluster-installation","text":"","title":"Cluster installation"},{"location":"fr/installation/cluster/#pre-requisites","text":"A Kubernetes 1.18+ cluster that has an ingress controller and storage class with read/write many (RWX) support. Assemblyline is known to work with the following Kubernetes providers: Rancher AKS (Azure) EKS (Amazon) GKE (Google) kubectl already configured for your cluster on your machine helm already configured for your cluster on your machine","title":"Pre-requisites"},{"location":"fr/installation/cluster/#installation","text":"","title":"Installation"},{"location":"fr/installation/cluster/#1-get-assemblyline-helm-chart-ready","text":"Download the latest Assemblyline helm chart Unzip it into a directory of your choice which we will refer to as assemblyline-helm-chart Create a new directory of your choice which will hold your personal deployment configuration. We will refer to it as deployment_directory","title":"1. Get Assemblyline Helm chart ready"},{"location":"fr/installation/cluster/#2-create-the-assemblyline-namespace","text":"When deploying an Assemblyline instance using our chart, it must be in its own namespace. For this documentation, we will use the al namespace. kubectl create namespace al","title":"2. Create the assemblyline namespace"},{"location":"fr/installation/cluster/#3-setup-secrets","text":"In the deployment_directory you've just created, create a secrets.yaml file which will contain the different passwords used by Assemblyline. The secrets.yaml file should have the following format apiVersion : v1 kind : Secret metadata : name : assemblyline-system-passwords type : Opaque stringData : datastore-password : logging-password : # If this is the password for backends like azure blob storage, the password may need to be URL-encoded # if it includes non-alphanumeric characters filestore-password : initial-admin-password : Tip Here is an example of secrets.yaml file used for appliance deployments. When you're done setting the different passwords in your secrets.yaml file, upload it to your namespace: kubectl apply -f <deployment_directory>/secrets.yaml --namespace = al Warning From this point on, you will not need the secret.yaml file anymore. You should delete it.","title":"3. Setup secrets"},{"location":"fr/installation/cluster/#4-configure-your-deployment","text":"In your deployment_directory , create a values.yaml file which will contain the configuration specific to your deployment. Tip For an exhaustive view of all the possible parameters you can change the values.yaml you've created, refer to the assemblyline-helm-chart/assemblyline/values.yaml file. These are the strict minimum configuration changes you will need to do: Setup the ingress controller by changing the values of: ingressAnnotations.cert-manager.io/issuer: (Name of the issuer in K8s. This is for cert validation) tlsSecretName (Name of the TLS cert in k8s. This is for cert validation) configuration.ui.fqdn (Domain name for your al instance). Setup the storage classes according to your Kubernetes cluster : redisStorageClass (Use SSD backed managed disks) log-storage.volumeClaimTemplate.storageClassName (Use SSD backed managed disks) datastore.volumeClaimTemplate.storageClassName (Use SSD backed managed disks) updateStorageClass (Use standard file sharing disks) persistentStorageClass (Use standard file sharing disks) sharedStorageClass (Use standard file sharing disks) Decide where you want files stored, set the appropriate URI in the configuration.filestore.* fields. You should try to avoid using the internal filestore and use something like Azure blob store, Amazon S3... Enable/disable/configure logging features, (disabled by default). This is an example values.yaml file to get you started # 1. Setup the ingress controller ingressAnnotations : kubernetes.io/ingress.class : \"nginx\" nginx.ingress.kubernetes.io/proxy-body-size : 100M cert-manager.io/issuer : <CHANGE_ME> tlsSecretName : <CHANGE_ME> # 2. Setup the storage classes according to your Kubernetes cluster redisStorageClass : <CHANGE_ME> datastore : volumeClaimTemplate : storageClassName : <CHANGE_ME> log-storage : volumeClaimTemplate : storageClassName : <CHANGE_ME> updateStorageClass : <CHANGE_ME> persistantStorageClass : <CHANGE_ME> sharedStorageClass : <CHANGE_ME> # 3. Decide where you want files stored internalFilestore : false # Un-comment and setup if internal filestore used #filestore: # persistence: # size: 500Gi # StorageClass: <CHANGE_ME> # 4. Enable/disable/configure logging features enableLogging : false enableMetrics : false enableAPM : false internalELKStack : false seperateInternalELKStack : false loggingUsername : elastic loggingTLSVerify : \"none\" # Internal configuration for assemblyline components. See the assemblyline # administration documentation for more details. # https://cybercentrecanada.github.io/assemblyline4_docs/configuration/config_file/ configuration : # 1. Setup the ingress controller submission : max_file_size : 104857600 ui : fqdn : \"localhost\" # 3. Decide where you want files stored filestore : cache : [ \"s3://${INTERNAL_FILESTORE_ACCESS}:${INTERNAL_FILESTORE_KEY}@filestore:9000?s3_bucket=al-cache&use_ssl=False\" ] storage : [ \"s3://${INTERNAL_FILESTORE_ACCESS}:${INTERNAL_FILESTORE_KEY}@filestore:9000?s3_bucket=al-storage&use_ssl=False\" ] # 4. Enable/disable/configure logging features logging : log_level : WARNING","title":"4. Configure your deployment"},{"location":"fr/installation/cluster/#5-deploy-your-current-configuration","text":"Now that you've fully configured your values.yaml file, you can simply deploy it via helm by referencing the default assemblyline helm chart. helm install assemblyline <assemblyline-helm-chart>/assemblyline -f <deployment_directory>/values.yaml -n al Warning After you've ran the helm install command, the system has a lot of setting up to do (Creating database indexes, loading service, setting up default accounts, loading signatures ...). Don't expect it to be fully operational for at least the next 15 minutes.","title":"5. Deploy your current configuration"},{"location":"fr/installation/cluster/#update-your-deployment","text":"Once you have your Assemblyline chart deployed through helm, you can change any values in the values.yaml file and upgrade your deployment with the following command: helm upgrade assemblyline <assemblyline-helm-chart>/assemblyline -f <deployment_directory>/values.yaml -n al","title":"Update your deployment"},{"location":"fr/installation/deployment/","text":"Choose your deployment type \u00b6 Assemblyline has two distinctive deployment types: Appliance : Single host deployment Cluster : Multi-host deployment Warning Keep in mind that you will need extra hosts for running external resources such as anti-virus products, or sandboxes (such as Cuckoo Sandbox ). These complementary products are not mandatory but will greatly complement the static analysis and file extraction performed by Assemblyline. Features \u00b6 Both deployments are the same in terms of analysis capabilities however a cluster deployment can be scaled to scan multiple millions of files per day and offer redundancy and failover. If you are deploying in the cloud, a cluster will be easier to deploy (using cloud Kubernetes offerings). However, in a lab or to support an incident response team, any powerful computer with an appliance deployment will be able to process thousands of files a day. Tip If you choose to deploy a MicroK8s appliance, you will be able to scaled it up to a small cluster using multiple machines (nodes) if need be. Deployment features rundown \u00b6 Appliance (Docker) Appliance (MicroK8s) Cluster Support all analysis capabilities Yes Yes Yes Single host installation Yes Yes No Easy step by step installation Yes Yes No Simple to deploy and manage Yes No No Multiple host installation No Optional Yes Auto-scaling of core components No Optional Yes High volume throughput No No Yes Cloud provider support (AKS, EKS, GKE...) No No Yes Redundancy / Failover support No No Yes Installation stack \u00b6 Both clustered and MicroK8s deployments use a very similar stack in the background which allows them to share the same Helm chart. Only small changes in the values.yml file are required to differentiate them. As for the Docker compose appliance deployment, it uses a simpler stack which is easier to maintain and reset but offer less features to scale to high capacity. Installation instructions \u00b6 Now that you know the difference between the two types of deployment, you can refer to their respective installation instruction to get you started. Appliance installation (Docker) Appliance installation (Microk8s) Cluster installation Tip Consider reading the configuration section of the documentation before jumping into the installation instruction. This will help you understand all the different options you can modify during the installation process.","title":"Choose your deployment type"},{"location":"fr/installation/deployment/#choose-your-deployment-type","text":"Assemblyline has two distinctive deployment types: Appliance : Single host deployment Cluster : Multi-host deployment Warning Keep in mind that you will need extra hosts for running external resources such as anti-virus products, or sandboxes (such as Cuckoo Sandbox ). These complementary products are not mandatory but will greatly complement the static analysis and file extraction performed by Assemblyline.","title":"Choose your deployment type"},{"location":"fr/installation/deployment/#features","text":"Both deployments are the same in terms of analysis capabilities however a cluster deployment can be scaled to scan multiple millions of files per day and offer redundancy and failover. If you are deploying in the cloud, a cluster will be easier to deploy (using cloud Kubernetes offerings). However, in a lab or to support an incident response team, any powerful computer with an appliance deployment will be able to process thousands of files a day. Tip If you choose to deploy a MicroK8s appliance, you will be able to scaled it up to a small cluster using multiple machines (nodes) if need be.","title":"Features"},{"location":"fr/installation/deployment/#deployment-features-rundown","text":"Appliance (Docker) Appliance (MicroK8s) Cluster Support all analysis capabilities Yes Yes Yes Single host installation Yes Yes No Easy step by step installation Yes Yes No Simple to deploy and manage Yes No No Multiple host installation No Optional Yes Auto-scaling of core components No Optional Yes High volume throughput No No Yes Cloud provider support (AKS, EKS, GKE...) No No Yes Redundancy / Failover support No No Yes","title":"Deployment features rundown"},{"location":"fr/installation/deployment/#installation-stack","text":"Both clustered and MicroK8s deployments use a very similar stack in the background which allows them to share the same Helm chart. Only small changes in the values.yml file are required to differentiate them. As for the Docker compose appliance deployment, it uses a simpler stack which is easier to maintain and reset but offer less features to scale to high capacity.","title":"Installation stack"},{"location":"fr/installation/deployment/#installation-instructions","text":"Now that you know the difference between the two types of deployment, you can refer to their respective installation instruction to get you started. Appliance installation (Docker) Appliance installation (Microk8s) Cluster installation Tip Consider reading the configuration section of the documentation before jumping into the installation instruction. This will help you understand all the different options you can modify during the installation process.","title":"Installation instructions"},{"location":"fr/installation/monitoring/","text":"Monitoring with ELK \u00b6 The Assemblyline helm chart gives you the option of pointing logs to an existing ELK stack or having Assemblyline create its own internal ELK for logging and metrics. Elk Stack configuration \u00b6 In the values.yaml file of your deployment, you can edit the following parameters to configure Assemblyline to send metrics and logs to a specific ELK stack. Choose the type of ELK stack deployment that corresponds the best to your setup: Appliance Internal ELK stack Partial values.yaml config for an Appliance internal ELK stack ... # Have Assemblyline send logs to the configured ELK stack enableLogging : true # Have Assemblyline send metrics to the configured ELK stack enableMetrics : true # This would have Assemblyline send APM metrics to the # configured ELK stack as well but it is very costly in # terms of resources so only turn it on if you really # need insight on API response time and core components # operation timing. enableAPM : false # We are setting up an internal ELK stack so we can turn that on internalELKStack : true # Because this is an appliance, we will reuse the same elastic # database used for data to store logs as well seperateInternalELKStack : false # The internal ELK stack use elastic as its base username and # does not verify TLS loggingUsername : elastic loggingTLSVerify : none ... Cluster Internal ELK stack Partial values.yaml config for a cluster internal ELK stack ... # Have Assemblyline send logs to the configured ELK stack enableLogging : true # Have Assemblyline send metrics to the configured ELK stack enableMetrics : true # This would have Assemblyline send APM metrics to the # configured ELK stack as well but it is very costly in # terms of resources so only turn it on if you really # need insight on API response time and core components # operation timing. enableAPM : false # We are setting up an internal ELK stack so we can turn that on internalELKStack : true # Because this is a cluster, we will have Assemblyline spin up # a completely different elastic database so the logging does not # interfere with the performance of the data seperateInternalELKStack : true # The internal ELK stack use elastic as its base username and # does not verify TLS loggingUsername : elastic loggingTLSVerify : none ... External ELK stack Partial values.yaml config for external ELK stack ... # Have Assemblyline send logs to the configured ELK stack enableLogging : true # Have Assemblyline send metrics to the configured ELK stack enableMetrics : true # This would have Assemblyline send APM metrics to the # configured ELK stack as well but it is very costly in # terms of resources so only turn it on if you really # need insight on API response time and core components # operation timing. enableAPM : false # We are setting up an external ELK stack so we will disable # those settings internalELKStack : false seperateInternalELKStack : false # -- EXTERNAL ELK Stack config -- # Elastic host where the logs will be shipped to loggingHost : https://<ELK_HOST>:443/ # Kibana dashboard location kibanaHost : https://<ELK_HOST>:443/kibana # Username that will be used to login to the elastic on your # ELK stack loggingUsername : <YOUR_ELK_USERNAME> # Should you verify TLS on your ELK stack? loggingTLSVerify : \"full\" # Finally configure al_metrics to save metrics to your stack configuration : core : metrics : elasticsearch : hosts : [ \"https://${LOGGING_USERNAME}:${LOGGING_PASSWORD}@<ELK_HOST>:443\" ] # If you're using HTTPS and don't want certificate failures you can put # your CA here host_certificates : | -----BEGIN CERTIFICATE----- ... -----END CERTIFICATE----- ... Finally update your deployment using helm upgrade command : Appliance sudo microk8s helm upgrade assemblyline ~/git/assemblyline-helm-chart/assemblyline -f ~/git/deployment/values.yaml -n al Cluster helm upgrade assemblyline <assemblyline-helm-chart>/assemblyline -f <deployment_directory>/values.yaml -n al Logstash Pipelines \u00b6 You can write custom pipelines to help enrich your data when passed through Logstash. You can set your custom Logstash pipeline under customLogstashPipeline in your values.yaml file of your deployment. Partial values.yaml to add a simple Logstash pipeline ... # Turn on Logstash support useLogstash : true customLogstashPipeline : | input { beats { port => 5044 codec => \"json\" } } filter{ mutate { add_field => {\"sent_to_logstash\" => \"True\"} } } output { elasticsearch{ hosts => \"http://elasticsearch:9200\" index => \"assemblyline-logs\" codec => \"json\" } } ... Kibana Dashboards \u00b6 Within Kibana, there is the ability to use dashboards to visualize your data into one consolidated view to make it easier for monitoring, like a hub. You can get our latest exported dashboards directly from the assemblyline-base source and then use Kibana import features to use them in your ELK Stack. Creation \u00b6 Dashboards are made up of visualizations, and these can come in different forms: graphs, metrics, gauges, tables, maps, etc. Each visualization requires an index pattern to get the data from and setting a date range, this throws all relevant data within the specified timeframe into a bucket to be used by the visualization. Dashboards can also be imported/exported for use across different ELKs but require dependencies like index patterns for them to function out of the box, otherwise requires editing the dashboard file. Navigation \u00b6 All dashboards give you the ability to filter your data, like what you will find under the Discover tab of Kibana. This will allow you to filter a certain dashboard based on a query you give. If you want more info about using Kibana's filtering and navigation feature, check the explore your data documentation.","title":"Monitoring with ELK"},{"location":"fr/installation/monitoring/#monitoring-with-elk","text":"The Assemblyline helm chart gives you the option of pointing logs to an existing ELK stack or having Assemblyline create its own internal ELK for logging and metrics.","title":"Monitoring with ELK"},{"location":"fr/installation/monitoring/#elk-stack-configuration","text":"In the values.yaml file of your deployment, you can edit the following parameters to configure Assemblyline to send metrics and logs to a specific ELK stack. Choose the type of ELK stack deployment that corresponds the best to your setup: Appliance Internal ELK stack Partial values.yaml config for an Appliance internal ELK stack ... # Have Assemblyline send logs to the configured ELK stack enableLogging : true # Have Assemblyline send metrics to the configured ELK stack enableMetrics : true # This would have Assemblyline send APM metrics to the # configured ELK stack as well but it is very costly in # terms of resources so only turn it on if you really # need insight on API response time and core components # operation timing. enableAPM : false # We are setting up an internal ELK stack so we can turn that on internalELKStack : true # Because this is an appliance, we will reuse the same elastic # database used for data to store logs as well seperateInternalELKStack : false # The internal ELK stack use elastic as its base username and # does not verify TLS loggingUsername : elastic loggingTLSVerify : none ... Cluster Internal ELK stack Partial values.yaml config for a cluster internal ELK stack ... # Have Assemblyline send logs to the configured ELK stack enableLogging : true # Have Assemblyline send metrics to the configured ELK stack enableMetrics : true # This would have Assemblyline send APM metrics to the # configured ELK stack as well but it is very costly in # terms of resources so only turn it on if you really # need insight on API response time and core components # operation timing. enableAPM : false # We are setting up an internal ELK stack so we can turn that on internalELKStack : true # Because this is a cluster, we will have Assemblyline spin up # a completely different elastic database so the logging does not # interfere with the performance of the data seperateInternalELKStack : true # The internal ELK stack use elastic as its base username and # does not verify TLS loggingUsername : elastic loggingTLSVerify : none ... External ELK stack Partial values.yaml config for external ELK stack ... # Have Assemblyline send logs to the configured ELK stack enableLogging : true # Have Assemblyline send metrics to the configured ELK stack enableMetrics : true # This would have Assemblyline send APM metrics to the # configured ELK stack as well but it is very costly in # terms of resources so only turn it on if you really # need insight on API response time and core components # operation timing. enableAPM : false # We are setting up an external ELK stack so we will disable # those settings internalELKStack : false seperateInternalELKStack : false # -- EXTERNAL ELK Stack config -- # Elastic host where the logs will be shipped to loggingHost : https://<ELK_HOST>:443/ # Kibana dashboard location kibanaHost : https://<ELK_HOST>:443/kibana # Username that will be used to login to the elastic on your # ELK stack loggingUsername : <YOUR_ELK_USERNAME> # Should you verify TLS on your ELK stack? loggingTLSVerify : \"full\" # Finally configure al_metrics to save metrics to your stack configuration : core : metrics : elasticsearch : hosts : [ \"https://${LOGGING_USERNAME}:${LOGGING_PASSWORD}@<ELK_HOST>:443\" ] # If you're using HTTPS and don't want certificate failures you can put # your CA here host_certificates : | -----BEGIN CERTIFICATE----- ... -----END CERTIFICATE----- ... Finally update your deployment using helm upgrade command : Appliance sudo microk8s helm upgrade assemblyline ~/git/assemblyline-helm-chart/assemblyline -f ~/git/deployment/values.yaml -n al Cluster helm upgrade assemblyline <assemblyline-helm-chart>/assemblyline -f <deployment_directory>/values.yaml -n al","title":"Elk Stack configuration"},{"location":"fr/installation/monitoring/#logstash-pipelines","text":"You can write custom pipelines to help enrich your data when passed through Logstash. You can set your custom Logstash pipeline under customLogstashPipeline in your values.yaml file of your deployment. Partial values.yaml to add a simple Logstash pipeline ... # Turn on Logstash support useLogstash : true customLogstashPipeline : | input { beats { port => 5044 codec => \"json\" } } filter{ mutate { add_field => {\"sent_to_logstash\" => \"True\"} } } output { elasticsearch{ hosts => \"http://elasticsearch:9200\" index => \"assemblyline-logs\" codec => \"json\" } } ...","title":"Logstash Pipelines"},{"location":"fr/installation/monitoring/#kibana-dashboards","text":"Within Kibana, there is the ability to use dashboards to visualize your data into one consolidated view to make it easier for monitoring, like a hub. You can get our latest exported dashboards directly from the assemblyline-base source and then use Kibana import features to use them in your ELK Stack.","title":"Kibana Dashboards"},{"location":"fr/installation/monitoring/#creation","text":"Dashboards are made up of visualizations, and these can come in different forms: graphs, metrics, gauges, tables, maps, etc. Each visualization requires an index pattern to get the data from and setting a date range, this throws all relevant data within the specified timeframe into a bucket to be used by the visualization. Dashboards can also be imported/exported for use across different ELKs but require dependencies like index patterns for them to function out of the box, otherwise requires editing the dashboard file.","title":"Creation"},{"location":"fr/installation/monitoring/#navigation","text":"All dashboards give you the ability to filter your data, like what you will find under the Discover tab of Kibana. This will allow you to filter a certain dashboard based on a query you give. If you want more info about using Kibana's filtering and navigation feature, check the explore your data documentation.","title":"Navigation"},{"location":"fr/installation/configuration/authentication/","text":"Authentication section \u00b6 Assemblyline comes with a built-in user management database, so no external identity sources are required. However, to facilitate user management in larger organizations you can integrate Assemblyline with external identity providers. The authentication section ( auth: ) of the configuration files contains all the different parameters that you can change to turn on/off the different authentication features that Assemblyline supports. Default values for the authentication section ... auth : allow_2fa : true allow_apikeys : true allow_extended_apikeys : true allow_security_tokens : true internal : enabled : true failure_ttl : 60 max_failures : 5 password_requirements : lower : false min_length : 12 number : false special : false upper : false signup : enabled : false notify : activated_template : null api_key : null authorization_template : null base_url : null password_reset_template : null registration_template : null smtp : from_adr : null host : null password : null port : 587 tls : true user : null valid_email_patterns : - .* - .*@localhost ldap : admin_dn : null auto_create : true auto_sync : true base : ou=people,dc=assemblyline,dc=local bind_pass : null bind_user : null classification_mappings : {} email_field : mail enabled : false group_lookup_query : (&(objectClass=Group)(member=%s)) image_field : jpegPhoto image_format : jpeg name_field : cn signature_importer_dn : null signature_manager_dn : null uid_field : uid uri : ldap://localhost:389 oauth : enabled : false gravatar_enabled : true providers : auth0 : access_token_url : https://{TENANT}.auth0.com/oauth/token api_base_url : https://{TENANT}.auth0.com/ authorize_url : https://{TENANT}.auth0.com/authorize client_id : null client_kwargs : scope : openid email profile client_secret : null jwks_uri : https://{TENANT}.auth0.com/.well-known/jwks.json user_get : userinfo azure_ad : access_token_url : https://login.microsoftonline.com/common/oauth2/token api_base_url : https://login.microsoft.com/common/ authorize_url : https://login.microsoftonline.com/common/oauth2/authorize client_id : null client_kwargs : scope : openid email profile client_secret : null jwks_uri : https://login.microsoftonline.com/common/discovery/v2.0/keys user_get : openid/userinfo google : access_token_url : https://oauth2.googleapis.com/token api_base_url : https://openidconnect.googleapis.com/ authorize_url : https://accounts.google.com/o/oauth2/v2/auth client_id : null client_kwargs : scope : openid email profile client_secret : null jwks_uri : https://www.googleapis.com/oauth2/v3/certs user_get : v1/userinfo ... Tip Refer to the changing the configuration file documentation for more detail on where and how to change the configuration of the system. Parameter definitions \u00b6 The auth configuration block has a few parameters at the top level that help you turn on or off a few security features supported in the system. Here is an example of a configuration block with those top-level parameters and an explanation of what they do: Top-level parameters auth : # Turns on/off two-factor authentication in the system allow_2fa : true # Turn on/off usage of API Keys in the system # NOTE: if you turn this off, this will severely limit API access allow_apikeys : true # Turn on/off usage of extended API via the API keys allow_extended_apikeys : true # Turn on/off usage of security token as two-factor authentication (ex: yubikeys) allow_security_tokens : true Internal authenticator \u00b6 The configuration block at auth.internal allows you to configure the Assemblyline internal authenticator. Here is an example of a configuration block with inline comments about the purpose of every single parameter: Internal auth configuration example auth : internal : # Enable or disable the internal authenticator enabled : true # Time in seconds the user will have to wait after # too many authentication failures failure_ttl : 60 # Number of authentication failures before temporarily # locking down the user max_failures : 5 # Password complexity requirements for the system password_requirements : # Are lowercase characters mandatory? lower : false # What is the minimal password length min_length : 12 # Are numbers mandatory? number : false # Are special characters mandatory? special : false # Are uppercase characters mandatory? upper : false signup : # Can a user automatically signup for the system enabled : false # Configuration block for GC Notify signup and password reset # see: https://notification.canada.ca/ notify : activated_template : null api_key : null authorization_template : null base_url : null password_reset_template : null registration_template : null # Configuration block for SMTP signup and password reset smtp : # Email address used for sender from_adr : null # Host of the SMTP server host : null # Password for the SMTP server password : null # Port of the SMTP server port : 587 # Should we communicate with SMTP server via TLS? tls : true # User to authenticate to the SMTP server user : null # Email patterns that will be allowed to # automatically signup for an account valid_email_patterns : - .* - .*@localhost LDAP Authentication \u00b6 The configuration block at auth.ldap allows you to easily add authentication via your LDAP server. The LDAP authentication module will be able to automatically assign roles, classification, avatar, name, and email address based on the properties of the LDAP user and the groups it is a member of. Here is an example configuration block to add to your configuration file that will allow you to connect to the docker-test-openldap server from: https://github.com/rroemhild/docker-test-openldap LDAP configuration example auth : internal : # Disable internal login, you could also leave it on if you want enabled : false ldap : # Should LDAP be enabled or not? enabled : true # DN of the group or the user who will get admin privileges admin_dn : cn=admin_staff,ou=people,dc=planetexpress,dc=com # Auto-create users if they are missing, this means # that if a user exists in LDAP, Assemblyline will create an # account for it upon the first login auto_create : true # Should we automatically sync roles, classification, avatar # email, name... with the LDAP server upon each login? auto_sync : true # Base DN for the users base : ou=people,dc=planetexpress,dc=com # Password used to query the LDAP server bind_pass : null # User use to query the LDAP server bind_user : null classification_mappings : {} # Name of the field containing the email address email_field : mail # How the group lookup is queried group_lookup_query : (&(objectClass=Group)(member=%s)) # Name of the field containing the user's avatar image_field : jpegPhoto # Type of image used to store the avatar image_format : jpeg # Name of the field containing the user's name name_field : cn # DN of the group or the user who will get signature_importer role signature_importer_dn : null # DN of the group or the user who will get signature_manager role signature_manager_dn : null # Field name for the UID uid_field : uid # URI to the LDAP server uri : ldaps://<ldap_ip_or_domain>:636 OAuth Authentication \u00b6 The configuration block at auth.oauth allows you to add OAuth authentication to your system. Assemblyline OAuth module is configurable enough to allow you to use almost any OAuth provider. It has been thoroughly tested with: Microsoft Accounts Google Accounts Auth0 Microsoft Azure Active Directory Accounts Here is an exhaustive configuration block that explains every single parameter from the OAuth configuration block: Exhaustive OAuth configuration example auth : internal : # Disable internal login, you could also leave it on if you want enabled : false oauth : # Should OAuth authentication be enabled or not enabled : true # Should we try to pull the user's avatar using gravatar gravatar_enabled : false # OAuth providers configuration block, you can have as many OAuth # providers as you want providers : # Name of the provider displayed in the UI local_provider : # Auto-create users if they are missing, this means # that if a user exists in the OAuth provider, Assemblyline # will create an account for it upon the first login # WARNING: If you set it to true for let's say Google's # OAuth provider, anyone with a google account # essentially has access to your system auto_create : true # Should we automatically sync roles, classification, avatar # email, name... with the OAuth provider upon each login? auto_sync : true # Automatic role and classification assignments auto_properties : # any user with a @localhost.local email will be given # TLP:Amber classification - field : email pattern : .*@localhost\\.local$ type : classification value : \"TLP:A\" # any user within the admins-sg will be made # administrator in the system - field : groups pattern : ^admins-sg$ type : role value : admin # URL used to get the access token access_token_url : https://oauth2.localhost/token # Base URL for downloading the user's and groups info api_base_url : https://openidconnect.localhost/ # URL used to authorize access to a resource authorize_url : https://localhost/oauth2/auth # ID of your application to authenticate to the OAuth # provider client_id : null # Password to your application to authenticate to the # OAuth provider client_secret : null # Keyword arguments passed to the different URLs # (to set the scope for example) client_kwargs : scope : openid email profile # URL used to verify if a returned JWKS token is valid jwks_uri : https://localhost/oauth2/certs # Name of the field that will contain the user ID uid_field : uid # Should we generate a random username for the # authenticated user? uid_randomize : false # How many digits should we add at the end of the username? uid_randomize_digits : 0 # What is the delimiter used by the random name generator? uid_randomize_delimiter : \"-\" # Reged used to parse and email address and capture parts # to create a user ID out of it uid_regex : ^(.*)@(\\w*).*$ # Format of the user ID based on the captured parts from the regex uid_format : '{}-{}' # Should we use the new callback method? use_new_callback_format : true # Path from the base_url to fetch the user info user_get : user/info # Path from the base to fetch the group info user_groups : group/info # Field return by the group info API call that contains the # list of groups user_groups_data_field : null # Name of the field in the list of groups that contains the # name of the group user_groups_name_field : null Here is an example configuration block that would let you use Auth0 if you would change your client_id and client_secret and that you would change the tenant_name to yours: Auth0 configuration example auth : internal : # Disable internal login, you could also leave it on if you want enabled : false oauth : # Enable oAuth enabled : true # Setup the auto0 provider providers : auth0 : # It is safe to auto-create users here # because it is your OAuth tenant auto_create : true auto_sync : true # Put your client ID and secret here client_id : <YOUR_CLIENT_ID> client_secret : <YOUR_CLIENT_SECRET> client_kwargs : scope : openid email profile # Set your tenant's name in the following URLs access_token_url : https://<TENANT_NAME>.auth0.com/oauth/token api_base_url : https://<TENANT_NAME>.auth0.com/ authorize_url : https://<TENANT_NAME>.auth0.com/authorize jwks_uri : https://<TENANT_NAME>.auth0.com/.well-known/jwks.json user_get : userinfo","title":"Authentication section"},{"location":"fr/installation/configuration/authentication/#authentication-section","text":"Assemblyline comes with a built-in user management database, so no external identity sources are required. However, to facilitate user management in larger organizations you can integrate Assemblyline with external identity providers. The authentication section ( auth: ) of the configuration files contains all the different parameters that you can change to turn on/off the different authentication features that Assemblyline supports. Default values for the authentication section ... auth : allow_2fa : true allow_apikeys : true allow_extended_apikeys : true allow_security_tokens : true internal : enabled : true failure_ttl : 60 max_failures : 5 password_requirements : lower : false min_length : 12 number : false special : false upper : false signup : enabled : false notify : activated_template : null api_key : null authorization_template : null base_url : null password_reset_template : null registration_template : null smtp : from_adr : null host : null password : null port : 587 tls : true user : null valid_email_patterns : - .* - .*@localhost ldap : admin_dn : null auto_create : true auto_sync : true base : ou=people,dc=assemblyline,dc=local bind_pass : null bind_user : null classification_mappings : {} email_field : mail enabled : false group_lookup_query : (&(objectClass=Group)(member=%s)) image_field : jpegPhoto image_format : jpeg name_field : cn signature_importer_dn : null signature_manager_dn : null uid_field : uid uri : ldap://localhost:389 oauth : enabled : false gravatar_enabled : true providers : auth0 : access_token_url : https://{TENANT}.auth0.com/oauth/token api_base_url : https://{TENANT}.auth0.com/ authorize_url : https://{TENANT}.auth0.com/authorize client_id : null client_kwargs : scope : openid email profile client_secret : null jwks_uri : https://{TENANT}.auth0.com/.well-known/jwks.json user_get : userinfo azure_ad : access_token_url : https://login.microsoftonline.com/common/oauth2/token api_base_url : https://login.microsoft.com/common/ authorize_url : https://login.microsoftonline.com/common/oauth2/authorize client_id : null client_kwargs : scope : openid email profile client_secret : null jwks_uri : https://login.microsoftonline.com/common/discovery/v2.0/keys user_get : openid/userinfo google : access_token_url : https://oauth2.googleapis.com/token api_base_url : https://openidconnect.googleapis.com/ authorize_url : https://accounts.google.com/o/oauth2/v2/auth client_id : null client_kwargs : scope : openid email profile client_secret : null jwks_uri : https://www.googleapis.com/oauth2/v3/certs user_get : v1/userinfo ... Tip Refer to the changing the configuration file documentation for more detail on where and how to change the configuration of the system.","title":"Authentication section"},{"location":"fr/installation/configuration/authentication/#parameter-definitions","text":"The auth configuration block has a few parameters at the top level that help you turn on or off a few security features supported in the system. Here is an example of a configuration block with those top-level parameters and an explanation of what they do: Top-level parameters auth : # Turns on/off two-factor authentication in the system allow_2fa : true # Turn on/off usage of API Keys in the system # NOTE: if you turn this off, this will severely limit API access allow_apikeys : true # Turn on/off usage of extended API via the API keys allow_extended_apikeys : true # Turn on/off usage of security token as two-factor authentication (ex: yubikeys) allow_security_tokens : true","title":"Parameter definitions"},{"location":"fr/installation/configuration/authentication/#internal-authenticator","text":"The configuration block at auth.internal allows you to configure the Assemblyline internal authenticator. Here is an example of a configuration block with inline comments about the purpose of every single parameter: Internal auth configuration example auth : internal : # Enable or disable the internal authenticator enabled : true # Time in seconds the user will have to wait after # too many authentication failures failure_ttl : 60 # Number of authentication failures before temporarily # locking down the user max_failures : 5 # Password complexity requirements for the system password_requirements : # Are lowercase characters mandatory? lower : false # What is the minimal password length min_length : 12 # Are numbers mandatory? number : false # Are special characters mandatory? special : false # Are uppercase characters mandatory? upper : false signup : # Can a user automatically signup for the system enabled : false # Configuration block for GC Notify signup and password reset # see: https://notification.canada.ca/ notify : activated_template : null api_key : null authorization_template : null base_url : null password_reset_template : null registration_template : null # Configuration block for SMTP signup and password reset smtp : # Email address used for sender from_adr : null # Host of the SMTP server host : null # Password for the SMTP server password : null # Port of the SMTP server port : 587 # Should we communicate with SMTP server via TLS? tls : true # User to authenticate to the SMTP server user : null # Email patterns that will be allowed to # automatically signup for an account valid_email_patterns : - .* - .*@localhost","title":"Internal authenticator"},{"location":"fr/installation/configuration/authentication/#ldap-authentication","text":"The configuration block at auth.ldap allows you to easily add authentication via your LDAP server. The LDAP authentication module will be able to automatically assign roles, classification, avatar, name, and email address based on the properties of the LDAP user and the groups it is a member of. Here is an example configuration block to add to your configuration file that will allow you to connect to the docker-test-openldap server from: https://github.com/rroemhild/docker-test-openldap LDAP configuration example auth : internal : # Disable internal login, you could also leave it on if you want enabled : false ldap : # Should LDAP be enabled or not? enabled : true # DN of the group or the user who will get admin privileges admin_dn : cn=admin_staff,ou=people,dc=planetexpress,dc=com # Auto-create users if they are missing, this means # that if a user exists in LDAP, Assemblyline will create an # account for it upon the first login auto_create : true # Should we automatically sync roles, classification, avatar # email, name... with the LDAP server upon each login? auto_sync : true # Base DN for the users base : ou=people,dc=planetexpress,dc=com # Password used to query the LDAP server bind_pass : null # User use to query the LDAP server bind_user : null classification_mappings : {} # Name of the field containing the email address email_field : mail # How the group lookup is queried group_lookup_query : (&(objectClass=Group)(member=%s)) # Name of the field containing the user's avatar image_field : jpegPhoto # Type of image used to store the avatar image_format : jpeg # Name of the field containing the user's name name_field : cn # DN of the group or the user who will get signature_importer role signature_importer_dn : null # DN of the group or the user who will get signature_manager role signature_manager_dn : null # Field name for the UID uid_field : uid # URI to the LDAP server uri : ldaps://<ldap_ip_or_domain>:636","title":"LDAP Authentication"},{"location":"fr/installation/configuration/authentication/#oauth-authentication","text":"The configuration block at auth.oauth allows you to add OAuth authentication to your system. Assemblyline OAuth module is configurable enough to allow you to use almost any OAuth provider. It has been thoroughly tested with: Microsoft Accounts Google Accounts Auth0 Microsoft Azure Active Directory Accounts Here is an exhaustive configuration block that explains every single parameter from the OAuth configuration block: Exhaustive OAuth configuration example auth : internal : # Disable internal login, you could also leave it on if you want enabled : false oauth : # Should OAuth authentication be enabled or not enabled : true # Should we try to pull the user's avatar using gravatar gravatar_enabled : false # OAuth providers configuration block, you can have as many OAuth # providers as you want providers : # Name of the provider displayed in the UI local_provider : # Auto-create users if they are missing, this means # that if a user exists in the OAuth provider, Assemblyline # will create an account for it upon the first login # WARNING: If you set it to true for let's say Google's # OAuth provider, anyone with a google account # essentially has access to your system auto_create : true # Should we automatically sync roles, classification, avatar # email, name... with the OAuth provider upon each login? auto_sync : true # Automatic role and classification assignments auto_properties : # any user with a @localhost.local email will be given # TLP:Amber classification - field : email pattern : .*@localhost\\.local$ type : classification value : \"TLP:A\" # any user within the admins-sg will be made # administrator in the system - field : groups pattern : ^admins-sg$ type : role value : admin # URL used to get the access token access_token_url : https://oauth2.localhost/token # Base URL for downloading the user's and groups info api_base_url : https://openidconnect.localhost/ # URL used to authorize access to a resource authorize_url : https://localhost/oauth2/auth # ID of your application to authenticate to the OAuth # provider client_id : null # Password to your application to authenticate to the # OAuth provider client_secret : null # Keyword arguments passed to the different URLs # (to set the scope for example) client_kwargs : scope : openid email profile # URL used to verify if a returned JWKS token is valid jwks_uri : https://localhost/oauth2/certs # Name of the field that will contain the user ID uid_field : uid # Should we generate a random username for the # authenticated user? uid_randomize : false # How many digits should we add at the end of the username? uid_randomize_digits : 0 # What is the delimiter used by the random name generator? uid_randomize_delimiter : \"-\" # Reged used to parse and email address and capture parts # to create a user ID out of it uid_regex : ^(.*)@(\\w*).*$ # Format of the user ID based on the captured parts from the regex uid_format : '{}-{}' # Should we use the new callback method? use_new_callback_format : true # Path from the base_url to fetch the user info user_get : user/info # Path from the base to fetch the group info user_groups : group/info # Field return by the group info API call that contains the # list of groups user_groups_data_field : null # Name of the field in the list of groups that contains the # name of the group user_groups_name_field : null Here is an example configuration block that would let you use Auth0 if you would change your client_id and client_secret and that you would change the tenant_name to yours: Auth0 configuration example auth : internal : # Disable internal login, you could also leave it on if you want enabled : false oauth : # Enable oAuth enabled : true # Setup the auto0 provider providers : auth0 : # It is safe to auto-create users here # because it is your OAuth tenant auto_create : true auto_sync : true # Put your client ID and secret here client_id : <YOUR_CLIENT_ID> client_secret : <YOUR_CLIENT_SECRET> client_kwargs : scope : openid email profile # Set your tenant's name in the following URLs access_token_url : https://<TENANT_NAME>.auth0.com/oauth/token api_base_url : https://<TENANT_NAME>.auth0.com/ authorize_url : https://<TENANT_NAME>.auth0.com/authorize jwks_uri : https://<TENANT_NAME>.auth0.com/.well-known/jwks.json user_get : userinfo","title":"OAuth Authentication"},{"location":"fr/installation/configuration/config_file/","text":"Configuration YAML file \u00b6 Assemblyline 4 configuration is done using a YAML file ( config.yml ) which is deployed to all containers when they are launched. Specification and defaults \u00b6 The full specification of the file is defined here . The Object Data Model (ODM) converts the python model to a YAML file which looks like the following by default: Default configuration file values auth : allow_2fa : true allow_apikeys : true allow_extended_apikeys : true allow_security_tokens : true internal : enabled : true failure_ttl : 60 max_failures : 5 password_requirements : lower : false min_length : 12 number : false special : false upper : false signup : enabled : false notify : activated_template : null api_key : null authorization_template : null base_url : null password_reset_template : null registration_template : null smtp : from_adr : null host : null password : null port : 587 tls : true user : null valid_email_patterns : - .* - .*@localhost ldap : admin_dn : null auto_create : true auto_sync : true base : ou=people,dc=assemblyline,dc=local bind_pass : null bind_user : null classification_mappings : {} email_field : mail enabled : false group_lookup_query : (&(objectClass=Group)(member=%s)) image_field : jpegPhoto image_format : jpeg name_field : cn signature_importer_dn : null signature_manager_dn : null uid_field : uid uri : ldap://localhost:389 oauth : enabled : false gravatar_enabled : true providers : auth0 : access_token_url : https://{TENANT}.auth0.com/oauth/token api_base_url : https://{TENANT}.auth0.com/ authorize_url : https://{TENANT}.auth0.com/authorize client_id : null client_kwargs : scope : openid email profile client_secret : null jwks_uri : https://{TENANT}.auth0.com/.well-known/jwks.json user_get : userinfo azure_ad : access_token_url : https://login.microsoftonline.com/common/oauth2/token api_base_url : https://login.microsoft.com/common/ authorize_url : https://login.microsoftonline.com/common/oauth2/authorize client_id : null client_kwargs : scope : openid email profile client_secret : null jwks_uri : https://login.microsoftonline.com/common/discovery/v2.0/keys user_get : openid/userinfo google : access_token_url : https://oauth2.googleapis.com/token api_base_url : https://openidconnect.googleapis.com/ authorize_url : https://accounts.google.com/o/oauth2/v2/auth client_id : null client_kwargs : scope : openid email profile client_secret : null jwks_uri : https://www.googleapis.com/oauth2/v3/certs user_get : v1/userinfo core : alerter : alert_ttl : 90 constant_alert_fields : - alert_id - file - ts default_group_field : file.sha256 delay : 300 filtering_group_fields : - file.name - status - priority non_filtering_group_fields : - file.md5 - file.sha1 - file.sha256 process_alert_message : assemblyline_core.alerter.processing.process_alert_message dispatcher : max_inflight : 1000 timeout : 900 expiry : batch_delete : false delay : 0 delete_storage : true sleep_time : 15 workers : 20 ingester : cache_dtl : 2 default_max_extracted : 100 default_max_supplementary : 100 default_resubmit_services : [] default_services : [] default_user : internal description_prefix : Bulk expire_after : 1296000 get_whitelist_verdict : assemblyline.common.signaturing.drop incomplete_expire_after_seconds : 3600 incomplete_stale_after_seconds : 1800 is_low_priority : assemblyline.common.null.always_false max_inflight : 500 sampling_at : critical : 500000 high : 1000000 low : 10000000 medium : 2000000 stale_after_seconds : 86400 whitelist : assemblyline.common.null.whitelist metrics : apm_server : server_url : null token : null elasticsearch : cold : 30 delete : 90 host_certificates : null hosts : null unit : d warm : 2 export_interval : 5 redis : &id001 host : 127.0.0.1 port : 6379 redis : nonpersistent : *id001 persistent : host : 127.0.0.1 port : 6380 scaler : service_defaults : backlog : 100 environment : - name : SERVICE_API_HOST value : http://service-server:5003 - name : AL_SERVICE_TASK_LIMIT value : inf growth : 60 min_instances : 0 shrink : 30 datasources : al : classpath : assemblyline.datasource.al.AL config : {} alert : classpath : assemblyline.datasource.alert.Alert config : {} datastore : hosts : - http://elastic:devpass@localhost ilm : days_until_archive : 15 enabled : false indexes : alert : &id002 cold : 15 delete : 30 unit : d warm : 5 error : *id002 file : *id002 result : *id002 submission : *id002 update_archive : false type : elasticsearch filestore : cache : - s3://al_storage_key:Ch@ngeTh!sPa33w0rd@localhost:9000?s3_bucket=al-cache&use_ssl=False storage : - s3://al_storage_key:Ch@ngeTh!sPa33w0rd@localhost:9000?s3_bucket=al-storage&use_ssl=False logging : export_interval : 5 heartbeat_file : /tmp/heartbeat log_as_json : true log_directory : /var/log/assemblyline/ log_level : INFO log_to_console : true log_to_file : false log_to_syslog : false syslog_host : localhost syslog_port : 514 services : allow_insecure_registry : false categories : - Antivirus - Dynamic Analysis - External - Extraction - Filtering - Networking - Static Analysis cpu_reservation : 0.25 default_timeout : 60 image_variables : {} min_service_workers : 0 preferred_update_channel : stable stages : - FILTER - EXTRACT - CORE - SECONDARY - POST submission : default_max_extracted : 500 default_max_supplementary : 500 dtl : 30 max_dtl : 0 max_extraction_depth : 6 max_file_size : 104857600 max_metadata_length : 4096 tag_types : attribution : - attribution.actor - attribution.campaign - attribution.exploit - attribution.implant - attribution.family - attribution.network - av.virus_name - file.config - technique.obfuscation behavior : - file.behavior ioc : - network.email.address - network.static.ip - network.static.domain - network.static.uri - network.dynamic.ip - network.dynamic.domain - network.dynamic.uri system : constants : assemblyline.common.constants organisation : ACME type : production ui : allow_malicious_hinting : false allow_raw_downloads : true allow_url_submissions : true audit : true banner : null banner_level : info debug : false download_encoding : cart email : null enforce_quota : true fqdn : localhost ingest_max_priority : 250 read_only : false read_only_offset : '' secret_key : This is the default flask secret key... you should change this! session_duration : 3600 statistics : alert : - al.attrib - al.av - al.behavior - al.domain - al.ip - al.yara - file.name - file.md5 - owner submission : - params.submitter tos : null tos_lockout : false tos_lockout_notify : null url_submission_headers : {} url_submission_proxies : {} validate_session_ip : true validate_session_useragent : true Layers of the configuration file \u00b6 The configuration file is built in layers: The ODM converts the python classes to the default values as shown above The default assemblyline helm chart values.yaml file changes certain of these values to adapt them to a Kubernetes deployment Your deployment's values.yaml file change the values to their final form Changing the configuration file \u00b6 If you want to change the config.yml file that will be deployed in the containers, it will have to be done through the configuration section found in the values.yml file of your deployment. Example Let's say that you would want to change the log level in the system to ERROR an up. First of you would edit the values.yaml file of your personal deployment to add the changes to the configuration section: ... configuration : logging : log_level : ERROR ... Then you would simply deploy that new values.yaml file using the helm upgrade command specific to your deployment: Cluster deployment update Appliance deployment update Exhaustive configuration file documentation \u00b6 All parameters of each configuration section will be thoroughly documented in their respective pages. Here are the links to the different section documentations: Authentication (auth:) Core components (core:) Data sources (datasources:) Database (datastore:) File storage (filestore:) Logging (logging:) Services (services:) Submission (submission:) System (system:) User Interface (ui:)","title":"Configuration YAML file"},{"location":"fr/installation/configuration/config_file/#configuration-yaml-file","text":"Assemblyline 4 configuration is done using a YAML file ( config.yml ) which is deployed to all containers when they are launched.","title":"Configuration YAML file"},{"location":"fr/installation/configuration/config_file/#specification-and-defaults","text":"The full specification of the file is defined here . The Object Data Model (ODM) converts the python model to a YAML file which looks like the following by default: Default configuration file values auth : allow_2fa : true allow_apikeys : true allow_extended_apikeys : true allow_security_tokens : true internal : enabled : true failure_ttl : 60 max_failures : 5 password_requirements : lower : false min_length : 12 number : false special : false upper : false signup : enabled : false notify : activated_template : null api_key : null authorization_template : null base_url : null password_reset_template : null registration_template : null smtp : from_adr : null host : null password : null port : 587 tls : true user : null valid_email_patterns : - .* - .*@localhost ldap : admin_dn : null auto_create : true auto_sync : true base : ou=people,dc=assemblyline,dc=local bind_pass : null bind_user : null classification_mappings : {} email_field : mail enabled : false group_lookup_query : (&(objectClass=Group)(member=%s)) image_field : jpegPhoto image_format : jpeg name_field : cn signature_importer_dn : null signature_manager_dn : null uid_field : uid uri : ldap://localhost:389 oauth : enabled : false gravatar_enabled : true providers : auth0 : access_token_url : https://{TENANT}.auth0.com/oauth/token api_base_url : https://{TENANT}.auth0.com/ authorize_url : https://{TENANT}.auth0.com/authorize client_id : null client_kwargs : scope : openid email profile client_secret : null jwks_uri : https://{TENANT}.auth0.com/.well-known/jwks.json user_get : userinfo azure_ad : access_token_url : https://login.microsoftonline.com/common/oauth2/token api_base_url : https://login.microsoft.com/common/ authorize_url : https://login.microsoftonline.com/common/oauth2/authorize client_id : null client_kwargs : scope : openid email profile client_secret : null jwks_uri : https://login.microsoftonline.com/common/discovery/v2.0/keys user_get : openid/userinfo google : access_token_url : https://oauth2.googleapis.com/token api_base_url : https://openidconnect.googleapis.com/ authorize_url : https://accounts.google.com/o/oauth2/v2/auth client_id : null client_kwargs : scope : openid email profile client_secret : null jwks_uri : https://www.googleapis.com/oauth2/v3/certs user_get : v1/userinfo core : alerter : alert_ttl : 90 constant_alert_fields : - alert_id - file - ts default_group_field : file.sha256 delay : 300 filtering_group_fields : - file.name - status - priority non_filtering_group_fields : - file.md5 - file.sha1 - file.sha256 process_alert_message : assemblyline_core.alerter.processing.process_alert_message dispatcher : max_inflight : 1000 timeout : 900 expiry : batch_delete : false delay : 0 delete_storage : true sleep_time : 15 workers : 20 ingester : cache_dtl : 2 default_max_extracted : 100 default_max_supplementary : 100 default_resubmit_services : [] default_services : [] default_user : internal description_prefix : Bulk expire_after : 1296000 get_whitelist_verdict : assemblyline.common.signaturing.drop incomplete_expire_after_seconds : 3600 incomplete_stale_after_seconds : 1800 is_low_priority : assemblyline.common.null.always_false max_inflight : 500 sampling_at : critical : 500000 high : 1000000 low : 10000000 medium : 2000000 stale_after_seconds : 86400 whitelist : assemblyline.common.null.whitelist metrics : apm_server : server_url : null token : null elasticsearch : cold : 30 delete : 90 host_certificates : null hosts : null unit : d warm : 2 export_interval : 5 redis : &id001 host : 127.0.0.1 port : 6379 redis : nonpersistent : *id001 persistent : host : 127.0.0.1 port : 6380 scaler : service_defaults : backlog : 100 environment : - name : SERVICE_API_HOST value : http://service-server:5003 - name : AL_SERVICE_TASK_LIMIT value : inf growth : 60 min_instances : 0 shrink : 30 datasources : al : classpath : assemblyline.datasource.al.AL config : {} alert : classpath : assemblyline.datasource.alert.Alert config : {} datastore : hosts : - http://elastic:devpass@localhost ilm : days_until_archive : 15 enabled : false indexes : alert : &id002 cold : 15 delete : 30 unit : d warm : 5 error : *id002 file : *id002 result : *id002 submission : *id002 update_archive : false type : elasticsearch filestore : cache : - s3://al_storage_key:Ch@ngeTh!sPa33w0rd@localhost:9000?s3_bucket=al-cache&use_ssl=False storage : - s3://al_storage_key:Ch@ngeTh!sPa33w0rd@localhost:9000?s3_bucket=al-storage&use_ssl=False logging : export_interval : 5 heartbeat_file : /tmp/heartbeat log_as_json : true log_directory : /var/log/assemblyline/ log_level : INFO log_to_console : true log_to_file : false log_to_syslog : false syslog_host : localhost syslog_port : 514 services : allow_insecure_registry : false categories : - Antivirus - Dynamic Analysis - External - Extraction - Filtering - Networking - Static Analysis cpu_reservation : 0.25 default_timeout : 60 image_variables : {} min_service_workers : 0 preferred_update_channel : stable stages : - FILTER - EXTRACT - CORE - SECONDARY - POST submission : default_max_extracted : 500 default_max_supplementary : 500 dtl : 30 max_dtl : 0 max_extraction_depth : 6 max_file_size : 104857600 max_metadata_length : 4096 tag_types : attribution : - attribution.actor - attribution.campaign - attribution.exploit - attribution.implant - attribution.family - attribution.network - av.virus_name - file.config - technique.obfuscation behavior : - file.behavior ioc : - network.email.address - network.static.ip - network.static.domain - network.static.uri - network.dynamic.ip - network.dynamic.domain - network.dynamic.uri system : constants : assemblyline.common.constants organisation : ACME type : production ui : allow_malicious_hinting : false allow_raw_downloads : true allow_url_submissions : true audit : true banner : null banner_level : info debug : false download_encoding : cart email : null enforce_quota : true fqdn : localhost ingest_max_priority : 250 read_only : false read_only_offset : '' secret_key : This is the default flask secret key... you should change this! session_duration : 3600 statistics : alert : - al.attrib - al.av - al.behavior - al.domain - al.ip - al.yara - file.name - file.md5 - owner submission : - params.submitter tos : null tos_lockout : false tos_lockout_notify : null url_submission_headers : {} url_submission_proxies : {} validate_session_ip : true validate_session_useragent : true","title":"Specification and defaults"},{"location":"fr/installation/configuration/config_file/#layers-of-the-configuration-file","text":"The configuration file is built in layers: The ODM converts the python classes to the default values as shown above The default assemblyline helm chart values.yaml file changes certain of these values to adapt them to a Kubernetes deployment Your deployment's values.yaml file change the values to their final form","title":"Layers of the configuration file"},{"location":"fr/installation/configuration/config_file/#changing-the-configuration-file","text":"If you want to change the config.yml file that will be deployed in the containers, it will have to be done through the configuration section found in the values.yml file of your deployment. Example Let's say that you would want to change the log level in the system to ERROR an up. First of you would edit the values.yaml file of your personal deployment to add the changes to the configuration section: ... configuration : logging : log_level : ERROR ... Then you would simply deploy that new values.yaml file using the helm upgrade command specific to your deployment: Cluster deployment update Appliance deployment update","title":"Changing the configuration file"},{"location":"fr/installation/configuration/config_file/#exhaustive-configuration-file-documentation","text":"All parameters of each configuration section will be thoroughly documented in their respective pages. Here are the links to the different section documentations: Authentication (auth:) Core components (core:) Data sources (datasources:) Database (datastore:) File storage (filestore:) Logging (logging:) Services (services:) Submission (submission:) System (system:) User Interface (ui:)","title":"Exhaustive configuration file documentation"},{"location":"fr/installation/configuration/core/","text":"Core component section \u00b6 The core components configuration section ( core: ) of the configuration file contains all the different parameters that you can change to modify the behavior of each core components. Default values for the core section ... core : alerter : alert_ttl : 90 constant_alert_fields : - alert_id - file - ts default_group_field : file.sha256 delay : 300 filtering_group_fields : - file.name - status - priority non_filtering_group_fields : - file.md5 - file.sha1 - file.sha256 process_alert_message : assemblyline_core.alerter.processing.process_alert_message dispatcher : max_inflight : 1000 timeout : 900 expiry : batch_delete : false delay : 0 delete_storage : true sleep_time : 15 workers : 20 ingester : cache_dtl : 2 default_max_extracted : 100 default_max_supplementary : 100 default_resubmit_services : [] default_services : [] default_user : internal description_prefix : Bulk expire_after : 1296000 get_whitelist_verdict : assemblyline.common.signaturing.drop incomplete_expire_after_seconds : 3600 incomplete_stale_after_seconds : 1800 is_low_priority : assemblyline.common.null.always_false max_inflight : 500 sampling_at : critical : 500000 high : 1000000 low : 10000000 medium : 2000000 stale_after_seconds : 86400 whitelist : assemblyline.common.null.whitelist metrics : apm_server : server_url : null token : null elasticsearch : cold : 30 delete : 90 host_certificates : null hosts : null unit : d warm : 2 export_interval : 5 redis : &id001 host : 127.0.0.1 port : 6379 redis : nonpersistent : *id001 persistent : host : 127.0.0.1 port : 6380 scaler : service_defaults : backlog : 100 environment : - name : SERVICE_API_HOST value : http://service-server:5003 - name : AL_SERVICE_TASK_LIMIT value : inf growth : 60 min_instances : 0 shrink : 30 ... Tip Refer to the changing the configuration file documentation for more detail on where and how to change the configuration of the system. Alerter \u00b6 The configuration block at core.alerter contains all the configuration parameters that the Assemblyline alerter component can take. Here is an example configuration block with inline comments about the purpose of every single parameters: Alerter configuration example core : alerter : # Time to live in days for alerts in the system alert_ttl : 90 # List of fields that should keep the same value in between normal and extended scan # NOTE: You should not have to change those ever, this might get removed in the future constant_alert_fields : - alert_id - file - ts # Default field to group alerts with in the UI default_group_field : file.sha256 # Delay applied to the alert UI to leave time to the extended scan to complete delay : 300 # List of fields allowed to be used for grouping that are present in every single alerts filtering_group_fields : - file.name - status - priority # List of fields allowed to be used for grouping that are present only in a subset of alerts non_filtering_group_fields : - file.md5 - file.sha1 - file.sha256 # Python class used to process alert messages # NOTE: This should not be changed unless you've built your own container with # extra packages with alert processing capability process_alert_message : assemblyline_core.alerter.processing.process_alert_message Dispatcher \u00b6 The configuration block at core.dispatcher contains all the configuration parameters that the Assemblyline dispatcher component can take. Here is an example configuration block with inline comments about the purpose of every single parameters: Dispatcher configuration example core : dispatcher : # Maximum amount of concurrent submission processing in the system max_inflight : 1000 # Use in earlier version of dispatcher (To be removed) timeout : 900 Expiry \u00b6 The configuration block at core.expiry contains all the configuration parameters that the Assemblyline expiry component can take. Here is an example configuration block with inline comments about the purpose of every single parameters: Expiry configuration example core : expiry : # Perform delete operation in batch when changing day instead of throughout the day # NOTE: Not deleting during the day will make the system more responsive during the day # but will significantly reduce performance when changing to a new day until # all delete operations are done. batch_delete : false # Delay in hours applied to the deletion schedule delay : 0 # Should data expiry operation get rid of files as well? delete_storage : true # Time to sleep (sec) in between runs when there is not data to delete sleep_time : 15 # Number of workers used to delete data workers : 20 Ingester \u00b6 The configuration block at core.ingester contains all the configuration parameters that the Assemblyline ingester component can take. Here is an example configuration block with inline comments about the purpose of every single parameters: Ingester configuration example core : ingester : # Number of days ingested files are valid in the ingester cache cache_dtl : 2 # Maximum number of extracted files for ingested submissions default_max_extracted : 100 # Maximum number of supplementary files for ingested submissions default_max_supplementary : 100 # UNUSED default_resubmit_services : [] default_services : [] default_user : internal description_prefix : Bulk # Seconds before a previously ingested file will be considered new again, # the file will then be processed as if never seen before. expire_after : 1296000 # Function import path for method to determine whitelisting. # Files selected by this function will be dropped. # See the default function for signature. get_whitelist_verdict : assemblyline.common.signaturing.drop # Special version of 'expire_after' applied when the previous run of a file had errors. incomplete_expire_after_seconds : 3600 # Special version of 'stale_after_seconds' applied when the previous run of a file had errors. incomplete_stale_after_seconds : 1800 # Function import path for method to determine low-priority filter. # Files selected by this function will be forced to low-priority. # See the default function for signature. is_low_priority : assemblyline.common.null.always_false # How many submissions should ingester try to submit concurrently. max_inflight : 500 # How long should X queue be before the sampling (randomly dropping files) starts. # At the given value sampling will start, and grow gradually more agressive until 3 times # the value given. At 3 times the value given the system will proccess as many files # as possible, and all others will be discarded. sampling_at : critical : 500000 high : 1000000 low : 10000000 medium : 2000000 # Seconds before a previously ingested file will be considered stale, # the file will be reprocessed but the priority will be modified depending # on the score from the previous run. stale_after_seconds : 86400 # File whitelist import path. Imported object will be passed to the 'get_whitelist_verdict' # function. The default verdict function will use this as a file whitelist. # See default value for sample. whitelist : assemblyline.common.null.whitelist Metrics \u00b6 The configuration block at core.metrics contains all the configuration parameters that the Assemblyline metrics gathering components can take. Here is an example configuration block with inline comments about the purpose of every single parameters: Metrics configuration example core : metrics : # APM specific settings apm_server : # URL to the APM server server_url : null # Token use to connect to the APM server token : null # Elasticsearch specific configuration elasticsearch : # Number of `unit` document spend time in ILM cold storage cold : 30 # Number of `unit` after which documents are deleted using ILM delete : 90 # Cert used to connect to Elastic host_certificates : null # List of Elatic hosts hosts : null # Time unit for ILM cold, warm and delete unit : d # Number of `unit` document spend time in ILM warm storage warm : 2 # Interval at which the metrics components export their data export_interval : 5 # Redis specific configuration redis : # Host of the redis server host : 127.0.0.1 # Port of the redis server port : 6379 Redis \u00b6 The configuration block at core.redis contains all the configuration parameters used by Assemblyline components to connect to redis. Here is an example configuration block with inline comments about the purpose of every single parameters: Redis configuration example core : redis : # Configuration of the non-persistent redis (use mainly for messaging) nonpersistent : # Host of the redis server host : 127.0.0.1 # Port of the redis server port : 6379 # Configuration of the persistent redis (use mainly for task queuing) persistent : # Host of the redis server host : 127.0.0.1 # Port of the redis server port : 6380 Scaler \u00b6 The configuration block at core.scaler contains all the configuration parameters that the Assemblyline scaler component can take. Here is an example configuration block with inline comments about the purpose of every single parameters: Scaler configuration example core : scaler : service_defaults : # How many files in a service queue are considered a backlog. # This weights how important scaling up a service is relative # to its queue. You probably don't want to change this. backlog : 100 # List of environment variables set for all services. # Usually used for deployment related environment variables. # Service specific environent variables can be set in the # service manifest or in the UI for a particular system. environment : - name : SERVICE_API_HOST value : http://service-server:5003 - name : AL_SERVICE_TASK_LIMIT value : inf # Roughly how many seconds to wait before a service # scales up to meet increased demand. growth : 60 # The minimum number of instances of every service that # should be kept running at all times. min_instances : 0 # Roughly how many seconds to wait before a service scales down when # instances are consistently idle. shrink : 30","title":"Core component section"},{"location":"fr/installation/configuration/core/#core-component-section","text":"The core components configuration section ( core: ) of the configuration file contains all the different parameters that you can change to modify the behavior of each core components. Default values for the core section ... core : alerter : alert_ttl : 90 constant_alert_fields : - alert_id - file - ts default_group_field : file.sha256 delay : 300 filtering_group_fields : - file.name - status - priority non_filtering_group_fields : - file.md5 - file.sha1 - file.sha256 process_alert_message : assemblyline_core.alerter.processing.process_alert_message dispatcher : max_inflight : 1000 timeout : 900 expiry : batch_delete : false delay : 0 delete_storage : true sleep_time : 15 workers : 20 ingester : cache_dtl : 2 default_max_extracted : 100 default_max_supplementary : 100 default_resubmit_services : [] default_services : [] default_user : internal description_prefix : Bulk expire_after : 1296000 get_whitelist_verdict : assemblyline.common.signaturing.drop incomplete_expire_after_seconds : 3600 incomplete_stale_after_seconds : 1800 is_low_priority : assemblyline.common.null.always_false max_inflight : 500 sampling_at : critical : 500000 high : 1000000 low : 10000000 medium : 2000000 stale_after_seconds : 86400 whitelist : assemblyline.common.null.whitelist metrics : apm_server : server_url : null token : null elasticsearch : cold : 30 delete : 90 host_certificates : null hosts : null unit : d warm : 2 export_interval : 5 redis : &id001 host : 127.0.0.1 port : 6379 redis : nonpersistent : *id001 persistent : host : 127.0.0.1 port : 6380 scaler : service_defaults : backlog : 100 environment : - name : SERVICE_API_HOST value : http://service-server:5003 - name : AL_SERVICE_TASK_LIMIT value : inf growth : 60 min_instances : 0 shrink : 30 ... Tip Refer to the changing the configuration file documentation for more detail on where and how to change the configuration of the system.","title":"Core component section"},{"location":"fr/installation/configuration/core/#alerter","text":"The configuration block at core.alerter contains all the configuration parameters that the Assemblyline alerter component can take. Here is an example configuration block with inline comments about the purpose of every single parameters: Alerter configuration example core : alerter : # Time to live in days for alerts in the system alert_ttl : 90 # List of fields that should keep the same value in between normal and extended scan # NOTE: You should not have to change those ever, this might get removed in the future constant_alert_fields : - alert_id - file - ts # Default field to group alerts with in the UI default_group_field : file.sha256 # Delay applied to the alert UI to leave time to the extended scan to complete delay : 300 # List of fields allowed to be used for grouping that are present in every single alerts filtering_group_fields : - file.name - status - priority # List of fields allowed to be used for grouping that are present only in a subset of alerts non_filtering_group_fields : - file.md5 - file.sha1 - file.sha256 # Python class used to process alert messages # NOTE: This should not be changed unless you've built your own container with # extra packages with alert processing capability process_alert_message : assemblyline_core.alerter.processing.process_alert_message","title":"Alerter"},{"location":"fr/installation/configuration/core/#dispatcher","text":"The configuration block at core.dispatcher contains all the configuration parameters that the Assemblyline dispatcher component can take. Here is an example configuration block with inline comments about the purpose of every single parameters: Dispatcher configuration example core : dispatcher : # Maximum amount of concurrent submission processing in the system max_inflight : 1000 # Use in earlier version of dispatcher (To be removed) timeout : 900","title":"Dispatcher"},{"location":"fr/installation/configuration/core/#expiry","text":"The configuration block at core.expiry contains all the configuration parameters that the Assemblyline expiry component can take. Here is an example configuration block with inline comments about the purpose of every single parameters: Expiry configuration example core : expiry : # Perform delete operation in batch when changing day instead of throughout the day # NOTE: Not deleting during the day will make the system more responsive during the day # but will significantly reduce performance when changing to a new day until # all delete operations are done. batch_delete : false # Delay in hours applied to the deletion schedule delay : 0 # Should data expiry operation get rid of files as well? delete_storage : true # Time to sleep (sec) in between runs when there is not data to delete sleep_time : 15 # Number of workers used to delete data workers : 20","title":"Expiry"},{"location":"fr/installation/configuration/core/#ingester","text":"The configuration block at core.ingester contains all the configuration parameters that the Assemblyline ingester component can take. Here is an example configuration block with inline comments about the purpose of every single parameters: Ingester configuration example core : ingester : # Number of days ingested files are valid in the ingester cache cache_dtl : 2 # Maximum number of extracted files for ingested submissions default_max_extracted : 100 # Maximum number of supplementary files for ingested submissions default_max_supplementary : 100 # UNUSED default_resubmit_services : [] default_services : [] default_user : internal description_prefix : Bulk # Seconds before a previously ingested file will be considered new again, # the file will then be processed as if never seen before. expire_after : 1296000 # Function import path for method to determine whitelisting. # Files selected by this function will be dropped. # See the default function for signature. get_whitelist_verdict : assemblyline.common.signaturing.drop # Special version of 'expire_after' applied when the previous run of a file had errors. incomplete_expire_after_seconds : 3600 # Special version of 'stale_after_seconds' applied when the previous run of a file had errors. incomplete_stale_after_seconds : 1800 # Function import path for method to determine low-priority filter. # Files selected by this function will be forced to low-priority. # See the default function for signature. is_low_priority : assemblyline.common.null.always_false # How many submissions should ingester try to submit concurrently. max_inflight : 500 # How long should X queue be before the sampling (randomly dropping files) starts. # At the given value sampling will start, and grow gradually more agressive until 3 times # the value given. At 3 times the value given the system will proccess as many files # as possible, and all others will be discarded. sampling_at : critical : 500000 high : 1000000 low : 10000000 medium : 2000000 # Seconds before a previously ingested file will be considered stale, # the file will be reprocessed but the priority will be modified depending # on the score from the previous run. stale_after_seconds : 86400 # File whitelist import path. Imported object will be passed to the 'get_whitelist_verdict' # function. The default verdict function will use this as a file whitelist. # See default value for sample. whitelist : assemblyline.common.null.whitelist","title":"Ingester"},{"location":"fr/installation/configuration/core/#metrics","text":"The configuration block at core.metrics contains all the configuration parameters that the Assemblyline metrics gathering components can take. Here is an example configuration block with inline comments about the purpose of every single parameters: Metrics configuration example core : metrics : # APM specific settings apm_server : # URL to the APM server server_url : null # Token use to connect to the APM server token : null # Elasticsearch specific configuration elasticsearch : # Number of `unit` document spend time in ILM cold storage cold : 30 # Number of `unit` after which documents are deleted using ILM delete : 90 # Cert used to connect to Elastic host_certificates : null # List of Elatic hosts hosts : null # Time unit for ILM cold, warm and delete unit : d # Number of `unit` document spend time in ILM warm storage warm : 2 # Interval at which the metrics components export their data export_interval : 5 # Redis specific configuration redis : # Host of the redis server host : 127.0.0.1 # Port of the redis server port : 6379","title":"Metrics"},{"location":"fr/installation/configuration/core/#redis","text":"The configuration block at core.redis contains all the configuration parameters used by Assemblyline components to connect to redis. Here is an example configuration block with inline comments about the purpose of every single parameters: Redis configuration example core : redis : # Configuration of the non-persistent redis (use mainly for messaging) nonpersistent : # Host of the redis server host : 127.0.0.1 # Port of the redis server port : 6379 # Configuration of the persistent redis (use mainly for task queuing) persistent : # Host of the redis server host : 127.0.0.1 # Port of the redis server port : 6380","title":"Redis"},{"location":"fr/installation/configuration/core/#scaler","text":"The configuration block at core.scaler contains all the configuration parameters that the Assemblyline scaler component can take. Here is an example configuration block with inline comments about the purpose of every single parameters: Scaler configuration example core : scaler : service_defaults : # How many files in a service queue are considered a backlog. # This weights how important scaling up a service is relative # to its queue. You probably don't want to change this. backlog : 100 # List of environment variables set for all services. # Usually used for deployment related environment variables. # Service specific environent variables can be set in the # service manifest or in the UI for a particular system. environment : - name : SERVICE_API_HOST value : http://service-server:5003 - name : AL_SERVICE_TASK_LIMIT value : inf # Roughly how many seconds to wait before a service # scales up to meet increased demand. growth : 60 # The minimum number of instances of every service that # should be kept running at all times. min_instances : 0 # Roughly how many seconds to wait before a service scales down when # instances are consistently idle. shrink : 30","title":"Scaler"},{"location":"fr/installation/configuration/datasources/","text":"Data sources section \u00b6 The Data Sources configuration section ( datasources: ) of the configuration file contains all the different parameters that you can change to add/modify data sources used by the hash search API. Since this section is quite simple, we will list the default configuration at the same time as we describe the different values. Datasources section configuration example ... # The data source section is essentially a key/value pair of source and their configuration datasources : # Source name (al) al : # Path to the module that will process the query received by the hash search API classpath : assemblyline.datasource.al.AL # Dictionary holding the configuration for the module config : {} # Source name (alert) alert : # Path to the module that will process the query received by the hash search API classpath : assemblyline.datasource.alert.Alert # Dictionary holding the configuration for the module config : {} ... Tip Refer to the changing the configuration file documentation for more detail on where and how to change the configuration of the system.","title":"Data sources section"},{"location":"fr/installation/configuration/datasources/#data-sources-section","text":"The Data Sources configuration section ( datasources: ) of the configuration file contains all the different parameters that you can change to add/modify data sources used by the hash search API. Since this section is quite simple, we will list the default configuration at the same time as we describe the different values. Datasources section configuration example ... # The data source section is essentially a key/value pair of source and their configuration datasources : # Source name (al) al : # Path to the module that will process the query received by the hash search API classpath : assemblyline.datasource.al.AL # Dictionary holding the configuration for the module config : {} # Source name (alert) alert : # Path to the module that will process the query received by the hash search API classpath : assemblyline.datasource.alert.Alert # Dictionary holding the configuration for the module config : {} ... Tip Refer to the changing the configuration file documentation for more detail on where and how to change the configuration of the system.","title":"Data sources section"},{"location":"fr/installation/configuration/datastore/","text":"Database section \u00b6 The Database configuration section ( datastore: ) of the configuration file contains all the different parameters that you can change modify how to connect to the database and to modify the Index Lifecycle Management (ILM) parameters. Since this section is quite simple, we will list the default configuration at the same time as we describe the different values. Datastore section configuration example ... datastore : # List of elastic hosts to connect to hosts : - http://elastic:devpass@localhost # Index Lifecycle management configuration block ilm : # After how many days do documents go in the ILM managed indexes days_until_archive : 15 # Is ILM enabled or not? enabled : false # Index specific ILM configuration indexes : alert : # After how many `unit` documents goes in cold storage cold : 15 # After how many `unit` documents are deleted delete : 30 # Time unit definition for the current index unit : d # After how many `unit` documents goes in warm storage warm : 5 error : cold : 15 delete : 30 unit : d warm : 5 file : cold : 15 delete : 30 unit : d warm : 5 result : cold : 15 delete : 30 unit : d warm : 5 submission : cold : 15 delete : 30 unit : d warm : 5 # Show saving new document update it's archive counterpart # NOTE: Setting this to false makes it faster but it will be possible to have # duplicate documents update_archive : false # Type of datastore (only elasticsearch is supported so far, do not change) type : elasticsearch ... Tip Refer to the changing the configuration file documentation for more detail on where and how to change the configuration of the system.","title":"Database section"},{"location":"fr/installation/configuration/datastore/#database-section","text":"The Database configuration section ( datastore: ) of the configuration file contains all the different parameters that you can change modify how to connect to the database and to modify the Index Lifecycle Management (ILM) parameters. Since this section is quite simple, we will list the default configuration at the same time as we describe the different values. Datastore section configuration example ... datastore : # List of elastic hosts to connect to hosts : - http://elastic:devpass@localhost # Index Lifecycle management configuration block ilm : # After how many days do documents go in the ILM managed indexes days_until_archive : 15 # Is ILM enabled or not? enabled : false # Index specific ILM configuration indexes : alert : # After how many `unit` documents goes in cold storage cold : 15 # After how many `unit` documents are deleted delete : 30 # Time unit definition for the current index unit : d # After how many `unit` documents goes in warm storage warm : 5 error : cold : 15 delete : 30 unit : d warm : 5 file : cold : 15 delete : 30 unit : d warm : 5 result : cold : 15 delete : 30 unit : d warm : 5 submission : cold : 15 delete : 30 unit : d warm : 5 # Show saving new document update it's archive counterpart # NOTE: Setting this to false makes it faster but it will be possible to have # duplicate documents update_archive : false # Type of datastore (only elasticsearch is supported so far, do not change) type : elasticsearch ... Tip Refer to the changing the configuration file documentation for more detail on where and how to change the configuration of the system.","title":"Database section"},{"location":"fr/installation/configuration/filestore/","text":"File storage section \u00b6 The file storage configuration section ( filestore: ) of the configuration file contains URLs to the different filestores and cachestore used by Assemblyline. Since this section is quite simple, we will list the default configuration at the same time as we describe the different values. Filestore section configuration example ... # Assemblyline uses a multistage file storing system. When multiple filestores are defined for # a single type, Assemblyline will save to all levels at once when adding files but when # retrieving file will try one level at the time in order until it finds the file. # # This allows you to have different retention schedule on the different levels and have faster # filestore store only files that are currently scanning in the system but slower ones to keep # more files but to look them up less often. filestore : # List of URLs to connect to the cache filestore cache : - s3://al_storage_key:Ch@ngeTh!sPa33w0rd@localhost:9000?s3_bucket=al-cache&use_ssl=False # List of URLs to connect to the data filestore storage : - s3://al_storage_key:Ch@ngeTh!sPa33w0rd@localhost:9000?s3_bucket=al-storage&use_ssl=False ... Tip Refer to the changing the configuration file documentation for more detail on where and how to change the configuration of the system.","title":"File storage section"},{"location":"fr/installation/configuration/filestore/#file-storage-section","text":"The file storage configuration section ( filestore: ) of the configuration file contains URLs to the different filestores and cachestore used by Assemblyline. Since this section is quite simple, we will list the default configuration at the same time as we describe the different values. Filestore section configuration example ... # Assemblyline uses a multistage file storing system. When multiple filestores are defined for # a single type, Assemblyline will save to all levels at once when adding files but when # retrieving file will try one level at the time in order until it finds the file. # # This allows you to have different retention schedule on the different levels and have faster # filestore store only files that are currently scanning in the system but slower ones to keep # more files but to look them up less often. filestore : # List of URLs to connect to the cache filestore cache : - s3://al_storage_key:Ch@ngeTh!sPa33w0rd@localhost:9000?s3_bucket=al-cache&use_ssl=False # List of URLs to connect to the data filestore storage : - s3://al_storage_key:Ch@ngeTh!sPa33w0rd@localhost:9000?s3_bucket=al-storage&use_ssl=False ... Tip Refer to the changing the configuration file documentation for more detail on where and how to change the configuration of the system.","title":"File storage section"},{"location":"fr/installation/configuration/logging/","text":"Logging section \u00b6 The logging configuration section ( logging: ) of the configuration file allows you to modify the log level and where the logs will be shipped in the system . Since this section is quite simple, we will list the default configuration at the same time as we describe the different values. Logging section configuration example ... logging : # Interval at which the container heartbeat is written export_interval : 5 # Location of the container heartbeat heartbeat_file : /tmp/heartbeat # Should logs use a JSON format # (mainly used to parse logs into kibana, otherwise set to false to make them readable) log_as_json : true # Location on disk where the logs are stored if log_to_file enabled log_directory : /var/log/assemblyline/ # Minimum log level # (DEBUG, INFO, WARNING, ERROR) log_level : INFO # Should logs be shown in the console? # You should have that to true if running inside containers log_to_console : true # Should you write logs to files? # Set this to false when running inside a container log_to_file : false # Should you send logs to a syslog server? log_to_syslog : false # Host of the syslog server syslog_host : localhost # Port of the syslog server syslog_port : 514 ... Tip Refer to the changing the configuration file documentation for more detail on where and how to change the configuration of the system.","title":"Logging section"},{"location":"fr/installation/configuration/logging/#logging-section","text":"The logging configuration section ( logging: ) of the configuration file allows you to modify the log level and where the logs will be shipped in the system . Since this section is quite simple, we will list the default configuration at the same time as we describe the different values. Logging section configuration example ... logging : # Interval at which the container heartbeat is written export_interval : 5 # Location of the container heartbeat heartbeat_file : /tmp/heartbeat # Should logs use a JSON format # (mainly used to parse logs into kibana, otherwise set to false to make them readable) log_as_json : true # Location on disk where the logs are stored if log_to_file enabled log_directory : /var/log/assemblyline/ # Minimum log level # (DEBUG, INFO, WARNING, ERROR) log_level : INFO # Should logs be shown in the console? # You should have that to true if running inside containers log_to_console : true # Should you write logs to files? # Set this to false when running inside a container log_to_file : false # Should you send logs to a syslog server? log_to_syslog : false # Host of the syslog server syslog_host : localhost # Port of the syslog server syslog_port : 514 ... Tip Refer to the changing the configuration file documentation for more detail on where and how to change the configuration of the system.","title":"Logging section"},{"location":"fr/installation/configuration/services/","text":"Service section \u00b6 The service configuration section ( services: ) of the configuration file allows you to modify the parameters on how services are executed in the system. Since this section is quite simple, we will list the default configuration at the same time as we describe the different values. Service section configuration example ... services : # Are we allowed to pull service containers from insecure registries allow_insecure_registry : false # List of valid categories for services # You should not change this except to add a category maybe? categories : - Antivirus - Dynamic Analysis - External - Extraction - Filtering - Networking - Static Analysis # Percentage of CPU resevation scaler will do for each service (1 = 100%) cpu_reservation : 0.25 # Default service execution timeout default_timeout : 60 # Set of environment varables applied to the service containers # while loaded from updater or scaler image_variables : {} # Minimum amount of service that will be loaded for each service min_service_workers : 0 # Type of services the updater will be looking for when looking for service update # (dev or stable) preferred_update_channel : stable # List of available stages : - FILTER - EXTRACT - CORE - SECONDARY - POST ... Tip Refer to the changing the configuration file documentation for more detail on where and how to change the configuration of the system.","title":"Service section"},{"location":"fr/installation/configuration/services/#service-section","text":"The service configuration section ( services: ) of the configuration file allows you to modify the parameters on how services are executed in the system. Since this section is quite simple, we will list the default configuration at the same time as we describe the different values. Service section configuration example ... services : # Are we allowed to pull service containers from insecure registries allow_insecure_registry : false # List of valid categories for services # You should not change this except to add a category maybe? categories : - Antivirus - Dynamic Analysis - External - Extraction - Filtering - Networking - Static Analysis # Percentage of CPU resevation scaler will do for each service (1 = 100%) cpu_reservation : 0.25 # Default service execution timeout default_timeout : 60 # Set of environment varables applied to the service containers # while loaded from updater or scaler image_variables : {} # Minimum amount of service that will be loaded for each service min_service_workers : 0 # Type of services the updater will be looking for when looking for service update # (dev or stable) preferred_update_channel : stable # List of available stages : - FILTER - EXTRACT - CORE - SECONDARY - POST ... Tip Refer to the changing the configuration file documentation for more detail on where and how to change the configuration of the system.","title":"Service section"},{"location":"fr/installation/configuration/submission/","text":"Submission section \u00b6 The submission configuration section ( submission: ) of the configuration file allows you to modify the parameters off how submissions are handled in the system. Since this section is quite simple, we will list the default configuration at the same time as we describe the different values. Submission section configuration example ... submission : # Maximum amount of extracted files for a submission default_max_extracted : 500 # Maximum amount of supplementary files for a submission default_max_supplementary : 500 # Default amount of days submissions live in the system dtl : 30 # Maximum amount of days submissions live in the system max_dtl : 0 # Maximum extraction depth service can go max_extraction_depth : 6 # Maximum file size allowed in the system max_file_size : 104857600 # Maximum size of each metadata entry max_metadata_length : 4096 # Types of tags to be included in the submission summary in # the attribution, behavior and ioc sectiona. tag_types : attribution : - attribution.actor - attribution.campaign - attribution.exploit - attribution.implant - attribution.family - attribution.network - av.virus_name - file.config - technique.obfuscation behavior : - file.behavior ioc : - network.email.address - network.static.ip - network.static.domain - network.static.uri - network.dynamic.ip - network.dynamic.domain - network.dynamic.uri ... Tip Refer to the changing the configuration file documentation for more detail on where and how to change the configuration of the system.","title":"Submission section"},{"location":"fr/installation/configuration/submission/#submission-section","text":"The submission configuration section ( submission: ) of the configuration file allows you to modify the parameters off how submissions are handled in the system. Since this section is quite simple, we will list the default configuration at the same time as we describe the different values. Submission section configuration example ... submission : # Maximum amount of extracted files for a submission default_max_extracted : 500 # Maximum amount of supplementary files for a submission default_max_supplementary : 500 # Default amount of days submissions live in the system dtl : 30 # Maximum amount of days submissions live in the system max_dtl : 0 # Maximum extraction depth service can go max_extraction_depth : 6 # Maximum file size allowed in the system max_file_size : 104857600 # Maximum size of each metadata entry max_metadata_length : 4096 # Types of tags to be included in the submission summary in # the attribution, behavior and ioc sectiona. tag_types : attribution : - attribution.actor - attribution.campaign - attribution.exploit - attribution.implant - attribution.family - attribution.network - av.virus_name - file.config - technique.obfuscation behavior : - file.behavior ioc : - network.email.address - network.static.ip - network.static.domain - network.static.uri - network.dynamic.ip - network.dynamic.domain - network.dynamic.uri ... Tip Refer to the changing the configuration file documentation for more detail on where and how to change the configuration of the system.","title":"Submission section"},{"location":"fr/installation/configuration/system/","text":"System section \u00b6 The system configuration section ( system: ) of the configuration file allows you to modify the parameters of your system. Since this section is quite simple, we will list the default configuration at the same time as we describe the different values. System section configuration example ... system : # Path path to the constants module constants : assemblyline.common.constants # Organisation Name organisation : ACME # Type of system. If different then production, watermark will be shown in the UI type : production ... Tip Refer to the changing the configuration file documentation for more detail on where and how to change the configuration of the system.","title":"System section"},{"location":"fr/installation/configuration/system/#system-section","text":"The system configuration section ( system: ) of the configuration file allows you to modify the parameters of your system. Since this section is quite simple, we will list the default configuration at the same time as we describe the different values. System section configuration example ... system : # Path path to the constants module constants : assemblyline.common.constants # Organisation Name organisation : ACME # Type of system. If different then production, watermark will be shown in the UI type : production ... Tip Refer to the changing the configuration file documentation for more detail on where and how to change the configuration of the system.","title":"System section"},{"location":"fr/installation/configuration/ui/","text":"User Interface section \u00b6 The user interface configuration section ( ui: ) of the configuration file allows you to modify the parameters of the user interface and API Server. Since this section is quite simple, we will list the default configuration at the same time as we describe the different values. User interface section configuration example ... ui : # Show the malicious hinting checkbox when submitting files allow_malicious_hinting : false # Allow malicious files to be download in raw format allow_raw_downloads : true # Allow URL submissions to be processed in the system allow_url_submissions : true # Audit API queries audit : true # String to be displayed in the banner banner : null # Color of the banner (info, success, error, warning) banner_level : info # Turn on/off debug mode debug : false # Default encoding for downloaded files download_encoding : cart # Email address users can reach the admins at email : null # Enforce API and submissions quotas or not enforce_quota : true # Domain for your deployment (Especially important for kubernetes deployments) fqdn : localhost # Maximum submission priority for ingestion tasks ingest_max_priority : 250 # Make the UI read only # (Not supported in the new UI yet) read_only : false # Time offset for queries done in raed only mode # (Not supported in the new UI yet) read_only_offset : '' # Secret key for your flask app (API) # You should definitely change this! secret_key : This is the default flask secret key... you should change this! # Timeout after which a stale session is no longer valid session_duration : 3600 # Fields to generate statistics on statistics : # Statistics in the alert view alert : - al.attrib - al.av - al.behavior - al.domain - al.ip - al.yara - file.name - file.md5 - owner # Statistics in the submission view submission : - params.submitter # Terms of service for the deployment (in markdown format) tos : null # Lockout the user after they agree to the terms of service # (requires an admin to enable their account) tos_lockout : false # List of email addresses to notify when a user agreed to the TOS # and its account is locked out tos_lockout_notify : null # Headers added to fetch the files during URL submissions url_submission_headers : {} # Proxy configuration to use while fetching the file during URL submissions url_submission_proxies : {} # Should we validate that the session comes from the same IP? validate_session_ip : true # Should we validate that the session uses the same user agent validate_session_useragent : true ... Tip Refer to the changing the configuration file documentation for more detail on where and how to change the configuration of the system.","title":"User Interface section"},{"location":"fr/installation/configuration/ui/#user-interface-section","text":"The user interface configuration section ( ui: ) of the configuration file allows you to modify the parameters of the user interface and API Server. Since this section is quite simple, we will list the default configuration at the same time as we describe the different values. User interface section configuration example ... ui : # Show the malicious hinting checkbox when submitting files allow_malicious_hinting : false # Allow malicious files to be download in raw format allow_raw_downloads : true # Allow URL submissions to be processed in the system allow_url_submissions : true # Audit API queries audit : true # String to be displayed in the banner banner : null # Color of the banner (info, success, error, warning) banner_level : info # Turn on/off debug mode debug : false # Default encoding for downloaded files download_encoding : cart # Email address users can reach the admins at email : null # Enforce API and submissions quotas or not enforce_quota : true # Domain for your deployment (Especially important for kubernetes deployments) fqdn : localhost # Maximum submission priority for ingestion tasks ingest_max_priority : 250 # Make the UI read only # (Not supported in the new UI yet) read_only : false # Time offset for queries done in raed only mode # (Not supported in the new UI yet) read_only_offset : '' # Secret key for your flask app (API) # You should definitely change this! secret_key : This is the default flask secret key... you should change this! # Timeout after which a stale session is no longer valid session_duration : 3600 # Fields to generate statistics on statistics : # Statistics in the alert view alert : - al.attrib - al.av - al.behavior - al.domain - al.ip - al.yara - file.name - file.md5 - owner # Statistics in the submission view submission : - params.submitter # Terms of service for the deployment (in markdown format) tos : null # Lockout the user after they agree to the terms of service # (requires an admin to enable their account) tos_lockout : false # List of email addresses to notify when a user agreed to the TOS # and its account is locked out tos_lockout_notify : null # Headers added to fetch the files during URL submissions url_submission_headers : {} # Proxy configuration to use while fetching the file during URL submissions url_submission_proxies : {} # Should we validate that the session comes from the same IP? validate_session_ip : true # Should we validate that the session uses the same user agent validate_session_useragent : true ... Tip Refer to the changing the configuration file documentation for more detail on where and how to change the configuration of the system.","title":"User Interface section"},{"location":"fr/integration/ingestion_method/","text":"Choosing your ingestion method \u00b6 While integrating Assemblyline with other systems, the first thing you will need to do is to pick an ingestion method. Assemblyline gives you two options: Asynchronous (Using the Ingest API: /api/v4/ingest/ ) Synchronous (Using the Submit API: /api/v4/submit/ ) We will give you here a rundown of the different particularities of each method so you can pick the one that fits your needs the best. Asynchronous ingestion \u00b6 This is the preferred ingestion method for use with Assemblyline. In this mode, Assemblyline will queue your submission based on priority and will process them when the services have empty processing cycles. For each submission in this mode, you will get assigned an ingestion ID and you can be notified via a completion queue when your file has completed scanning. Alternatively, you can use the alerting page in the Assemblyline UI if you want to only view asynchronous submissions that Assemblyline deems highly suspicious. The asynchronous model was built to sustain a very large sample set of files and to help analysts focus on what is important. Benefits and Drawbacks \u00b6 Benefits Support very large volume of files Not subjected to any quota Will resort to data sampling if it gets overwhelmed with too many files Allows for alerting perspective to be used Does submission level caching if the same file is submitted twice with the same parameters Drawbacks Submissions may sit in the queue a long time if the system is very busy Submissions may be skipped if the system is overwhelmed Metadata is not searchable for all submissions since the system does not create a submission entry for cache submissions Typical use cases \u00b6 Here are the typical use cases that users encounter while using the asynchronous submission mode in the system. Using the Ingest API while reading a message from the notification queue The user submits all its files and receives ingestion IDs for its files API: /api/v4/ingest/ The user asks the notification for messages until it receives a confirmation message for all its files API: /api/v4/ingest/get_message_list/ This is how this works in the backend: Using the Ingest API ignoring the notification queue but using the alert perspective The user submits all its files and ignores the returned ingestion IDs API: /api/v4/ingest/ The user then monitors the UI alerting perspective for newly created alerts UI: /alerts This is how this works in the backend: Synchronous ingestion \u00b6 In this mode, Assemblyline will start the scanning of your file right away and will return you the ID of your submission. You will be able to use this ID to ask the system if the submission is complete and to pull the results when all the services are done reporting results for that submission. This is more suited for a very small volume of files and manual analysis. Files submitted via the User interface are using the synchronous mode. Benefits and Drawbacks \u00b6 Benefits Instant scanning Higher priority than asynchronous Submission guaranteed to be processed Metadata searchable for all submissions Drawbacks Subjected to quota (Default: 5 concurrent submissions) Not suited for high load No submission level caching Alerting not available Typical use cases \u00b6 Here are the typical use cases that user's encounter while using the synchronous submission mode in the system. Using the Submit API waiting for the submission to be done The user sends its file for processing and receives an ID for its submission API: /api/v4/submit/ The user queries the is completed API until the system says the submission is completed API: /api/v4/submission/is_completed/ / The user pulls the results for the submission API: /api/v4/submission/full/ / This is how this works in the backend:","title":"Choosing your ingestion method"},{"location":"fr/integration/ingestion_method/#choosing-your-ingestion-method","text":"While integrating Assemblyline with other systems, the first thing you will need to do is to pick an ingestion method. Assemblyline gives you two options: Asynchronous (Using the Ingest API: /api/v4/ingest/ ) Synchronous (Using the Submit API: /api/v4/submit/ ) We will give you here a rundown of the different particularities of each method so you can pick the one that fits your needs the best.","title":"Choosing your ingestion method"},{"location":"fr/integration/ingestion_method/#asynchronous-ingestion","text":"This is the preferred ingestion method for use with Assemblyline. In this mode, Assemblyline will queue your submission based on priority and will process them when the services have empty processing cycles. For each submission in this mode, you will get assigned an ingestion ID and you can be notified via a completion queue when your file has completed scanning. Alternatively, you can use the alerting page in the Assemblyline UI if you want to only view asynchronous submissions that Assemblyline deems highly suspicious. The asynchronous model was built to sustain a very large sample set of files and to help analysts focus on what is important.","title":"Asynchronous ingestion"},{"location":"fr/integration/ingestion_method/#benefits-and-drawbacks","text":"Benefits Support very large volume of files Not subjected to any quota Will resort to data sampling if it gets overwhelmed with too many files Allows for alerting perspective to be used Does submission level caching if the same file is submitted twice with the same parameters Drawbacks Submissions may sit in the queue a long time if the system is very busy Submissions may be skipped if the system is overwhelmed Metadata is not searchable for all submissions since the system does not create a submission entry for cache submissions","title":"Benefits and Drawbacks"},{"location":"fr/integration/ingestion_method/#typical-use-cases","text":"Here are the typical use cases that users encounter while using the asynchronous submission mode in the system. Using the Ingest API while reading a message from the notification queue The user submits all its files and receives ingestion IDs for its files API: /api/v4/ingest/ The user asks the notification for messages until it receives a confirmation message for all its files API: /api/v4/ingest/get_message_list/ This is how this works in the backend: Using the Ingest API ignoring the notification queue but using the alert perspective The user submits all its files and ignores the returned ingestion IDs API: /api/v4/ingest/ The user then monitors the UI alerting perspective for newly created alerts UI: /alerts This is how this works in the backend:","title":"Typical use cases"},{"location":"fr/integration/ingestion_method/#synchronous-ingestion","text":"In this mode, Assemblyline will start the scanning of your file right away and will return you the ID of your submission. You will be able to use this ID to ask the system if the submission is complete and to pull the results when all the services are done reporting results for that submission. This is more suited for a very small volume of files and manual analysis. Files submitted via the User interface are using the synchronous mode.","title":"Synchronous ingestion"},{"location":"fr/integration/ingestion_method/#benefits-and-drawbacks_1","text":"Benefits Instant scanning Higher priority than asynchronous Submission guaranteed to be processed Metadata searchable for all submissions Drawbacks Subjected to quota (Default: 5 concurrent submissions) Not suited for high load No submission level caching Alerting not available","title":"Benefits and Drawbacks"},{"location":"fr/integration/ingestion_method/#typical-use-cases_1","text":"Here are the typical use cases that user's encounter while using the synchronous submission mode in the system. Using the Submit API waiting for the submission to be done The user sends its file for processing and receives an ID for its submission API: /api/v4/submit/ The user queries the is completed API until the system says the submission is completed API: /api/v4/submission/is_completed/ / The user pulls the results for the submission API: /api/v4/submission/full/ / This is how this works in the backend:","title":"Typical use cases"},{"location":"fr/integration/java/","text":"Java Client \u00b6 The assemblyline java client library provides methods to submit requests to assemblyline. Get the Java client Using the client \u00b6 To instantiate the client bean set the application properties associated with the desired authentication method. The client can be accessed by auto-wiring the bean into the class using it. There are two authentication methods: username/apikey or username/password. API Key Authentication \u00b6 To instantiate an API key authenticated assemblyline client, define the following properties: assemblyline-java-client: url: <assemblyline-instance-url> api-auth: apikey: <api-key> username: <username> Password Authentication \u00b6 To instantiate a password authenticated assemblyline client, define the following properties: assemblyline-java-client: url: <assemblyline-instance-url> password-auth: password: <password> username: <username> Proxy \u00b6 To go through a proxy, add the following properties: assemblyline-java-client: proxy: host: <host> port: <port>","title":"Java Client"},{"location":"fr/integration/java/#java-client","text":"The assemblyline java client library provides methods to submit requests to assemblyline. Get the Java client","title":"Java Client"},{"location":"fr/integration/java/#using-the-client","text":"To instantiate the client bean set the application properties associated with the desired authentication method. The client can be accessed by auto-wiring the bean into the class using it. There are two authentication methods: username/apikey or username/password.","title":"Using the client"},{"location":"fr/integration/java/#api-key-authentication","text":"To instantiate an API key authenticated assemblyline client, define the following properties: assemblyline-java-client: url: <assemblyline-instance-url> api-auth: apikey: <api-key> username: <username>","title":"API Key Authentication"},{"location":"fr/integration/java/#password-authentication","text":"To instantiate a password authenticated assemblyline client, define the following properties: assemblyline-java-client: url: <assemblyline-instance-url> password-auth: password: <password> username: <username>","title":"Password Authentication"},{"location":"fr/integration/java/#proxy","text":"To go through a proxy, add the following properties: assemblyline-java-client: proxy: host: <host> port: <port>","title":"Proxy"},{"location":"fr/integration/key_generation/","text":"Generating an API key \u00b6 While integrating Assemblyline to another system, you should not save your username and password into another app. Instead, you should create an API Key with only the appropriate requirements for that specific integration. Here is how to do this: Login to Assemblyline's user interface with the user that will perform API requests Click on your avatar in the top-right corner of the Assemblyline UI and select \"Manage Account\" Scroll down to the bottom to the \"Security\" section and select \"Manage API Keys\" Add the API Key name, select access privileges then click the \"Add\" button. The API KEY will only be displayed once and can't be recovered. Copy it somewhere safe so that you can use it later. Click the \"Done\" button.","title":"Generating an API key"},{"location":"fr/integration/key_generation/#generating-an-api-key","text":"While integrating Assemblyline to another system, you should not save your username and password into another app. Instead, you should create an API Key with only the appropriate requirements for that specific integration. Here is how to do this: Login to Assemblyline's user interface with the user that will perform API requests Click on your avatar in the top-right corner of the Assemblyline UI and select \"Manage Account\" Scroll down to the bottom to the \"Security\" section and select \"Manage API Keys\" Add the API Key name, select access privileges then click the \"Add\" button. The API KEY will only be displayed once and can't be recovered. Copy it somewhere safe so that you can use it later. Click the \"Done\" button.","title":"Generating an API key"},{"location":"fr/integration/python/","text":"Python Client \u00b6 The Assemblyline python client facilitates issuing requests to Assemblyline. Installing the client \u00b6 pip install assemblyline_client Connecting to Assemblyline \u00b6 You can instantiate the client by using the following snippet of Python code: When connecting to Assemblyline, you can also provide a certificate for SSL or ignore the certificate error al_client = get_client ( ... , verify = '/path/to/server.crt' ) al_client = get_client ( ... , verify = False ) API Key You will need an API key . from assemblyline_client import get_client al_client = get_client ( \"https://yourdomain:443\" , apikey = ( 'user' , 'key' )) User/Password from assemblyline_client import get_client al_client = get_client ( \"https://yourdomain:443\" , auth = ( 'user' , 'password' )) Certificate from assemblyline_client import get_client # and if your Assemblyline server is using a self-signed certificate al_client = get_client ( \"https://yourdomain:443\" , cert = '/path/to/cert/file.pem' ) The client is fully documented in the docstrings so that you can use the 'help' feature of IPython or Jupyter Notebook al_client . search . alert ? Signature : al_client . search . alert ( query , filters = None , fl = None , offset = 0 , rows = 25 , sort = None , timeout = None , ) Docstring : Search alerts with a Lucene query . Required : query : Lucene query . ( string ) Optional : filters : Additional Lucene queries used to filter the data ( list of strings ) fl : List of fields to return ( comma separated string of fields ) offset : Offset at which the query items should start ( integer ) rows : Number of records to return ( integer ) sort : Field used for sorting with direction ( string : ex . 'id desc' ) timeout : Max number of milliseconds the query will run ( integer ) Returns all results . File : / usr / local / lib / python3 .7 / site - packages / assemblyline_client / v4_client / module / search / __init__ . py Type : method Examples \u00b6 Submit a file or URL for analysis \u00b6 There are two methods for sending a file/URL to Assemblyline for analysis: Ingest and Submit . In most cases, you want to use the Ingest API via the CLI Ingest Provides a fast, non-blocking method of submitting many files Ingest results will typically be analyzed by using a callback (if you need to look at all results) or by monitoring the alerts Supports alert generation Submit High priority, low volume (5 concurrent submissions by default, this can be increased slightly in the user settings) You will need to wait for analysis to complete before submitting more Useful to support manual analysis (Optional) Customizing your submission Note: Service names are case-sensitive # Submission parameters (works for both Ingest and Submit) settings = { 'classification' : 'TLP:A' , # classification 'description' : 'Hello world' , # file description 'name' : 'filename' , # file name 'deep_scan' : False , # activate deep scan mode 'priority' : 1000 , # queue priority (the higher the number, the higher the priority) 'ignore_cache' : False , # ignore system cache 'services' : { 'selected' : [ # selected service list (override user profile) 'Cuckoo' , 'Extract' ], 'resubmit' : [], # resubmit to these services if file initially scores > 500 'excluded' : [], # exclude these services }, 'service_spec' : { # provide a service parameter 'Extract' : { 'password' : 'password' } } } # Adding metadata (such as the source of the files or anything you want!) my_meta = { 'my_metadata' : 'value' , # any metadata of your liking 'my_metadata2' : 'value2' # any metadata of your liking } You can find all parameters and their default values in the SubmissionParams class . For submitting a URL instead of a file, use the url argument instead of path Ingest The Ingest API supports three additional functionalities over the Submit API: The ingest API is for high throughput submission (feeding the system) By passing the argument alert=True , the system will generate an alert if the score is over 500 By passing the argument nq='notification_queue_name' , you can use the client to poll a notification queue for a message indicating if the analysis has completed If you don't need to know about when the analysis completes, then you can omit the nq argument and ignore the subsequent code that interacts with the notification queue ingest_id = al_client . ingest ( path = '/pathto/file.txt' , nq = 'my_queue_name' , params = settings , metadata = my_meta ) # If you use a notification queue you can get your asynchronous results with: from time import sleep message = None while True : message = al_client . ingest . get_message ( \"my_queue_name\" ) if message is None : sleep ( 1 ) # Poll every second else : do something ... Submit submit_results = al_client . submit ( path = '/pathto/file.txt' , fname = 'fname' , params = settings , metadata = my_meta ) Submission details \u00b6 To get the details about a submission, you simply need to pass the client a submission ID (sid) submission_details = al_client . submission ( \"4nxrpBePQDLH427aA8m3TZ\" ) Using search \u00b6 More details about Search You can use the search engine in the client by simply passing a Lucene query. In the following example, we want to retrieve the first page of submissions made by user : search_result = al_client . search . submission ( \"params.submitter:user\" ) Using search iterator \u00b6 Instead of using search and getting a page of results, you can use the search iterator stream to go through all the results. Streamed results only return indexed fields. If you want the full result, you have to go get it via the client for submission in al_client . search . stream . submission ( \"params.submitter:user\" ): submission_id = submission [ \"sid\" ] full_submission = al_client . submission ( submission_id ) Using search parameters \u00b6 In the following example, we want to retrieve the first page of submissions that were submitted in the last week, and we only want the submission IDs: submission_results = al_client . search . submission ( 'times.submitted:[now-7d TO now]' , fl = 'sid' ) Using facet searching \u00b6 In the following example, we want to retrieve the users who have made submissions in the last week, and the number of submissions that they have made: submission_results = al_client . search . facet . submission ( 'params.submitter' , query = 'times.submitted:[now-7d TO now]' ) Using the Command-line Tool \u00b6 By installing the assemblyline_client PIP package, a command-line tool al-submit is installed. In case you don't want to use Python code to interface with the Assemblyline client, you can use this tool instead. You can view the user options via al-submit --help . (Optional) Configuration file example Rather than passing authentication and server details as parameters in a command-line, you can use a configuration file. This configuration file should be placed at ~/.al/submit.cfg . A template for this configuration file can be found below. NOTE: You can use = or : as the delimiter between key and value. [ auth ] # Username for the Assemblyline account. user = # There are three methods to authenticate a user account. Choose one: # - Password Provided via User Prompt # Leave the `password' configuration value below empty. # - Password Provided in Configuration File # Enter the password for the Assemblyline account in plaintext. password = # - API Key in Configuration File # Enter the API key to use in plaintext for the user to login. # NOTE: The API key must have WRITE access for INGEST and WRITE+READ for SUBMIT. apikey = # Skip server cert validation. # Value can be one of: true, false, yes, no # If not supplied, the default value is: false insecure = [ server ] # Method of network transport. # If not supplied, the default value is: https transport = # Domain of Assemblyline instance. # If not supplied, the default value is: localhost host = # Port to which traffic will be sent. # If not supplied, the default value is: 443 port = # Server cert used to connect to server. cert = Mass Submission Toolkit \u00b6 The Assemblyline Incident Manager can assist you with this process. One key consideration for a very large volume of files in a burst is the default sampling values . You must keep your ingestion flow at a rate such that the size of the priority ingestion queue remains lower than the corresponding priority queue sampling_at values, otherwise, Assemblyline will skip files.","title":"Python Client"},{"location":"fr/integration/python/#python-client","text":"The Assemblyline python client facilitates issuing requests to Assemblyline.","title":"Python Client"},{"location":"fr/integration/python/#installing-the-client","text":"pip install assemblyline_client","title":"Installing the client"},{"location":"fr/integration/python/#connecting-to-assemblyline","text":"You can instantiate the client by using the following snippet of Python code: When connecting to Assemblyline, you can also provide a certificate for SSL or ignore the certificate error al_client = get_client ( ... , verify = '/path/to/server.crt' ) al_client = get_client ( ... , verify = False ) API Key You will need an API key . from assemblyline_client import get_client al_client = get_client ( \"https://yourdomain:443\" , apikey = ( 'user' , 'key' )) User/Password from assemblyline_client import get_client al_client = get_client ( \"https://yourdomain:443\" , auth = ( 'user' , 'password' )) Certificate from assemblyline_client import get_client # and if your Assemblyline server is using a self-signed certificate al_client = get_client ( \"https://yourdomain:443\" , cert = '/path/to/cert/file.pem' ) The client is fully documented in the docstrings so that you can use the 'help' feature of IPython or Jupyter Notebook al_client . search . alert ? Signature : al_client . search . alert ( query , filters = None , fl = None , offset = 0 , rows = 25 , sort = None , timeout = None , ) Docstring : Search alerts with a Lucene query . Required : query : Lucene query . ( string ) Optional : filters : Additional Lucene queries used to filter the data ( list of strings ) fl : List of fields to return ( comma separated string of fields ) offset : Offset at which the query items should start ( integer ) rows : Number of records to return ( integer ) sort : Field used for sorting with direction ( string : ex . 'id desc' ) timeout : Max number of milliseconds the query will run ( integer ) Returns all results . File : / usr / local / lib / python3 .7 / site - packages / assemblyline_client / v4_client / module / search / __init__ . py Type : method","title":"Connecting to Assemblyline"},{"location":"fr/integration/python/#examples","text":"","title":"Examples"},{"location":"fr/integration/python/#submit-a-file-or-url-for-analysis","text":"There are two methods for sending a file/URL to Assemblyline for analysis: Ingest and Submit . In most cases, you want to use the Ingest API via the CLI Ingest Provides a fast, non-blocking method of submitting many files Ingest results will typically be analyzed by using a callback (if you need to look at all results) or by monitoring the alerts Supports alert generation Submit High priority, low volume (5 concurrent submissions by default, this can be increased slightly in the user settings) You will need to wait for analysis to complete before submitting more Useful to support manual analysis (Optional) Customizing your submission Note: Service names are case-sensitive # Submission parameters (works for both Ingest and Submit) settings = { 'classification' : 'TLP:A' , # classification 'description' : 'Hello world' , # file description 'name' : 'filename' , # file name 'deep_scan' : False , # activate deep scan mode 'priority' : 1000 , # queue priority (the higher the number, the higher the priority) 'ignore_cache' : False , # ignore system cache 'services' : { 'selected' : [ # selected service list (override user profile) 'Cuckoo' , 'Extract' ], 'resubmit' : [], # resubmit to these services if file initially scores > 500 'excluded' : [], # exclude these services }, 'service_spec' : { # provide a service parameter 'Extract' : { 'password' : 'password' } } } # Adding metadata (such as the source of the files or anything you want!) my_meta = { 'my_metadata' : 'value' , # any metadata of your liking 'my_metadata2' : 'value2' # any metadata of your liking } You can find all parameters and their default values in the SubmissionParams class . For submitting a URL instead of a file, use the url argument instead of path Ingest The Ingest API supports three additional functionalities over the Submit API: The ingest API is for high throughput submission (feeding the system) By passing the argument alert=True , the system will generate an alert if the score is over 500 By passing the argument nq='notification_queue_name' , you can use the client to poll a notification queue for a message indicating if the analysis has completed If you don't need to know about when the analysis completes, then you can omit the nq argument and ignore the subsequent code that interacts with the notification queue ingest_id = al_client . ingest ( path = '/pathto/file.txt' , nq = 'my_queue_name' , params = settings , metadata = my_meta ) # If you use a notification queue you can get your asynchronous results with: from time import sleep message = None while True : message = al_client . ingest . get_message ( \"my_queue_name\" ) if message is None : sleep ( 1 ) # Poll every second else : do something ... Submit submit_results = al_client . submit ( path = '/pathto/file.txt' , fname = 'fname' , params = settings , metadata = my_meta )","title":"Submit a file or URL for analysis"},{"location":"fr/integration/python/#submission-details","text":"To get the details about a submission, you simply need to pass the client a submission ID (sid) submission_details = al_client . submission ( \"4nxrpBePQDLH427aA8m3TZ\" )","title":"Submission details"},{"location":"fr/integration/python/#using-search","text":"More details about Search You can use the search engine in the client by simply passing a Lucene query. In the following example, we want to retrieve the first page of submissions made by user : search_result = al_client . search . submission ( \"params.submitter:user\" )","title":"Using search"},{"location":"fr/integration/python/#using-search-iterator","text":"Instead of using search and getting a page of results, you can use the search iterator stream to go through all the results. Streamed results only return indexed fields. If you want the full result, you have to go get it via the client for submission in al_client . search . stream . submission ( \"params.submitter:user\" ): submission_id = submission [ \"sid\" ] full_submission = al_client . submission ( submission_id )","title":"Using search iterator"},{"location":"fr/integration/python/#using-search-parameters","text":"In the following example, we want to retrieve the first page of submissions that were submitted in the last week, and we only want the submission IDs: submission_results = al_client . search . submission ( 'times.submitted:[now-7d TO now]' , fl = 'sid' )","title":"Using search parameters"},{"location":"fr/integration/python/#using-facet-searching","text":"In the following example, we want to retrieve the users who have made submissions in the last week, and the number of submissions that they have made: submission_results = al_client . search . facet . submission ( 'params.submitter' , query = 'times.submitted:[now-7d TO now]' )","title":"Using facet searching"},{"location":"fr/integration/python/#using-the-command-line-tool","text":"By installing the assemblyline_client PIP package, a command-line tool al-submit is installed. In case you don't want to use Python code to interface with the Assemblyline client, you can use this tool instead. You can view the user options via al-submit --help . (Optional) Configuration file example Rather than passing authentication and server details as parameters in a command-line, you can use a configuration file. This configuration file should be placed at ~/.al/submit.cfg . A template for this configuration file can be found below. NOTE: You can use = or : as the delimiter between key and value. [ auth ] # Username for the Assemblyline account. user = # There are three methods to authenticate a user account. Choose one: # - Password Provided via User Prompt # Leave the `password' configuration value below empty. # - Password Provided in Configuration File # Enter the password for the Assemblyline account in plaintext. password = # - API Key in Configuration File # Enter the API key to use in plaintext for the user to login. # NOTE: The API key must have WRITE access for INGEST and WRITE+READ for SUBMIT. apikey = # Skip server cert validation. # Value can be one of: true, false, yes, no # If not supplied, the default value is: false insecure = [ server ] # Method of network transport. # If not supplied, the default value is: https transport = # Domain of Assemblyline instance. # If not supplied, the default value is: localhost host = # Port to which traffic will be sent. # If not supplied, the default value is: 443 port = # Server cert used to connect to server. cert =","title":"Using the Command-line Tool"},{"location":"fr/integration/python/#mass-submission-toolkit","text":"The Assemblyline Incident Manager can assist you with this process. One key consideration for a very large volume of files in a burst is the default sampling values . You must keep your ingestion flow at a rate such that the size of the priority ingestion queue remains lower than the corresponding priority queue sampling_at values, otherwise, Assemblyline will skip files.","title":"Mass Submission Toolkit"},{"location":"fr/integration/rest/","text":"RESTful API \u00b6 When it is impossible to integrate your application using the dedicated python client, you can use Assemblyline's RESTful API to perform any task that you can think of. API documentation \u00b6 Each instance of Assemblyline comes with its internal API documentation which can be viewed by browsing to https://yourdomain/help/api Connecting to the API \u00b6 For easy integration, it is recommended that you generate an API key for the user who will perform RESTful queries. Otherwise, you will have to build yourself a library that will handle session cookies and XSRF tokens and you probably want something simpler. Using the API key \u00b6 To use your newly created API key you can simply add the X-USER and X-APIKEY headers to your request and the system will identify you with that key at each request instead of relying on a session cookie. Example Let's use a hypothetical API key to ask the system who we are. (Using the /api/v4/user/whoami/ API) CURL curl -X GET \"https://yourdomain/api/v4/user/whoami/\" \\ -H 'x-user: <your_user_id>' \\ -H 'x-apikey: <key_name:randomly_generated_password>' \\ -H 'accept: application/json' Javascript (fetch) fetch ( \"https://yourdomain/api/v4/user/whoami/\" , { \"headers\" : { \"accept\" : \"application/json\" , \"x-apikey\" : \"<key_name:randomly_generated_password>\" , \"x-user\" : \"<your_user_id>\" }, \"method\" : \"GET\" } ); Python (requests) import requests requests . get ( \"https://yourdomain/api/v4/user/whoami/\" , headers = { \"x-user\" : \"<your_user_id>\" , \"x-apikey\" : \"<key_name:randomly_generated_password>\" , \"accept\" : \"application/json\" } ) API Gotcha! \u00b6 Here is a list of the most common issues users are facing while using the API Wrong content type \u00b6 All Assemblyline APIs are built around receiving and returning JSON data. Do not forget to set your Content-Type and Accept headers to \"application/json\" or you might encounter some issues. Trailing forward slash \u00b6 All Assemblyline APIs end with a trailing forward slash \"/\" . Make sure that the API URL has it at the end of the URL otherwise you may get a \"Method not allowed\" error and you'll have issues figuring out why.","title":"RESTful API"},{"location":"fr/integration/rest/#restful-api","text":"When it is impossible to integrate your application using the dedicated python client, you can use Assemblyline's RESTful API to perform any task that you can think of.","title":"RESTful API"},{"location":"fr/integration/rest/#api-documentation","text":"Each instance of Assemblyline comes with its internal API documentation which can be viewed by browsing to https://yourdomain/help/api","title":"API documentation"},{"location":"fr/integration/rest/#connecting-to-the-api","text":"For easy integration, it is recommended that you generate an API key for the user who will perform RESTful queries. Otherwise, you will have to build yourself a library that will handle session cookies and XSRF tokens and you probably want something simpler.","title":"Connecting to the API"},{"location":"fr/integration/rest/#using-the-api-key","text":"To use your newly created API key you can simply add the X-USER and X-APIKEY headers to your request and the system will identify you with that key at each request instead of relying on a session cookie. Example Let's use a hypothetical API key to ask the system who we are. (Using the /api/v4/user/whoami/ API) CURL curl -X GET \"https://yourdomain/api/v4/user/whoami/\" \\ -H 'x-user: <your_user_id>' \\ -H 'x-apikey: <key_name:randomly_generated_password>' \\ -H 'accept: application/json' Javascript (fetch) fetch ( \"https://yourdomain/api/v4/user/whoami/\" , { \"headers\" : { \"accept\" : \"application/json\" , \"x-apikey\" : \"<key_name:randomly_generated_password>\" , \"x-user\" : \"<your_user_id>\" }, \"method\" : \"GET\" } ); Python (requests) import requests requests . get ( \"https://yourdomain/api/v4/user/whoami/\" , headers = { \"x-user\" : \"<your_user_id>\" , \"x-apikey\" : \"<key_name:randomly_generated_password>\" , \"accept\" : \"application/json\" } )","title":"Using the API key"},{"location":"fr/integration/rest/#api-gotcha","text":"Here is a list of the most common issues users are facing while using the API","title":"API Gotcha!"},{"location":"fr/integration/rest/#wrong-content-type","text":"All Assemblyline APIs are built around receiving and returning JSON data. Do not forget to set your Content-Type and Accept headers to \"application/json\" or you might encounter some issues.","title":"Wrong content type"},{"location":"fr/integration/rest/#trailing-forward-slash","text":"All Assemblyline APIs end with a trailing forward slash \"/\" . Make sure that the API URL has it at the end of the URL otherwise you may get a \"Method not allowed\" error and you'll have issues figuring out why.","title":"Trailing forward slash"},{"location":"fr/overview/community_services/","text":"Services de la communaut\u00e9 \u00b6 La communaut\u00e9 d'Assemblyline travaille fort pour am\u00e9liorer cet outil \u00e0 d\u00e9tect\u00e9 des fichiers malicieux. Cette page contient la liste de services cr\u00e9e et partag\u00e9 avec le publique. Attention Ces services ne sont pas g\u00e8rer par l'\u00e9quipe d'Assemblyline, nous vous invitons \u00e0 faire une revue de leur code pour vous assurez d'\u00eatre confortable avec ce qu'ils font avant de les utilis\u00e9s dans v\u00f4tre syst\u00e8me. Liste de services (anglais seulement) \u00b6 Nom de service Description Auteur Source AutoItRipper AutoIt unpacker service NVISO link Cape Assemblyline service build for CAPE's API NVISO link ClamAV Assemblyline service which submits a file to ClamAV and displays the result NVISO link DocumentPreview Document preview service NVISO link IntezerStatic Assemblyline service which fetchs the result of a specific sha256 intezer scan NVISO link MalwareBazaar Assemblyline service fetching Malware Bazaar report NVISO link MsgParser Simple MSG extractor AssemblyLine service NVISO link PythonExeUnpack Python exe unpacker service NVISO link StegFinder AssemblyLine service which scans for embedded data in image using StegExpose NVISO link Unfurl Assemblyline service parsing a submitted URL to unshorten it. NVISO link UrlScanIo URLScan.io AL service NVISO link WindowsDefender Windows defender service being adapted from an Assemblyline community conversation Adam McHugh link Faire nous part de vos services! \u00b6 Contactez-nous sur google groups pour faire ajout\u00e9 vos services a cette page.","title":"Services de la communaut\u00e9"},{"location":"fr/overview/community_services/#services-de-la-communaute","text":"La communaut\u00e9 d'Assemblyline travaille fort pour am\u00e9liorer cet outil \u00e0 d\u00e9tect\u00e9 des fichiers malicieux. Cette page contient la liste de services cr\u00e9e et partag\u00e9 avec le publique. Attention Ces services ne sont pas g\u00e8rer par l'\u00e9quipe d'Assemblyline, nous vous invitons \u00e0 faire une revue de leur code pour vous assurez d'\u00eatre confortable avec ce qu'ils font avant de les utilis\u00e9s dans v\u00f4tre syst\u00e8me.","title":"Services de la communaut\u00e9"},{"location":"fr/overview/community_services/#liste-de-services-anglais-seulement","text":"Nom de service Description Auteur Source AutoItRipper AutoIt unpacker service NVISO link Cape Assemblyline service build for CAPE's API NVISO link ClamAV Assemblyline service which submits a file to ClamAV and displays the result NVISO link DocumentPreview Document preview service NVISO link IntezerStatic Assemblyline service which fetchs the result of a specific sha256 intezer scan NVISO link MalwareBazaar Assemblyline service fetching Malware Bazaar report NVISO link MsgParser Simple MSG extractor AssemblyLine service NVISO link PythonExeUnpack Python exe unpacker service NVISO link StegFinder AssemblyLine service which scans for embedded data in image using StegExpose NVISO link Unfurl Assemblyline service parsing a submitted URL to unshorten it. NVISO link UrlScanIo URLScan.io AL service NVISO link WindowsDefender Windows defender service being adapted from an Assemblyline community conversation Adam McHugh link","title":"Liste de services (anglais seulement)"},{"location":"fr/overview/community_services/#faire-nous-part-de-vos-services","text":"Contactez-nous sur google groups pour faire ajout\u00e9 vos services a cette page.","title":"Faire nous part de vos services!"},{"location":"fr/overview/how_it_works/","text":"Comment \u00e7a fonctionne \u00b6 Assemblyline r\u00e9duit le nombre de fichiers anodins que les praticiens de TI doivent v\u00e9rifier chaque jour, ce qui leur permet de collaborer avec les autres utilisateurs \u00e0 la personnalisation et l\u2019am\u00e9lioration de la plateforme. A) Assemblyline fonctionne un peu comme une cha\u00eene de montage : les fichiers arrivent dans le syst\u00e8me et font l\u2019objet d\u2019un tri selon une certaine s\u00e9quence. B) Assemblyline g\u00e9n\u00e8re de l\u2019information sur chacun des fichiers, puis leur attribue un identifiant unique qui les suit alors qu\u2019ils sont achemin\u00e9s dans le syst\u00e8me. C) Les utilisateurs peuvent ajouter leurs propres outils d\u2019analyse, que l\u2019on appelle services. D) Les services s\u00e9lectionn\u00e9s par l\u2019utilisateur dans Assemblyline analysent alors les fichiers \u00e0 la recherche d\u2019indications de malveillance et en extraient les caract\u00e9ristiques \u00e0 des fins d\u2019analyse plus pouss\u00e9e. Lors de l\u2019analyse, le syst\u00e8me g\u00e9n\u00e8re des alertes concernant un fichier malveillant et lui accorde une note. Le syst\u00e8me peut \u00e9galement lancer l\u2019ex\u00e9cution de syst\u00e8mes de d\u00e9fense automatis\u00e9s. Il est d\u2019ailleurs possible de transmettre les indicateurs de malveillance g\u00e9n\u00e9r\u00e9s par le syst\u00e8me aux autres syst\u00e8mes de d\u00e9fense.","title":"Comment \u00e7a fonctionne"},{"location":"fr/overview/how_it_works/#comment-ca-fonctionne","text":"Assemblyline r\u00e9duit le nombre de fichiers anodins que les praticiens de TI doivent v\u00e9rifier chaque jour, ce qui leur permet de collaborer avec les autres utilisateurs \u00e0 la personnalisation et l\u2019am\u00e9lioration de la plateforme. A) Assemblyline fonctionne un peu comme une cha\u00eene de montage : les fichiers arrivent dans le syst\u00e8me et font l\u2019objet d\u2019un tri selon une certaine s\u00e9quence. B) Assemblyline g\u00e9n\u00e8re de l\u2019information sur chacun des fichiers, puis leur attribue un identifiant unique qui les suit alors qu\u2019ils sont achemin\u00e9s dans le syst\u00e8me. C) Les utilisateurs peuvent ajouter leurs propres outils d\u2019analyse, que l\u2019on appelle services. D) Les services s\u00e9lectionn\u00e9s par l\u2019utilisateur dans Assemblyline analysent alors les fichiers \u00e0 la recherche d\u2019indications de malveillance et en extraient les caract\u00e9ristiques \u00e0 des fins d\u2019analyse plus pouss\u00e9e. Lors de l\u2019analyse, le syst\u00e8me g\u00e9n\u00e8re des alertes concernant un fichier malveillant et lui accorde une note. Le syst\u00e8me peut \u00e9galement lancer l\u2019ex\u00e9cution de syst\u00e8mes de d\u00e9fense automatis\u00e9s. Il est d\u2019ailleurs possible de transmettre les indicateurs de malveillance g\u00e9n\u00e9r\u00e9s par le syst\u00e8me aux autres syst\u00e8mes de d\u00e9fense.","title":"Comment \u00e7a fonctionne"},{"location":"fr/overview/services/","text":"Services d'Assemblyline \u00b6 Les Services install\u00e9 sur un syst\u00e8me peuvent \u00eatre trouv\u00e9 sous: Help > Service Listing . Cette liste contient tous les services inclus et maintenue avec Assemblyline: Service Name Speciality Description Source APKaye Android APK APKs are decompiled and inspected. Network indicators and information found in the APK manifest file are displayed link Anti-virus Anti-virus Generic ICAP client to integrate with most Anti-virus enterprise scanners link Characterize Analyze d'entropy Calcule l'entropy des fichiers et extrait les meta-donn\u00e9e Exif. link ConfigExtractor Extraction Extrait la configuration de malware connu, pour trouv\u00e9 des liste de C2s, cle d'encryption etc. link Cuckoo Sandbox Int\u00e9gration avec la platform d'analyze dynamic Cuckoo link DeobfuScripter Deobfuscation D\u00e9obfuscation de script statique. link EmlParser Email Analyze de fichier email avec GOVCERT-LU eml_parser library comme les attachements, URL etc link Espresso Java Extrait les classes Java, d\u00e9compile et analyze pour comportement malicieux link Extract Compressed file Extrait la plus part des type de compression (like ZIP, RAR, 7z, ...) link Floss IoC extraction Extrait des cha\u00eene de charracters obfusqu\u00e9 avec FireEye Labs Obfuscated String Solver link FrankenStrings IoC extraction This service performs file and IOC extractions using pattern matching, simple encoding decoder and script de-obfuscators link IPArse Apple IOS Analyze de fichier Apple IOS link JSJaws Javascript Analyze de fichier Javascript link MetaDefender Anti-virus Integration avec MetaDefender (multi-engine anti-virus) link MetaPeek Meta data analysis D\u00e9tect les signe malicieux dans les meta-donn\u00e9es et les noms de fichier (double extension etc) link Oletools Office documents Ce service analyze les fichiers Office et extrait des indicateurs de compromis avec Python library py-oletools by Philippe Lagadec - http://www.decalage.info link Overpower PowerShell D\u00e9obfusque les fichier powershell link PDFId PDF Analyze de fichier PDF avec Didier Stevens PDFId & PDFParse link PEFile Windows binaries Analyze de fichier Windows (imports, exports, section names, ...) avec Python library pefile link PeePDF PDF Ce service utilise Python PeePDF library pour extraire de l'information de fichier PDF link PixAxe Images Extrait du text des images link Safelist Safelisting Permet de \"safelister\" des indicateur dans le syst\u00e8me comme des domaines, hash etc NSRL link Sigma Eventlog signatures Analyze de \"Windows Event logs\" avec les r\u00e8gles Sigma link Suricata Network signatures Analyze les captures r\u00e9seaux (pcap) avec Suricata link Swiffer Adobe Shockwave Analyze de fichier Shockwave (.swf) link TagCheck Tag signatures Signatures YARA sur les tags d'Assemblyline Tags link TorrentSlicer Torrent files Analyze de fichier torrent link Unpacker UPX Unpacker Extrait des executable a partir de fichier \"packer\" avec UPX link Unpac.me Unpacker Int\u00e9gration avec unpac.me link ViperMonkey Office documents ViperMonkey est un programme d'emulation pour VBA par http://www.decalage.info link VirusTotalDynamic Anti-virus Envoie des fichiers sur Virustotal et collect les r\u00e9sultats link VirusTotalStatic Anti-virus Enrichie les donn\u00e9es d'Assemblyline avec les donn\u00e9es de VirusTotal link XLMMacroDeobfuscator Office documents Analyse de Macro Excel 4.0 link YARA File signatures Signatures de fichier link","title":"Services d'Assemblyline"},{"location":"fr/overview/services/#services-dassemblyline","text":"Les Services install\u00e9 sur un syst\u00e8me peuvent \u00eatre trouv\u00e9 sous: Help > Service Listing . Cette liste contient tous les services inclus et maintenue avec Assemblyline: Service Name Speciality Description Source APKaye Android APK APKs are decompiled and inspected. Network indicators and information found in the APK manifest file are displayed link Anti-virus Anti-virus Generic ICAP client to integrate with most Anti-virus enterprise scanners link Characterize Analyze d'entropy Calcule l'entropy des fichiers et extrait les meta-donn\u00e9e Exif. link ConfigExtractor Extraction Extrait la configuration de malware connu, pour trouv\u00e9 des liste de C2s, cle d'encryption etc. link Cuckoo Sandbox Int\u00e9gration avec la platform d'analyze dynamic Cuckoo link DeobfuScripter Deobfuscation D\u00e9obfuscation de script statique. link EmlParser Email Analyze de fichier email avec GOVCERT-LU eml_parser library comme les attachements, URL etc link Espresso Java Extrait les classes Java, d\u00e9compile et analyze pour comportement malicieux link Extract Compressed file Extrait la plus part des type de compression (like ZIP, RAR, 7z, ...) link Floss IoC extraction Extrait des cha\u00eene de charracters obfusqu\u00e9 avec FireEye Labs Obfuscated String Solver link FrankenStrings IoC extraction This service performs file and IOC extractions using pattern matching, simple encoding decoder and script de-obfuscators link IPArse Apple IOS Analyze de fichier Apple IOS link JSJaws Javascript Analyze de fichier Javascript link MetaDefender Anti-virus Integration avec MetaDefender (multi-engine anti-virus) link MetaPeek Meta data analysis D\u00e9tect les signe malicieux dans les meta-donn\u00e9es et les noms de fichier (double extension etc) link Oletools Office documents Ce service analyze les fichiers Office et extrait des indicateurs de compromis avec Python library py-oletools by Philippe Lagadec - http://www.decalage.info link Overpower PowerShell D\u00e9obfusque les fichier powershell link PDFId PDF Analyze de fichier PDF avec Didier Stevens PDFId & PDFParse link PEFile Windows binaries Analyze de fichier Windows (imports, exports, section names, ...) avec Python library pefile link PeePDF PDF Ce service utilise Python PeePDF library pour extraire de l'information de fichier PDF link PixAxe Images Extrait du text des images link Safelist Safelisting Permet de \"safelister\" des indicateur dans le syst\u00e8me comme des domaines, hash etc NSRL link Sigma Eventlog signatures Analyze de \"Windows Event logs\" avec les r\u00e8gles Sigma link Suricata Network signatures Analyze les captures r\u00e9seaux (pcap) avec Suricata link Swiffer Adobe Shockwave Analyze de fichier Shockwave (.swf) link TagCheck Tag signatures Signatures YARA sur les tags d'Assemblyline Tags link TorrentSlicer Torrent files Analyze de fichier torrent link Unpacker UPX Unpacker Extrait des executable a partir de fichier \"packer\" avec UPX link Unpac.me Unpacker Int\u00e9gration avec unpac.me link ViperMonkey Office documents ViperMonkey est un programme d'emulation pour VBA par http://www.decalage.info link VirusTotalDynamic Anti-virus Envoie des fichiers sur Virustotal et collect les r\u00e9sultats link VirusTotalStatic Anti-virus Enrichie les donn\u00e9es d'Assemblyline avec les donn\u00e9es de VirusTotal link XLMMacroDeobfuscator Office documents Analyse de Macro Excel 4.0 link YARA File signatures Signatures de fichier link","title":"Services d'Assemblyline"},{"location":"fr/user_manual/results/","text":"R\u00e9sultats d\u2019Assemblyline \u00b6 Heuristiques \u00b6 Par heuristique, on entend une fonction qui peut \u00eatre d\u00e9tect\u00e9e par le service dans le cadre de l\u2019analyse. - Elle se compose de ce qui suit : - un identifiant; - un nom; - une description; - une note (utilis\u00e9e pour signaler une heuristique comme \u00e9tant MALICIOUS [MALVEILLANTE], SUSPICIOUS [SUSPECTE] ou INFO); - l\u2019ID de la matrice Mitre\u2019s Att&ck; - des signatures qui sont souvent utilis\u00e9es pour donner plus de contexte. Le syst\u00e8me fait le suivi des heuristiques pour fournir des statistiques sur le nombre d\u2019occurrences et les notes attribu\u00e9es afin d\u2019apporter des ajustements si les performances des heuristiques sont ad\u00e9quates ou insuffisantes. Les heuristiques s\u2019affichent dans l\u2019interface utilisateur et sont associ\u00e9es \u00e0 des codes de couleurs bas\u00e9s sur le degr\u00e9 de malveillance. Tags \u00b6 Les \u00e9tiquettes sont d\u2019importantes m\u00e9tadonn\u00e9es extraites d\u2019un fichier. Leur nom doit respecter la m\u00eame convention d\u2019affectation que l\u2019espace de noms, ce qui facilite leur organisation et permet de trouver plus facilement une information en particulier dans le syst\u00e8me. De plus, les \u00e9tiquettes sont index\u00e9es de mani\u00e8re \u00e0 g\u00e9n\u00e9rer des r\u00e9sultats \u00e0 une vitesse \u00e9tonnante. # Cette \u00e9tiquette peut trouver les adresses IP extraites de mani\u00e8re statique peu importe le service utilis\u00e9 lors de l\u2019extraction. result.sections.tags.network.static.ip; Toutes les \u00e9tiquettes enregistr\u00e9es dans le syst\u00e8me s\u2019affichent dans le menu Help [Aide] > Searching Help [Aide \u00e0 la recherche] de votre instance d\u2019Assemblyline. Niveau de malveillance (Score) \u00b6 La note attribu\u00e9e \u00e0 une soumission (degr\u00e9 de malveillance) est d\u00e9termin\u00e9e en fonction de la note la plus haute ayant \u00e9t\u00e9 attribu\u00e9e aux fichiers extraits au cours du processus d\u2019analyse. Prenons un fichier .zip comme exemple. Le fichier .zip peut avoir obtenu une note de 0, mais s\u2019il contient deux fichiers ayant respectivement obtenus des notes de 100 et de 500, la note maximale de la soumission sera de 500. Si on pousse l\u2019analyse, il est possible de comprendre sur quoi est bas\u00e9e chacune de ces notes. Signification de la note (en supposant que vous ex\u00e9cutez la plupart des services, dont certains antivirus et de bonnes signatures Yara) : -1000: sans dang\u00e9 0 - 299: information 300 - 699: suspicieux 700 - 999: tr\u00e8s suspicieux >= 1000: malicieux Rapport de soumission \u00b6 Le rapport de soumission s\u2019affiche sur la premi\u00e8re page lorsqu\u2019on consulte une soumission. Il s\u2019agit d\u2019un r\u00e9sum\u00e9 g\u00e9n\u00e9ral que l\u2019analyste peut consulter pour d\u00e9terminer s\u2019il est justifi\u00e9 de pousser l\u2019analyse plus loin. Information g\u00e9n\u00e9rale \u00b6 On retrouve de l\u2019information importante au haut de la fen\u00eatre : la date et l\u2019heure, le type de fichier d\u00e9tect\u00e9, sa taille, la note maximale et des condens\u00e9s divers. Heuristiques \u00b6 Dans cette section, on retrouve toutes les heuristiques cat\u00e9goris\u00e9es selon leur degr\u00e9 de malveillance, ainsi que tous les fichiers connexes. Attribution \u00b6 Dans cette section, l\u2019attribution est effectu\u00e9e \u00e0 partir des signatures Yara (si l\u2019\u00e9tiquette de l\u2019auteur de menace est fourni dans les m\u00e9tadonn\u00e9es de la r\u00e8gle) et les noms des virus dans l\u2019antivirus. Pour des r\u00e9sultats optimaux, il convient de mettre en pratique les r\u00e8gles Yara dict\u00e9es par les normes du CCC . D\u00e9tails de la soumission \u00b6 Le bouton Submission Details [D\u00e9tails de la soumission] est situ\u00e9 au haut du rapport de soumission. Les d\u00e9tails de la soumission affichent les param\u00e8tres de la soumission, \u00e0 savoir les services s\u00e9lectionn\u00e9s au moment o\u00f9 le fichier a \u00e9t\u00e9 soumis et les m\u00e9tadonn\u00e9es de la soumission. La section la plus importante est celle qui contient le bouton. Arborescence des fichiers d\u2019extraction \u00b6 La section contenant l\u2019arborescence offre une vue de tous les fichiers ayant \u00e9t\u00e9 trait\u00e9s et extraits, de m\u00eame que leur note et leur type de fichier respectifs. En cliquant sur les fichiers, on peut r\u00e9v\u00e9ler la section la plus int\u00e9ressante d\u2019Assemblyline : la page de d\u00e9tails des fichiers. D\u00e9tails des fichiers \u00b6 La section des d\u00e9tails contient toute l\u2019information concernant un fichier en particulier. Elle ne tient pas compte de la soumission dont le fichier faisait partie. Dans le coin sup\u00e9rieur droit, on retrouve une s\u00e9rie de fonctions utiles. Icone Decription Trouver toutes les soumissions connexes T\u00e9l\u00e9charger le fichier (lequel sera ins\u00e9r\u00e9 au format CaRT ) par d\u00e9faut pour \u00e9viter une auto-infection accidentelle Visionneuse de fichiers (vue ASCII, cha\u00eenes, hexad\u00e9cimal) Resoumettre le fichier aux fins d\u2019analyse Ajouter le fichier \u00e0 la liste s\u00fbre Fr\u00e9quence des fichiers \u00b6 Cette section indique combien de fois ce fichier a \u00e9t\u00e9 d\u00e9tect\u00e9, ainsi que la premi\u00e8re et derni\u00e8re d\u00e9tection. Ce compte est bas\u00e9 sur la p\u00e9riode de conservation du fichier dans le syst\u00e8me. \u00c9tiquettes des fichiers \u00b6 Cette section pr\u00e9sente toutes les \u00e9tiquettes regroup\u00e9es selon leur type, qui sont extraites dans ce fichier. On y retrouve l\u2019adresse IP, l\u2019URL et plusieurs autres indicateurs de compromission (IC) que vous pouvez utiliser dans le cadre de votre enqu\u00eate ou pour lancer une action dynamique (p. ex. mettre en place des interdictions sur vos pare-feux). En cliquant sur l\u2019une des \u00e9tiquettes, il est possible d\u2019afficher le service auquel elle appartient. R\u00e9sultats du service \u00b6 Cette section permet de consulter la sortie de chaque service, ainsi que les heuristiques et les \u00e9tiquettes observ\u00e9es. Il est \u00e9galement possible de voir les services \u00e0 l\u2019origine des \u00ab fichiers extraits \u00bb \u00e0 la fin des r\u00e9sultats de chaque service. Les r\u00e9sultats de fichiers mis en cache sont ignor\u00e9s chaque fois qu\u2019un service est mis \u00e0 jour. Si plusieurs versions des r\u00e9sultats sont disponibles, elles s\u2019affichent dans une liste d\u00e9roulante, ce qui permet de consulter les r\u00e9sultats des analyses pr\u00e9c\u00e9dentes. Pour afficher les d\u00e9tails, cliquer sur la section des r\u00e9sultats du service.","title":"R\u00e9sultats d\u2019Assemblyline"},{"location":"fr/user_manual/results/#resultats-dassemblyline","text":"","title":"R\u00e9sultats d\u2019Assemblyline"},{"location":"fr/user_manual/results/#heuristiques","text":"Par heuristique, on entend une fonction qui peut \u00eatre d\u00e9tect\u00e9e par le service dans le cadre de l\u2019analyse. - Elle se compose de ce qui suit : - un identifiant; - un nom; - une description; - une note (utilis\u00e9e pour signaler une heuristique comme \u00e9tant MALICIOUS [MALVEILLANTE], SUSPICIOUS [SUSPECTE] ou INFO); - l\u2019ID de la matrice Mitre\u2019s Att&ck; - des signatures qui sont souvent utilis\u00e9es pour donner plus de contexte. Le syst\u00e8me fait le suivi des heuristiques pour fournir des statistiques sur le nombre d\u2019occurrences et les notes attribu\u00e9es afin d\u2019apporter des ajustements si les performances des heuristiques sont ad\u00e9quates ou insuffisantes. Les heuristiques s\u2019affichent dans l\u2019interface utilisateur et sont associ\u00e9es \u00e0 des codes de couleurs bas\u00e9s sur le degr\u00e9 de malveillance.","title":"Heuristiques"},{"location":"fr/user_manual/results/#tags","text":"Les \u00e9tiquettes sont d\u2019importantes m\u00e9tadonn\u00e9es extraites d\u2019un fichier. Leur nom doit respecter la m\u00eame convention d\u2019affectation que l\u2019espace de noms, ce qui facilite leur organisation et permet de trouver plus facilement une information en particulier dans le syst\u00e8me. De plus, les \u00e9tiquettes sont index\u00e9es de mani\u00e8re \u00e0 g\u00e9n\u00e9rer des r\u00e9sultats \u00e0 une vitesse \u00e9tonnante. # Cette \u00e9tiquette peut trouver les adresses IP extraites de mani\u00e8re statique peu importe le service utilis\u00e9 lors de l\u2019extraction. result.sections.tags.network.static.ip; Toutes les \u00e9tiquettes enregistr\u00e9es dans le syst\u00e8me s\u2019affichent dans le menu Help [Aide] > Searching Help [Aide \u00e0 la recherche] de votre instance d\u2019Assemblyline.","title":"Tags"},{"location":"fr/user_manual/results/#niveau-de-malveillance-score","text":"La note attribu\u00e9e \u00e0 une soumission (degr\u00e9 de malveillance) est d\u00e9termin\u00e9e en fonction de la note la plus haute ayant \u00e9t\u00e9 attribu\u00e9e aux fichiers extraits au cours du processus d\u2019analyse. Prenons un fichier .zip comme exemple. Le fichier .zip peut avoir obtenu une note de 0, mais s\u2019il contient deux fichiers ayant respectivement obtenus des notes de 100 et de 500, la note maximale de la soumission sera de 500. Si on pousse l\u2019analyse, il est possible de comprendre sur quoi est bas\u00e9e chacune de ces notes. Signification de la note (en supposant que vous ex\u00e9cutez la plupart des services, dont certains antivirus et de bonnes signatures Yara) : -1000: sans dang\u00e9 0 - 299: information 300 - 699: suspicieux 700 - 999: tr\u00e8s suspicieux >= 1000: malicieux","title":"Niveau de malveillance (Score)"},{"location":"fr/user_manual/results/#rapport-de-soumission","text":"Le rapport de soumission s\u2019affiche sur la premi\u00e8re page lorsqu\u2019on consulte une soumission. Il s\u2019agit d\u2019un r\u00e9sum\u00e9 g\u00e9n\u00e9ral que l\u2019analyste peut consulter pour d\u00e9terminer s\u2019il est justifi\u00e9 de pousser l\u2019analyse plus loin.","title":"Rapport de soumission"},{"location":"fr/user_manual/results/#information-generale","text":"On retrouve de l\u2019information importante au haut de la fen\u00eatre : la date et l\u2019heure, le type de fichier d\u00e9tect\u00e9, sa taille, la note maximale et des condens\u00e9s divers.","title":"Information g\u00e9n\u00e9rale"},{"location":"fr/user_manual/results/#heuristiques_1","text":"Dans cette section, on retrouve toutes les heuristiques cat\u00e9goris\u00e9es selon leur degr\u00e9 de malveillance, ainsi que tous les fichiers connexes.","title":"Heuristiques"},{"location":"fr/user_manual/results/#attribution","text":"Dans cette section, l\u2019attribution est effectu\u00e9e \u00e0 partir des signatures Yara (si l\u2019\u00e9tiquette de l\u2019auteur de menace est fourni dans les m\u00e9tadonn\u00e9es de la r\u00e8gle) et les noms des virus dans l\u2019antivirus. Pour des r\u00e9sultats optimaux, il convient de mettre en pratique les r\u00e8gles Yara dict\u00e9es par les normes du CCC .","title":"Attribution"},{"location":"fr/user_manual/results/#details-de-la-soumission","text":"Le bouton Submission Details [D\u00e9tails de la soumission] est situ\u00e9 au haut du rapport de soumission. Les d\u00e9tails de la soumission affichent les param\u00e8tres de la soumission, \u00e0 savoir les services s\u00e9lectionn\u00e9s au moment o\u00f9 le fichier a \u00e9t\u00e9 soumis et les m\u00e9tadonn\u00e9es de la soumission. La section la plus importante est celle qui contient le bouton.","title":"D\u00e9tails de la soumission"},{"location":"fr/user_manual/results/#arborescence-des-fichiers-dextraction","text":"La section contenant l\u2019arborescence offre une vue de tous les fichiers ayant \u00e9t\u00e9 trait\u00e9s et extraits, de m\u00eame que leur note et leur type de fichier respectifs. En cliquant sur les fichiers, on peut r\u00e9v\u00e9ler la section la plus int\u00e9ressante d\u2019Assemblyline : la page de d\u00e9tails des fichiers.","title":"Arborescence des fichiers d\u2019extraction"},{"location":"fr/user_manual/results/#details-des-fichiers","text":"La section des d\u00e9tails contient toute l\u2019information concernant un fichier en particulier. Elle ne tient pas compte de la soumission dont le fichier faisait partie. Dans le coin sup\u00e9rieur droit, on retrouve une s\u00e9rie de fonctions utiles. Icone Decription Trouver toutes les soumissions connexes T\u00e9l\u00e9charger le fichier (lequel sera ins\u00e9r\u00e9 au format CaRT ) par d\u00e9faut pour \u00e9viter une auto-infection accidentelle Visionneuse de fichiers (vue ASCII, cha\u00eenes, hexad\u00e9cimal) Resoumettre le fichier aux fins d\u2019analyse Ajouter le fichier \u00e0 la liste s\u00fbre","title":"D\u00e9tails des fichiers"},{"location":"fr/user_manual/results/#frequence-des-fichiers","text":"Cette section indique combien de fois ce fichier a \u00e9t\u00e9 d\u00e9tect\u00e9, ainsi que la premi\u00e8re et derni\u00e8re d\u00e9tection. Ce compte est bas\u00e9 sur la p\u00e9riode de conservation du fichier dans le syst\u00e8me.","title":"Fr\u00e9quence des fichiers"},{"location":"fr/user_manual/results/#etiquettes-des-fichiers","text":"Cette section pr\u00e9sente toutes les \u00e9tiquettes regroup\u00e9es selon leur type, qui sont extraites dans ce fichier. On y retrouve l\u2019adresse IP, l\u2019URL et plusieurs autres indicateurs de compromission (IC) que vous pouvez utiliser dans le cadre de votre enqu\u00eate ou pour lancer une action dynamique (p. ex. mettre en place des interdictions sur vos pare-feux). En cliquant sur l\u2019une des \u00e9tiquettes, il est possible d\u2019afficher le service auquel elle appartient.","title":"\u00c9tiquettes des fichiers"},{"location":"fr/user_manual/results/#resultats-du-service","text":"Cette section permet de consulter la sortie de chaque service, ainsi que les heuristiques et les \u00e9tiquettes observ\u00e9es. Il est \u00e9galement possible de voir les services \u00e0 l\u2019origine des \u00ab fichiers extraits \u00bb \u00e0 la fin des r\u00e9sultats de chaque service. Les r\u00e9sultats de fichiers mis en cache sont ignor\u00e9s chaque fois qu\u2019un service est mis \u00e0 jour. Si plusieurs versions des r\u00e9sultats sont disponibles, elles s\u2019affichent dans une liste d\u00e9roulante, ce qui permet de consulter les r\u00e9sultats des analyses pr\u00e9c\u00e9dentes. Pour afficher les d\u00e9tails, cliquer sur la section des r\u00e9sultats du service.","title":"R\u00e9sultats du service"},{"location":"fr/user_manual/searching/","text":"Recherche \u00b6 Assemblyline tire avantage des puissantes capacit\u00e9s de recherche pratiquement infinies d' Elasticsearch . Banque de documents \u00b6 Les index d\u2019information sont un des principaux concepts \u00e0 approfondir. Ils permettent \u00e0 Assemblyline de d\u00e9dupliquer la plupart des r\u00e9sultats dans le syst\u00e8me, ce qui explique pourquoi l\u2019outil peut s\u2019adapter aussi efficacement. Les recherches dans les champs index\u00e9s s\u2019effectuent tr\u00e8s rapidement. On retrouve 5 index principaux : Submissions [Soumissions]; Files [Fichiers]; Results [R\u00e9sultats]; Alerts [Alertes]; Signatures. Vous pouvez afficher tous les index et les champs index\u00e9s connexes apr\u00e8s avoir assur\u00e9 le bon fonctionnement d\u2019Assemblyline en consultant l\u2019aide sous Help > Search help menu . Recherche de comportements et limitations \u00b6 Lors d\u2019une recherche dans l\u2019interface utilisateur, la requ\u00eate est transmise dans tous les index et renvoie tous les r\u00e9sultats correspondants. Vous devez limiter vos crit\u00e8res de recherche \u00e0 un seul indexe. En d\u2019autres mots, vous ne pouvez pas rechercher de l\u2019information dans deux index diff\u00e9rents ou plus. Il est possible de contourner cette limitation en faisant appel \u00e0 l\u2019API REST. La m\u00e9thode consiste alors \u00e0 effectuer une recherche dans un indexe, puis \u00e0 l\u2019\u00e9largir ou l\u2019affiner en recherchant ces \u00e9l\u00e9ments dans d\u2019autres index. Exemples de Recherche \u00b6 Un moyen rapide de se familiariser avec les index de recherche consiste \u00e0 utiliser l'\u00e9l\u00e9ment \" Trouver les r\u00e9sultats associ\u00e9s \" dans le menu d\u00e9roulant des balises. Cliquer sur l'\u00e9l\u00e9ment \" Trouver les r\u00e9sultats associ\u00e9s \" sur la balise av.virus_name ( HEUR/Macro.Downloader.MRAA.Gen ) g\u00e9n\u00e9rera la requ\u00eate suivante : result . sections . tags . av . virus_name :\"HEUR/Macro.Downloader.MRAA.Gen\" Vous pouvez \u00e9galement g\u00e9n\u00e9rer des recherches plus complexes en utilisant une syntaxe de requ\u00eate compl\u00e8te. En voici quelques exemples : # Trouver tous les r\u00e9sultats o\u00f9 le service ViperMonkey a extrait l\u2019adresse IP 10.10.10.10 result . sections . tags . network . static . ip :\"10.10.10.10\" AND response . service_name :ViperMonkey # Trouver toutes les soumissions pour lesquelles une note de 2000 ou plus a \u00e9t\u00e9 attribu\u00e9e # au cours des deux derniers jours max_score : [ 2000 TO *] AND times . submitted : [ now - 2 d TO now ] # Trouver tous les r\u00e9sultats de l\u2019antivirus dont le nom de signature correspond \u00e0 Emotet result . sections . tags . av . virus_name :* Emotet * Le syst\u00e8me prend en charge un large \u00e9ventail de param\u00e8tres de recherche, comme les caract\u00e8res g\u00e9n\u00e9riques, les plages et les expressions r\u00e9guli\u00e8res. Vous trouverez la syntaxe compl\u00e8te dans l\u2019aide sous Help > Search Help . Les requ\u00eates de recherche peuvent \u00e9galement \u00eatre utilis\u00e9es dans le client d\u2019Assemblyline pour instaurer un puissant m\u00e9canisme qui s\u2019ex\u00e9cutera automatiquement lorsque de nouveaux fichiers seront analys\u00e9s par le syst\u00e8me.","title":"Recherche"},{"location":"fr/user_manual/searching/#recherche","text":"Assemblyline tire avantage des puissantes capacit\u00e9s de recherche pratiquement infinies d' Elasticsearch .","title":"Recherche"},{"location":"fr/user_manual/searching/#banque-de-documents","text":"Les index d\u2019information sont un des principaux concepts \u00e0 approfondir. Ils permettent \u00e0 Assemblyline de d\u00e9dupliquer la plupart des r\u00e9sultats dans le syst\u00e8me, ce qui explique pourquoi l\u2019outil peut s\u2019adapter aussi efficacement. Les recherches dans les champs index\u00e9s s\u2019effectuent tr\u00e8s rapidement. On retrouve 5 index principaux : Submissions [Soumissions]; Files [Fichiers]; Results [R\u00e9sultats]; Alerts [Alertes]; Signatures. Vous pouvez afficher tous les index et les champs index\u00e9s connexes apr\u00e8s avoir assur\u00e9 le bon fonctionnement d\u2019Assemblyline en consultant l\u2019aide sous Help > Search help menu .","title":"Banque de documents"},{"location":"fr/user_manual/searching/#recherche-de-comportements-et-limitations","text":"Lors d\u2019une recherche dans l\u2019interface utilisateur, la requ\u00eate est transmise dans tous les index et renvoie tous les r\u00e9sultats correspondants. Vous devez limiter vos crit\u00e8res de recherche \u00e0 un seul indexe. En d\u2019autres mots, vous ne pouvez pas rechercher de l\u2019information dans deux index diff\u00e9rents ou plus. Il est possible de contourner cette limitation en faisant appel \u00e0 l\u2019API REST. La m\u00e9thode consiste alors \u00e0 effectuer une recherche dans un indexe, puis \u00e0 l\u2019\u00e9largir ou l\u2019affiner en recherchant ces \u00e9l\u00e9ments dans d\u2019autres index.","title":"Recherche de comportements et limitations"},{"location":"fr/user_manual/searching/#exemples-de-recherche","text":"Un moyen rapide de se familiariser avec les index de recherche consiste \u00e0 utiliser l'\u00e9l\u00e9ment \" Trouver les r\u00e9sultats associ\u00e9s \" dans le menu d\u00e9roulant des balises. Cliquer sur l'\u00e9l\u00e9ment \" Trouver les r\u00e9sultats associ\u00e9s \" sur la balise av.virus_name ( HEUR/Macro.Downloader.MRAA.Gen ) g\u00e9n\u00e9rera la requ\u00eate suivante : result . sections . tags . av . virus_name :\"HEUR/Macro.Downloader.MRAA.Gen\" Vous pouvez \u00e9galement g\u00e9n\u00e9rer des recherches plus complexes en utilisant une syntaxe de requ\u00eate compl\u00e8te. En voici quelques exemples : # Trouver tous les r\u00e9sultats o\u00f9 le service ViperMonkey a extrait l\u2019adresse IP 10.10.10.10 result . sections . tags . network . static . ip :\"10.10.10.10\" AND response . service_name :ViperMonkey # Trouver toutes les soumissions pour lesquelles une note de 2000 ou plus a \u00e9t\u00e9 attribu\u00e9e # au cours des deux derniers jours max_score : [ 2000 TO *] AND times . submitted : [ now - 2 d TO now ] # Trouver tous les r\u00e9sultats de l\u2019antivirus dont le nom de signature correspond \u00e0 Emotet result . sections . tags . av . virus_name :* Emotet * Le syst\u00e8me prend en charge un large \u00e9ventail de param\u00e8tres de recherche, comme les caract\u00e8res g\u00e9n\u00e9riques, les plages et les expressions r\u00e9guli\u00e8res. Vous trouverez la syntaxe compl\u00e8te dans l\u2019aide sous Help > Search Help . Les requ\u00eates de recherche peuvent \u00e9galement \u00eatre utilis\u00e9es dans le client d\u2019Assemblyline pour instaurer un puissant m\u00e9canisme qui s\u2019ex\u00e9cutera automatiquement lorsque de nouveaux fichiers seront analys\u00e9s par le syst\u00e8me.","title":"Exemples de Recherche"},{"location":"fr/user_manual/submitting_file/","text":"Soumettre un fichier aux fins d\u2019analyse \u00b6 Soumission \u00b6 Il est facile de soumettre un fichier aux fins d\u2019analyse. On peut le faire directement \u00e0 partir de l\u2019interface Web d\u2019Assemblyline. En ce qui concerne l\u2019automatisation et l\u2019int\u00e9gration, vous pouvez utiliser l\u2019 REST API . Partage et classification \u00b6 Si votre syst\u00e8me est configur\u00e9 de mani\u00e8re \u00e0 permettre le contr\u00f4le partag\u00e9 (TLP) ou la classification, il est possible de choisir une restriction en cliquant sur la banni\u00e8re. S\u00e9lection d\u2019un fichier \u00e0 analyser \u00b6 Vous pouvez cliquer sur Select a file to scan [S\u00e9lectionner un fichier \u00e0 analyser] ou glissez-d\u00e9posez un fichier dans la zone pour l\u2019ajouter \u00e0 l\u2019analyse. Options \u00b6 D\u2019autres options de soumission sont offertes et permettent de faire ce qui suit : s\u00e9lectionner les cat\u00e9gories de services ou les services particuliers \u00e0 utiliser aux fins d\u2019analyse; pr\u00e9ciser les options de configuration des services (p. ex. fournir un mot de passe ou l\u2019expiration d\u2019une analyse dynamique). | Ignore filtering services | [Ignorer les services de filtrage] Contourne les services de mises en liste blanche | | Ignore result cache | [Ignorer le cache des r\u00e9sultats] Force une nouvelle analyse m\u00eame si le fichier a d\u00e9j\u00e0 \u00e9t\u00e9 analys\u00e9 r\u00e9cemment par la m\u00eame version des services | | Ignore dynamic recursion prevention | [Ignorer la pr\u00e9vention de la r\u00e9cursivit\u00e9 dynamique] D\u00e9sactive la limite d\u2019it\u00e9ration d'un fichier | | Profile current scan | [Profiler l\u2019analyse actuelle]| | Perform deep analysis | [Effectuer une analyse en profondeur] Fournit un d\u00e9sobscurcissement maximal - Fortement recommand\u00e9 pour les fichiers malveillants connus ou hautement suspects afin de d\u00e9tecter le contenu consid\u00e9rablement obscurci | | Time to live | [Dur\u00e9e de vie] Dur\u00e9e (en jours) avant que le fichier soit supprim\u00e9 du syst\u00e8me | Analyse de fichiers \u00b6 Une fois le fichier soumis dans Assemblyline, le syst\u00e8me proc\u00e9dera automatiquement \u00e0 plusieurs v\u00e9rifications afin de d\u00e9terminer la meilleure fa\u00e7on de le traiter. Le mod\u00e8le d\u2019analyse r\u00e9cursive est l\u2019une des fonctionnalit\u00e9s les plus puissantes d\u2019Assemblyline. Les maliciels et les documents malveillants utilisent souvent plusieurs couches d\u2019obscurcissement. L\u2019analyse r\u00e9cursive permet au syst\u00e8me de supprimer ces couches et de poursuivre l\u2019analyse du fichier. Il en r\u00e9sulte souvent un script en texte clair ou un maliciel non condens\u00e9 qu\u2019un antivirus conventionnel pourra d\u00e9tecter tr\u00e8s facilement.","title":"Soumettre un fichier aux fins d\u2019analyse"},{"location":"fr/user_manual/submitting_file/#soumettre-un-fichier-aux-fins-danalyse","text":"","title":"Soumettre un fichier aux fins d\u2019analyse"},{"location":"fr/user_manual/submitting_file/#soumission","text":"Il est facile de soumettre un fichier aux fins d\u2019analyse. On peut le faire directement \u00e0 partir de l\u2019interface Web d\u2019Assemblyline. En ce qui concerne l\u2019automatisation et l\u2019int\u00e9gration, vous pouvez utiliser l\u2019 REST API .","title":"Soumission"},{"location":"fr/user_manual/submitting_file/#partage-et-classification","text":"Si votre syst\u00e8me est configur\u00e9 de mani\u00e8re \u00e0 permettre le contr\u00f4le partag\u00e9 (TLP) ou la classification, il est possible de choisir une restriction en cliquant sur la banni\u00e8re.","title":"Partage et classification"},{"location":"fr/user_manual/submitting_file/#selection-dun-fichier-a-analyser","text":"Vous pouvez cliquer sur Select a file to scan [S\u00e9lectionner un fichier \u00e0 analyser] ou glissez-d\u00e9posez un fichier dans la zone pour l\u2019ajouter \u00e0 l\u2019analyse.","title":"S\u00e9lection d\u2019un fichier \u00e0 analyser"},{"location":"fr/user_manual/submitting_file/#options","text":"D\u2019autres options de soumission sont offertes et permettent de faire ce qui suit : s\u00e9lectionner les cat\u00e9gories de services ou les services particuliers \u00e0 utiliser aux fins d\u2019analyse; pr\u00e9ciser les options de configuration des services (p. ex. fournir un mot de passe ou l\u2019expiration d\u2019une analyse dynamique). | Ignore filtering services | [Ignorer les services de filtrage] Contourne les services de mises en liste blanche | | Ignore result cache | [Ignorer le cache des r\u00e9sultats] Force une nouvelle analyse m\u00eame si le fichier a d\u00e9j\u00e0 \u00e9t\u00e9 analys\u00e9 r\u00e9cemment par la m\u00eame version des services | | Ignore dynamic recursion prevention | [Ignorer la pr\u00e9vention de la r\u00e9cursivit\u00e9 dynamique] D\u00e9sactive la limite d\u2019it\u00e9ration d'un fichier | | Profile current scan | [Profiler l\u2019analyse actuelle]| | Perform deep analysis | [Effectuer une analyse en profondeur] Fournit un d\u00e9sobscurcissement maximal - Fortement recommand\u00e9 pour les fichiers malveillants connus ou hautement suspects afin de d\u00e9tecter le contenu consid\u00e9rablement obscurci | | Time to live | [Dur\u00e9e de vie] Dur\u00e9e (en jours) avant que le fichier soit supprim\u00e9 du syst\u00e8me |","title":"Options"},{"location":"fr/user_manual/submitting_file/#analyse-de-fichiers","text":"Une fois le fichier soumis dans Assemblyline, le syst\u00e8me proc\u00e9dera automatiquement \u00e0 plusieurs v\u00e9rifications afin de d\u00e9terminer la meilleure fa\u00e7on de le traiter. Le mod\u00e8le d\u2019analyse r\u00e9cursive est l\u2019une des fonctionnalit\u00e9s les plus puissantes d\u2019Assemblyline. Les maliciels et les documents malveillants utilisent souvent plusieurs couches d\u2019obscurcissement. L\u2019analyse r\u00e9cursive permet au syst\u00e8me de supprimer ces couches et de poursuivre l\u2019analyse du fichier. Il en r\u00e9sulte souvent un script en texte clair ou un maliciel non condens\u00e9 qu\u2019un antivirus conventionnel pourra d\u00e9tecter tr\u00e8s facilement.","title":"Analyse de fichiers"}]}